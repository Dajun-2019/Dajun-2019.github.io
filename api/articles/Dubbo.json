{"title":"Dubbo","uid":"0ca9a116507ce2deadb100db49d2064a","slug":"Dubbo","date":"2023-09-02T02:38:57.000Z","updated":"2023-09-18T10:50:05.073Z","comments":true,"path":"api/articles/Dubbo.json","keywords":null,"cover":[],"content":"<h1 id=\"Dubbo\"><a href=\"#Dubbo\" class=\"headerlink\" title=\"Dubbo\"></a>Dubbo</h1><ol>\n<li><p>RPC</p>\n<ol>\n<li><p>RPC（Remote Procedure Call）全称为远程过程调用，用于解决不同服务器之间两个方法的互相调用问题，并且通过网络编程来传递方法调用所需要的参数，通过RPC来简化底层网络编程（如TCP连接的建立）、规划化参数序列化反序列化等问题，使得这个过程就像调用本地方法一样简单</p>\n</li>\n<li><p>概念</p>\n<ul>\n<li><p><strong>客户端（服务消费端）</strong> ：调用远程方法的一端</p>\n</li>\n<li><p><strong>客户端 Stub（桩）</strong> ： 这其实就是一代理类。把要调用的方法、类、方法参数等信息传递到服务端（序列化成RpcRequest）</p>\n</li>\n<li><p><strong>网络传输</strong> ： 网络传输就是把要调用的方法的信息（参数、方法名、类等）传输到服务端，然后服务端执行完之后再把返回结果通过网络传输给你传输回来。网络传输的实现方式有Socket编程、Netty等</p>\n</li>\n<li><p><strong>服务端 Stub（桩）</strong> ：接收到客户端执行方法的请求后（反序列化RpcRequest），去指定对应的方法然后返回结果给客户端的类（序列化成RpcResponse）</p>\n</li>\n<li><p><strong>服务端（服务提供端）</strong> ：提供远程方法的一端</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902104047292.png\" alt=\"image-20230902104047292\"></p>\n</li>\n</ul>\n</li>\n<li><p>常见RPC框架</p>\n<ol>\n<li>Dubbo：为大规模微服务实践提供高性能 RPC 通信、流量治理、可观测性等解决方案， 涵盖 Java、Golang 等多种语言 SDK 实现。提供了从服务定义、服务发现、服务通信到流量管控等几乎所有的服务治理能力，支持 Triple 协议（基于 HTTP/2 之上定义的下一代 RPC 通信协议）、应用级服务发现、Dubbo Mesh （Dubbo3 赋予了很多云原生友好的新特性）等特性</li>\n<li>gRPC：Google 开源的一个高性能、通用的开源 RPC 框架。主要面向移动应用开发并基于 HTTP/2 协议标准而设计（支持双向流、消息头压缩等功能，更加节省带宽），基于 ProtoBuf 序列化协议开发，并且支持众多开发语言<ul>\n<li>ProtoBuf：一种更加灵活、高效的数据格式，可用于通讯协议、数据存储等领域，基本支持所有主流编程语言且与平台无关，但是接口和数据类型的定义比较繁琐</li>\n</ul>\n</li>\n<li>Thrift：Facebook 开源的跨语言的 RPC 通信框架，具有跨语言特性（比 gRPC 支持的语言多）和出色的性能，可以基于 Thrift 研发一套分布式服务框架，增加诸如服务注册、服务发现等功能</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>分布式</p>\n<ol>\n<li>What：把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能，比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上</li>\n<li>Why：每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展</li>\n</ol>\n</li>\n<li><p>Dubbo</p>\n<ol>\n<li><p>What</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902104334249.png\" alt=\"image-20230902104334249\"></p>\n</li>\n<li><p>Why：分布式架构下，不同微服务之间需要互相调用，Dubbo主要解决这种情况下产生的诸多问题，如下</p>\n<ol>\n<li><strong>负载均衡</strong> ： 同一个服务部署在不同的机器时该调用哪一台机器上的服务</li>\n<li><strong>服务调用链路生成</strong> ： 随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，通过Dubbo 可以了解服务之间互相是如何调用的及先后顺序如何</li>\n<li><strong>服务访问压力以及时长统计、资源调度和治理</strong> ：基于访问压力实时管理集群容量，提高集群利用率</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>Dubbo架构</p>\n<ol>\n<li><p>核心角色</p>\n<ul>\n<li><p><strong>Container：</strong> 服务运行容器，负责加载、运行服务提供者</p>\n</li>\n<li><p><strong>Provider：</strong> 暴露服务的服务提供方，会向注册中心注册自己提供的服务</p>\n</li>\n<li><p><strong>Consumer：</strong> 调用远程服务的服务消费方，会向注册中心订阅自己所需的服务</p>\n</li>\n<li><p><strong>Registry：</strong>服务注册与发现的注册中心。注册中心会返回服务提供者地址列表给消费者，非必须</p>\n<ul>\n<li>注册中心和监控中心都宕机的话，服务会挂掉吗？：不会。两者都宕机也不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表。注册中心和监控中心都是可选的，服务消费者可以直连服务提供者</li>\n</ul>\n</li>\n<li><p><strong>Monitor：</strong> 统计服务的调用次数和调用时间的监控中心。服务消费者和提供者会定时发送统计数据到监控中心，非必须</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902104418561.png\" alt=\"image-20230902104418561\"></p>\n</li>\n</ul>\n</li>\n<li><p>Invoker：Dubbo 对远程调用的抽象，主要分为服务提供Invoker和服务消费Invoker两种，用来封装使用动态代理来屏蔽远程调用的细节的相关代码</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902104624750.png\"></p>\n</li>\n<li><p>架构层次</p>\n<ul>\n<li><p><strong>config 配置层</strong>：Dubbo相关的配置。支持代码配置，同时也支持基于 Spring 来做配置，以 <code>ServiceConfig</code>, <code>ReferenceConfig</code> 为中心</p>\n</li>\n<li><p><strong>proxy 服务代理层</strong>：调用远程方法像调用本地的方法一样简单的一个关键，真实调用过程依赖代理类，以 <code>ServiceProxy</code> 为中心。</p>\n</li>\n<li><p>registry 注册中心层：封装服务地址的注册与发现</p>\n<ul>\n<li>注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互</li>\n<li>服务提供者宕机后，注册中心会立即推送事件通知消费者</li>\n</ul>\n</li>\n<li><p><strong>cluster 路由层</strong>：封装多个提供者的路由及负载均衡，并桥接注册中心，以 <code>Invoker</code> 为中心</p>\n</li>\n<li><p>monitor 监控层：RPC 调用次数和调用时间监控，以<code>Statistics</code>为中心</p>\n<ul>\n<li>监控中心负责统计各服务调用次数，调用时间等</li>\n</ul>\n</li>\n<li><p><strong>protocol 远程调用层</strong>：封装 RPC 调用，以 <code>Invocation</code>, <code>Result</code> 为中心</p>\n</li>\n<li><p><strong>exchange 信息交换层</strong>：封装请求响应模式，同步转异步，以 <code>Request</code>, <code>Response</code> 为中心</p>\n</li>\n<li><p><strong>transport 网络传输层</strong>：抽象 mina 和 netty 为统一接口，以 <code>Message</code> 为中心</p>\n</li>\n<li><p><strong>serialize 数据序列化层</strong> ：对需要在网络传输的数据进行序列化</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902104851058.png\" alt=\"image-20230902104851058\"></p>\n</li>\n</ul>\n</li>\n<li><p>SPI机制</p>\n<ol>\n<li>Wha&amp;Why：SPI（Service Provider Interface） 机制被大量用在开源项目中，帮助动态寻找服务/功能（比如负载均衡策略）的实现</li>\n<li>How：首先将接口的实现类放在配置文件中，在程序运行过程中读取配置文件，通过反射加载实现类。这样，就可以在运行的时候，动态替换接口的实现类。和 IoC 的解耦思想是类似的</li>\n<li>Java本身实现了SPI机制，但是Dubbo做了增强，并提供多种扩展机制，只需要实现接口或抽象类并且将新的实现类写入到配置文件中</li>\n</ol>\n</li>\n<li><p>微内核架构</p>\n<ol>\n<li>微内核架构：微内核架构模式（有时被称为插件架构模式）是实现基于产品应用程序的一种自然模式。基于产品的应用程序是已经打包好并且拥有不同版本，可作为第三方插件下载的。然后，很多公司也在开发、发布自己内部商业应用像有版本号、说明及可加载插件式的应用软件（这也是这种模式的特征）。微内核系统可让用户添加额外的应用如插件，到核心应用，继而提供了可扩展性和功能分离的用法（《软件架构模式》）</li>\n<li>通常情况下，微核心都会采用 Factory、IoC、OSGi 等方式管理插件生命周期。Dubbo 不想依赖 Spring 等 IoC 容器，也不想自己造一个小的 IoC 容器（过度设计），因此采用了一种最简单的 Factory 方式管理插件 ：<strong>JDK 标准的 SPI 扩展机制</strong> （<code>java.util.ServiceLoader</code>）</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>Dubbo负载均衡策略</p>\n<ol>\n<li><p>负载均衡：负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动）的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件</p>\n</li>\n<li><p>负载均衡策略：默认是random，所有策略继承自<code>AbstractLoadBalance</code>，该类实现了<code>LoadBalance</code>接口，并封装了一些公共的逻辑</p>\n<ol>\n<li><p>****RandomLoadBalance（对加权随机）****：根据权重所占比例来平均划分，生成随机数来确定对应的区间</p>\n<ul>\n<li><p>代码</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">public class RandomLoadBalance extends AbstractLoadBalance &#123;\n\n    public static final String NAME &#x3D; &quot;random&quot;;\n\n    @Override\n    protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123;\n\n        int length &#x3D; invokers.size();\n        boolean sameWeight &#x3D; true;\n        int[] weights &#x3D; new int[length]; \n        int totalWeight &#x3D; 0;\n        &#x2F;&#x2F; 下面这个for循环的主要作用就是计算所有该服务的提供者的权重之和 totalWeight（），除此之外，还会检测每个服务提供者的权重是否相同\n        for (int i &#x3D; 0; i &lt; length; i++) &#123;\n            int weight &#x3D; getWeight(invokers.get(i), invocation);\n            totalWeight +&#x3D; weight;\n            weights[i] &#x3D; totalWeight; &#x2F;&#x2F;序列\n            if (sameWeight &amp;&amp; totalWeight !&#x3D; weight * (i + 1)) &#123;\n                sameWeight &#x3D; false;\n            &#125;\n        &#125;\n        if (totalWeight &gt; 0 &amp;&amp; !sameWeight) &#123;\n            &#x2F;&#x2F; 随机生成一个 [0, totalWeight) 区间内的数字\n            int offset &#x3D; ThreadLocalRandom.current().nextInt(totalWeight);\n            &#x2F;&#x2F; 判断会落在哪个服务提供者的区间\n            for (int i &#x3D; 0; i &lt; length; i++) &#123;\n                if (offset &lt; weights[i]) &#123;\n                    return invokers.get(i);\n                &#125;\n            &#125;\n  \n        return invokers.get(ThreadLocalRandom.current().nextInt(length));\n    &#125;\n\n&#125;</code></pre></li>\n</ul>\n</li>\n<li><p>****LeastActiveLoadBalance（最小活跃数）****：初始状态下所有服务提供者的活跃数均为 0（每个服务提供者的中特定方法都对应一个活跃数），每收到一个请求后，对应的服务提供者的活跃数 +1，当这个请求处理完之后，活跃数 -1。因此，Dubbo 就认为谁的活跃数越少，谁的处理速度就越快，性能也越好，这样的话，就优先把请求给活跃数少的服务提供者处理。活跃数是通过 <code>RpcStatus</code> 中的一个 <code>ConcurrentMap</code> 保存的</p>\n<ul>\n<li><p>代码</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">public class LeastActiveLoadBalance extends AbstractLoadBalance &#123;\n\n    public static final String NAME &#x3D; &quot;leastactive&quot;;\n\n    @Override\n    protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123;\n        int length &#x3D; invokers.size();\n        int leastActive &#x3D; -1;\n        int leastCount &#x3D; 0;\n        int[] leastIndexes &#x3D; new int[length];\n        int[] weights &#x3D; new int[length];\n        int totalWeight &#x3D; 0;\n        int firstWeight &#x3D; 0;\n        boolean sameWeight &#x3D; true;\n        &#x2F;&#x2F; 这个 for 循环的主要作用是遍历 invokers 列表，找出活跃数最小的 Invoker\n        &#x2F;&#x2F; 如果有多个 Invoker 具有相同的最小活跃数，还会记录下这些 Invoker 在 invokers 集合中的下标，并累加它们的权重，比较它们的权重值是否相等\n        for (int i &#x3D; 0; i &lt; length; i++) &#123;\n            Invoker&lt;T&gt; invoker &#x3D; invokers.get(i);\n            &#x2F;&#x2F; 获取 invoker 对应的活跃(active)数\n            int active &#x3D; RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive();\n            int afterWarmup &#x3D; getWeight(invoker, invocation);\n            weights[i] &#x3D; afterWarmup;\n            if (leastActive &#x3D;&#x3D; -1 || active &lt; leastActive) &#123;\n                leastActive &#x3D; active;\n                leastCount &#x3D; 1;\n                leastIndexes[0] &#x3D; i;\n                totalWeight &#x3D; afterWarmup;\n                firstWeight &#x3D; afterWarmup;\n                sameWeight &#x3D; true;\n            &#125; else if (active &#x3D;&#x3D; leastActive) &#123;\n                leastIndexes[leastCount++] &#x3D; i;\n                totalWeight +&#x3D; afterWarmup;\n                if (sameWeight &amp;&amp; afterWarmup !&#x3D; firstWeight) &#123;\n                    sameWeight &#x3D; false;\n                &#125;\n            &#125;\n        &#125;\n       &#x2F;&#x2F; 如果只有一个 Invoker 具有最小的活跃数，此时直接返回该 Invoker 即可\n        if (leastCount &#x3D;&#x3D; 1) &#123;\n            return invokers.get(leastIndexes[0]);\n        &#125;\n        &#x2F;&#x2F; 如果有多个 Invoker 具有相同的最小活跃数，但它们之间的权重不同\n        &#x2F;&#x2F; 这里的处理方式就和  RandomLoadBalance 一致了\n        if (!sameWeight &amp;&amp; totalWeight &gt; 0) &#123;\n            int offsetWeight &#x3D; ThreadLocalRandom.current().nextInt(totalWeight);\n            for (int i &#x3D; 0; i &lt; leastCount; i++) &#123;\n                int leastIndex &#x3D; leastIndexes[i];\n                offsetWeight -&#x3D; weights[leastIndex];\n                if (offsetWeight &lt; 0) &#123;\n                    return invokers.get(leastIndex);\n                &#125;\n            &#125;\n        &#125;\n        return invokers.get(leastIndexes[ThreadLocalRandom.current().nextInt(leastCount)]);\n    &#125;\n&#125;</code></pre></li>\n</ul>\n</li>\n<li><p>****ConsistentHashLoadBalance（一致性Hash）****：没有权重的概念，具体是哪个服务提供者处理请求是由你的请求的参数决定的，也就是说相同参数的请求总是发到同一个服务提供者，并且引入虚拟节点，通过虚拟节点可以让节点更加分散，有效均衡各个节点的请求量</p>\n</li>\n<li><p>****RoundRobinLoadBalance（加权轮询）****：轮询就是把请求依次分配给每个服务提供者。加权轮询就是在轮询的基础上，让更多的请求落到权重更大的服务提供者上</p>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>Dubbo序列化协议</p>\n<ol>\n<li>Dubbo 支持多种序列化方式：JDK自带的序列化、hessian2、JSON、Kryo、FST、Protostuff，ProtoBuf等等，其中默认使用的序列化方式是 hessian2</li>\n<li>优缺点：官方推荐使用Kryo<ul>\n<li>JDK自带的序列化不支持跨语言调用、JSON性能差</li>\n<li>像 Protostuff，ProtoBuf、hessian2这些都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用</li>\n<li>Kryo和FST这两种序列化方式是 Dubbo 后来才引入的，性能非常好。不过，这两者都是专门针对 Java 语言的</li>\n</ul>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"Zookeeper\"><a href=\"#Zookeeper\" class=\"headerlink\" title=\"Zookeeper\"></a>Zookeeper</h1><ol>\n<li><p>What：是一个开源的，是用于维护配置信息，命名，提供分布式同步和提供组服务的集中式服务，可以基于 Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能，比如Dubbo和Kafka都通过Zookeeper作为注册中心，有以下特性</p>\n<ul>\n<li><p>顺序一致性：leader会根据请求顺序生成 ZXID 来严格保证请求顺序的下发执行</p>\n<ul>\n<li>识别请求的先后顺序：Leader 收到请求之后，会将每个请求分配一个全局唯一递增的事务ID：zxid，然后把请求放入到一个 <strong>FIFO 的队列</strong>中，之后就会按照 FIFO 的策略发送给所有的 Follower</li>\n</ul>\n</li>\n<li><p><strong>原子性</strong>：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，要么成功，要么就失败</p>\n</li>\n<li><p>单一视图：无论客户端连到哪一个 ZooKeeper 服务器上，看到的数据都是一致的</p>\n<ul>\n<li>数据不一致情况还是会有的，因为 Zookeeper 采用的是<strong>过半写</strong>机制，意味着<strong>3台服务器只要有两台写成功就代表整个集群写成功</strong>，如果刚好有请求打在这台还<strong>未写的服务器</strong>上就查询不到该数据，就会有数据不一致的情况产生</li>\n</ul>\n</li>\n<li><p><strong>可靠性</strong>：一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来</p>\n</li>\n<li><p><strong>实时性</strong>：Zookeeper 仅仅能保证在段时间内客户端最终一定能够从服务端上读取到最新的数据状态</p>\n</li>\n</ul>\n</li>\n<li><p>数据结构</p>\n<ol>\n<li><p>ZooKeeper 提供的名称空间与标准文件系统的名称空间非常相似。名称是由斜杠（“ /”）分隔的一系列路径元素。ZooKeeper 命名空间中的每个 znode 均由路径标识。<strong>每个 znode 都有一个父对象</strong>，其路径是 znode 的前缀，元素少一个（ root（“ /”）例外）。此外，与标准文件系统完全一样，<strong>如果 znode 有子节点，则无法删除它</strong></p>\n<img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902105226406.png\" alt=\"image-20230902105226406\" style=\"zoom: 50%;\" /></li>\n<li><p>ZooKeeper 与标准文件系统之间的主要区别在于，<strong>每个 znode 都可以具有与之关联的数据</strong>（<strong>每个文件也可以是目录</strong>，反之亦然），并且 znode 限于它们可以拥有的数据量。ZooKeeper 旨在存储协调数据：状态信息，配置，位置信息等。这种元信息通常以千字节来度量，最大支持1M的内置完整性检查</p>\n</li>\n</ol>\n</li>\n<li><p>znode</p>\n<ol>\n<li><p>znode分类</p>\n<ol>\n<li>三种类型<ul>\n<li><strong>持久节点</strong>（persistent node）节点会被持久化</li>\n<li><strong>临时节点</strong>（ephemeral node）客户端断开连接后，ZooKeeper 会自动删除临时节点</li>\n<li><strong>顺序节点</strong>（sequential node）每次创建顺序节点时，ZooKeeper 都会在路径后面自动添加上10位的数字，从1开始，最大是2147483647 （2^32-1）</li>\n</ul>\n</li>\n<li>四种形式<ul>\n<li><strong>持久节点：</strong>如 create /test/a “hello”，通过 create参数指定为持久节点</li>\n<li><strong>持久顺序节点：</strong>通过 create -s参数指定为顺序节点</li>\n<li><strong>临时节点：</strong>通过 create -e参数指定为顺序节点</li>\n<li><strong>临时顺序节点：</strong>通过 create -s -e参数指定为临时及顺序节点</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>znode内部结构：包括存储数据（data）、访问权限（acl）、子节点引用（child）、节点状态信息（stat）</p>\n<ul>\n<li><strong>data</strong>:：znode存储的业务数据信息</li>\n<li><strong>acl</strong>:：记录客户端对znode节点的访问权限，如IP等</li>\n<li><strong>child</strong>:：当前节点的子节点引用</li>\n<li><strong>stat</strong>:：包含Znode节点的状态信息，比如事务id、版本号、时间戳等等。</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>系统架构</p>\n<ol>\n<li><p>ZooKeeper 分为<strong>服务器端</strong>（Server） 和<strong>客户端</strong>（Client），客户端可以连接到整个 ZooKeeper 服务的任意服务器上（除非 leaderServes 参数被显式设置，leader 不允许接受客户端连接），客户端使用并维护一个 <strong>TCP 连接</strong>，通过这个连接发送请求、接受响应、获取观察的事件以及发送信息，其中服务器主要分为以下三种角色</p>\n<ul>\n<li><p><strong>Leader：</strong>负责投票的发起与决议，更新系统状态，写数据</p>\n</li>\n<li><p><strong>Follower：</strong>用于接收客户端请求并用来返回结果，在选主过程中参与投票</p>\n</li>\n<li><p><strong>Observer：**可以接受客户端连接，将**写请求转发给leader**节点，但是不参与投票过程，只</strong>同步leader状态<strong>，主要存在目的就是</strong>为了提高读取效率**</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902105339340.png\" alt=\"image-20230902105339340\"></p>\n</li>\n</ul>\n</li>\n<li><p>组成 ZooKeeper 服务的服务器必须彼此了解。它们维护一个内存中的状态图像，以及持久存储中的事务日志和快照，只要<strong>大多数服务器可用（总数为奇数，超过半数正常即可），ZooKeeper 服务就可用</strong></p>\n<ul>\n<li>将 server 分为三种是为了<strong>避免太多的从节点参与过半写</strong>的过程，导致影响性能，如果要横向扩展的话，只需要增加 Observer 节点即可</li>\n<li>ZooKeeper 启动时，将从实例中选举一个 leader，<strong>Leader 负责处理数据更新</strong>等操作，每个Server都存储一份数据，大多数Server更新成功后即认为更新成功</li>\n<li>Zookeeper 的数据一致性是依靠<strong>ZAB协议</strong>完成的</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>ZAB协议</p>\n<ol>\n<li><p>ZAB协议（ZooKeeper Atomic Broadcast 原子广播） ：是为 ZooKeeper 设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性，主要包括两种模式</p>\n<ul>\n<li><p><strong>崩溃恢复</strong>：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并<strong>选举产生新的 Leader</strong> 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有<strong>过半的机器与该 Leader 服务器完成了状态同步</strong>之后，ZAB 协议<strong>就会退出恢复模式</strong>。剩下未同步完成的机器会继续同步，<strong>直到同步完成并加入集群后该节点的服务才可用</strong></p>\n</li>\n<li><p>消息广播：当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进人消息广播模式（Leader 服务器在负责进行消息广播）<strong>了。ZooKeeper 设计成</strong>只允许唯一的一个 Leader 服务器来进行事务请求的处理。Leader 服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非 Leader 服务器会首先将这个事务请求转发给 Leader服务器</p>\n<ul>\n<li>当一台同样遵守 ZAB 协议的服务器启动后加人到集群中时，那么新加人的服务器就会<strong>自觉地进人数据恢复模式，</strong>找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>如何选举Leader：两台以上ZK启动发起leader选举</p>\n<ul>\n<li><p>发起投票：<strong>每个 Server 发出一个投票</strong>。初始选举 ZK1 和 ZK2 都会将自己作为 Leader 服务器来进行投票，每次投票会包含所推举的服务器的(<strong>myid, ZXID</strong>)，此时 ZK1 的投票为(1, 0)，ZK2 的投票为(2, 0)，然后各自<strong>将这个投票发给集群中其他机器</strong></p>\n</li>\n<li><p>收到投票：集群的每个服务器收到投票后，首先<strong>判断</strong>该投票的<strong>有效性</strong>，如检查是否是本轮投票、是否来自 LOOKING 状态的服务器</p>\n</li>\n<li><p>处理投票：每个发起投票的服务器需要将别人的投票和自己的投票进行比较，规则如下:</p>\n<ul>\n<li>优先检查 ZXID：<strong>ZXID 比较大的服务器优先作为 Leader</strong></li>\n<li>其次检查myid：<strong>如果 ZXID 相同：</strong>那么就比较 myid。<strong>myid 较大的服务器作为Leader服务器</strong></li>\n</ul>\n</li>\n<li><p>统计投票：每次投票后，服务器都会统计投票信息，<strong>判断是否已经有过半机器接收到相同的投票信息</strong>，对于 ZK1、ZK2 而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出 ZK2 作为Leader</p>\n</li>\n<li><p>改变服务器状态：<strong>一旦确定了 Leader，每个服务器就会更新自己的状态</strong>，如果是Follower，那么就变更为 FOLLOWING，如果是 Leader，就变更为 LEADING。当新的 Zookeeper 节点 ZK3 启动时，发现已经有 Leader 了，不再选举，直接将直接的状态从 LOOKING 改为 FOLLOWING</p>\n</li>\n</ul>\n</li>\n<li><p>Leader挂了进入崩溃恢复如何选举Leader</p>\n<ul>\n<li><strong>变更状态</strong>。Leader 挂后，余下的非 Observer 服务器都会将自己的服务器状态变更为 <strong>LOOKING</strong>，然后开始进入 Leader 选举过程</li>\n<li>其余与启动时过程相同：发起投票、收到投票、处理投票<strong>、</strong>统计投票、改变服务器的状态</li>\n</ul>\n</li>\n<li><p>leader选举后如何同步数据</p>\n<ul>\n<li><p>ZXID：写数据由leader负责，leader会给每个请求分配一个zxid，放入到队列中依次执行，leader会记录执行完的请求的zxid。将队列中的最大的 ZXID 称为 <strong>maxZXID</strong>，最小的 ZXID 称为 <strong>minZXID，</strong>将 Observer 和 follower 中最新的 ZXID <strong>称为lastSyncZXID，将请求中的一些信息如请求头，请求体以及 ZXID 等信息封装到 proposal对象当中</strong></p>\n</li>\n<li><p>差异化同步（触发条件:minZXID &lt; lastSyncZXID &lt; maxZXID<strong>）</strong></p>\n<ul>\n<li>leader 向 Observer 和 follower 发送 <code>DIFF</code> 指令，之后就开始差异化同步</li>\n<li>然后把差异数据 提议 proposal 发送给 Observer 和 follower , Observer 和 follower 返回ACK表示已经完成了同步</li>\n<li>只要集群中过半的 Observer 和 follower 响应了 ACK 就发送一个 UPTODATE 命令</li>\n<li>leader 返回 ACK，同步流程结束</li>\n</ul>\n</li>\n<li><p>回滚同步（触发条件maxZXID &lt; lastSyncZXID<strong>）</strong></p>\n<ul>\n<li><code>直接回滚到 maxZXID</code></li>\n<li><strong>举个例子</strong>：a，b，c三台服务服务器 a是leader，此时队列里面最大的 ZXID 为100，a 收到请求，该 ZXID 为101，还没来得及发送同步数据 a 就挂了，b 变为leader，然后 a 恢复了，此时就需要 a 先将之前 ZXID 为101的数据回滚</li>\n</ul>\n</li>\n<li><p>回滚+差异化同步（触发条件:如果Leader刚生成一个proposal，还没有来得及发送出去，此时Leader宕机，重新选举之后作为Follower，但是新的Leader没有这个proposal数据<strong>）</strong></p>\n<ul>\n<li>Observer 和 follower 将数据回滚</li>\n<li>进行差异化同步</li>\n<li><strong>举个例子</strong>：a，b，c三台服务服务器 a是leader，此时队列里面最大的 ZXID 为100，a 收到请求，该 ZXID 为101，还没来得及发送同步数据 a 就挂了，b 变为leader，b 又处理了3个请求，则 b 队列中最大的 ZXID 为103，然后 a 恢复了，此时就需要 <code>a 先将之前 ZXID 为101的数据回滚，再进行同步</code></li>\n</ul>\n</li>\n<li><p>全量同步（触发条件<code>lastSyncZXID &lt; minZXID</code>）</p>\n<ul>\n<li>Leader服务器上没有缓存队列，并且lastSyncZXID!=maxZXID</li>\n<li><strong>同步过程</strong>：<code>leader 向 Observer 和 follower 发送SNAP命令，进行数据全量同步</code></li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>Wather监听机制</p>\n<ol>\n<li>What：client 会对某个 znode 注册一个 watcher 事件，当该 <strong>znode 发生变化</strong>时，这些 client 会<strong>收到 ZooKeeper 的通知</strong></li>\n<li>How<ul>\n<li><strong>服务注册</strong>：Provider 启动时，会向 zookeeper 服务端<strong>注册服务信息</strong>，也就是创建一个节点</li>\n<li><strong>服务发现</strong>：Consumer 启动时，根据自身配置的依赖服务信息，向 zookeeper 服务端获取注册的服务信息并<strong>设置 watch 监听</strong>，获取到注册的服务信息之后，将服务提供者的信息<strong>缓存在本地</strong>，并进行服务的调用</li>\n<li><strong>服务通知</strong>：一旦服务提供者因某种原因宕机不再提供服务之后，客户端与 zookeeper <strong>服务端断开</strong>连接，zookeeper 服务端上服务提供者对应服务<strong>节点会被删除</strong>，随后 zookeeper 服务端会<strong>异步向所有注册了该服务，且设置了 watch 监听的服务消费者</strong>发出节点被删除的通知，消费者根据<strong>收到的通知拉取最新服务列表</strong>，<strong>更新本地缓存</strong>的服务列表</li>\n</ul>\n</li>\n<li>特性<ul>\n<li>一次性：一旦一个Wather<strong>触发</strong>之后，Zookeeper<strong>就会</strong>将它从存储中<strong>移除</strong>，<strong>如果还要继续监听</strong>这个节点，就<strong>需要</strong>我们在客户端的监听回调中，<strong>再次</strong>对节点的监听watch事件<strong>设置为True</strong>。否则客户端只能接收到一次该节点的变更通知</li>\n<li>客户端串行：客户端的<strong>Wather回调处理是串行同步</strong>的过程，不会因为一个Wather的逻辑阻塞整个客户端</li>\n<li>轻量：Wather通知的单位是WathedEvent，<strong>只包含通知状态、事件类型和节点路径，不包含具体的事件内容</strong>，具体的时间内容需要客户端主动去重新获取数据</li>\n<li>异步: Zookeeper服务器<strong>发送watcher的通知事件到客户端是异步</strong>的，不能期望能够监控到节点每次的变化，Zookeeper只能保证最终的一致性，而无法保证强一致性</li>\n</ul>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"Netty\"><a href=\"#Netty\" class=\"headerlink\" title=\"Netty\"></a>Netty</h1><ol>\n<li><p>Netty：基于NIO实现的高性能、异步事件驱动的网络编程框架，Dubbo和RocketMQ都基于Netty实现，其高性能体现在如下方面</p>\n<ol>\n<li><p><strong>异步非阻塞 <code>I/O</code> 模型</strong>：<code>Netty</code> 使用基于<code>NIO</code>的异步非阻塞 <code>I/O</code> 模型，可以大大提高网络通信效率，减少线程的阻塞等待时间，从而提高应用程序的响应速度和吞吐量</p>\n</li>\n<li><p><strong>零拷贝技术</strong>：<code>Netty</code> 支持零拷贝技术，可以避免数据在内核和用户空间之间的多次复制，减少了数据拷贝的次数，从而提高了数据传输的效率和性能</p>\n</li>\n<li><p><strong>线程模型优化</strong>：Netty的线程模型非常灵活，可以根据不同的业务场景选择不同的线程模型。例如，对于低延迟和高吞吐量的场景，可以选择 Reactor线程模型，对于 I/O操作比较简单的场景，可以选择单线程模型</p>\n</li>\n</ol>\n</li>\n<li><p>内存池技术：Netty提供了一套基于内存池技术的 ByteBuf缓冲区，可以重用已经分配的内存空间，减少内存的分配和回收次数，提高内存使用效率</p>\n<ul>\n<li><p>堆内存：ByteBuf以普通的字节数组为基础，在 JVM堆上分配内存。这种方式适用于小型数据的传输，如传输的是文本、XML等数据</p>\n<ul>\n<li>采用了类似于<code>JVM</code> 的分代内存管理机制，将缓冲区分为三种类型<strong>：堆缓冲区、直接缓冲区、复合缓冲区</strong>。Netty 会根据不同的使用场景和内存需求来决定使用哪种类型的缓冲区，从而提高内存利用率</li>\n<li>直接内存：<code>ByteBuf</code> 使用操作系统的堆外内存，由操作系统分配和回收内存。这种方式适用于大型数据的传输，如传输的是音视频、大型图片等数据</li>\n</ul>\n</li>\n<li><p>处理器链式调用：<code>Netty</code> 的 <code>ChannelHandler</code> 可以按照一定的顺序组成一个处理器链，当事件发生时，会按照处理器链的顺序依次调用处理器，从而实现对事件的处理。这种处理方式比传统的多线程处理方式更加高效，减少了线程上下文切换和锁竞争等问题</p>\n</li>\n</ul>\n</li>\n<li><p>核心组件</p>\n<ul>\n<li><p>Channel：用于网络通信的通道，可以理解为<code>Java NIO</code>中的<code>SocketChannel</code></p>\n<ul>\n<li>每个<code>Channel</code>都与一个<code>EventLoop</code>关联，而一个<code>EventLoop</code>可以关联多个<code>Channel</code>，Channel上有事件发生（数据可读/可写）时，加入到<code>EventLoop</code>的任务队列，通过异步I/O和事件驱动的方式处理，保证在处理每个<code>Channel</code>的事件时不会被阻塞</li>\n<li>每个<code>Channel</code>都有一个与之关联的<code>ChannelPipeline</code>，用于处理该<code>Channel</code>上的事件和请求。<code>ChannelPipeline</code>是一种基于事件驱动的处理机制，它由多个处理器（<code>Handler</code>）组成，每个处理器负责处理一个或多个事件类型，将事件转换为下一个处理器所需的数据格式，直到到达最后一个处理器或者被中途拦截（异常）</li>\n</ul>\n</li>\n<li><p>ChannelFuture：异步操作的结果，可以添加监听器以便在操作完成时得到通知</p>\n<ul>\n<li><code>ChannelFuture</code>用于在异步操作完成后通知应用程序结果。在异步操作执行后，<code>Netty</code>将一个<code>ChannelFuture</code>对象返回给调用方。调用方可以通过添加一个回调（<code>ChannelFutureListener</code>）来处理结果</li>\n<li><code>ChannelFuture</code>还提供了许多有用的方法，<strong>如检查操作是否成功、等待操作完成、添加监听器等</strong></li>\n</ul>\n</li>\n<li><p>EventLoop：事件循环器，用于处理一个或多个<code>Channel</code>的<code>I/O</code>事件和请求，是一个不断循环的<code>I/O</code>线程。<code>Netty</code>的<code>I/O</code>操作都是异步非阻塞的，它们由<code>EventLoop</code>处理并以事件的方式触发回调函数</p>\n</li>\n<li><p>EventLoopGroup：由一个或多个<code>EventLoop</code>组成的组，用于处理所有的<code>Channel</code>的<code>I/O</code>操作，可以将其看作是一个线程池</p>\n</li>\n<li><p>ChannelHandler：用于处理<code>Channel</code>上的<code>I/O</code>事件和请求（用于处理入站和出站数据流），包括编码、解码、业务逻辑等，为了将网络协议的细节与应用程序的逻辑分离开，可以理解为<code>NIO</code>中的<code>ChannelHandler</code>，通过实现ChannelHandler接口的以下方法来实现</p>\n<ul>\n<li><code>channelRead(ChannelHandlerContext ctx, Object msg)</code>: 处理接收到的数据，这个方法通常会被用于解码数据并将其转换为实际的业务对象</li>\n<li><code>channelReadComplete(ChannelHandlerContext ctx)</code>: 读取数据完成时被调用，可以用于向远程节点发送数据</li>\n<li><code>exceptionCaught(ChannelHandlerContext ctx, Throwable cause)</code>: 发生异常时被调用，可以在这个方法中处理异常或关闭连接</li>\n<li><code>channelActive(ChannelHandlerContext ctx)</code>: 当连接建立时被调用</li>\n<li><code>channelInactive(ChannelHandlerContext ctx)</code>: 当连接关闭时被调用</li>\n</ul>\n</li>\n<li><p>ChannelPipeline：由一组<code>ChannelHandler</code>组成的管道，用于处理<code>Channel</code>上的所有<code>I/O</code> 事件和请求，<code>Netty</code>中的数据处理通常是通过将一个数据包装成一个<code>ByteBuf</code>对象，并且通过一个 <code>ChannelPipeline</code>来传递处理，以达到业务逻辑与网络通信的解耦，工作方式如下：</p>\n<ul>\n<li>入站（<code>Inbound</code>）事件：由<code>Channel</code>接收到的事件，例如读取到新的数据、连接建立完成等等。入站事件将从<code>ChannelPipeline</code>的第一个<code>InboundHandler</code>开始流动，直到最后一个<code>InboundHandler</code></li>\n<li>出站（<code>Outbound</code>）事件：由<code>Channel</code>发送出去的事件，例如向对端发送数据、关闭连接等等。出站事件将从<code>ChannelPipeline</code>的最后一个<code>OutboundHandler</code>开始流动，直到第一个<code>OutboundHandler</code></li>\n<li><code>ChannelHandlerContext</code>：表示处理器和<code>ChannelPipeline</code>之间的关联关系。每个<code>ChannelHandler</code>都有一个<code>ChannelHandlerContext</code>，通过该对象可以实现在<code>ChannelPipeline</code>中的事件流中向前或向后传递事件，也可以通过该对象访问所属的<code>Channel</code>、所在的<code>ChannelPipeline</code>和其他<code>ChannelHandler</code>等。在处理<code>I/O</code>事件时，<code>Netty</code>会将<code>I/O</code>事件转发给与该事件相应的<code>ChannelHandlerContext</code>，该上下文对象可以使<code>Handler</code>访问与该事件相关的任何信息，也可以在管道中转发事件</li>\n</ul>\n</li>\n<li><p>ByteBuf：<code>Netty</code>提供的字节容器，可以对字节进行高效操作，包括读写、查找等，与 <code>Java NIO</code> 的 <code>ByteBuffer</code> 相比有以下优势</p>\n<ul>\n<li><p>容量可扩展：<code>ByteBuf</code>的容量可以动态扩展，而 <code>ByteBuffer</code> 的容量是固定的</p>\n</li>\n<li><p>内存分配：<code>ByteBuf</code> 内部采用了内存池的方式，可以有效地减少内存分配和释放的开销</p>\n</li>\n<li><p>读写操作：<code>ByteBuf</code> 提供了多个读写指针，可以方便地读写字节数据</p>\n</li>\n<li><p>零拷贝：<code>ByteBuf</code> 支持零拷贝技术，可以减少数据复制的次数</p>\n</li>\n<li><p>使用示例</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">ByteBuf buffer &#x3D; Unpooled.buffer(10);\nbuffer.writeBytes(&quot;hello&quot;.getBytes());\n\nwhile (buffer.isReadable()) &#123;\n  System.out.print((char) buffer.readByte());\n&#125;</code></pre></li>\n</ul>\n</li>\n<li><p>Codec：用于在<code>ChannelPipeline</code>中进行数据编码和解码的组件，如字符串编解码器、对象序列化编解码器等，将二进制数据与 <code>Java</code> 对象之间进行编码和解码的组件，常用的Codec如下</p>\n<ul>\n<li><code>ByteToMessageCodec</code>：将字节流解码为 <code>Java</code> 对象，同时也可以将 <code>Java</code> 对象编码为字节流。可以用于处理自定义协议的消息解析和封装。</li>\n<li><code>MessageToByteEncoder</code>：将 <code>Java</code> 对象编码为字节流。通常用于发送消息时将消息转换为二进制数据。</li>\n<li><code>ByteToMessageDecoder</code>：将字节流解码为 <code>Java</code> 对象。通常用于接收到数据后进行解码。</li>\n<li><code>StringEncoder 和 StringDecoder</code>：分别将字符串编码为字节流和将字节流解码为字符串。</li>\n<li><code>LengthFieldPrepender 和 LengthFieldBasedFrameDecoder</code>：用于处理 <code>TCP</code> 粘包和拆包问题。</li>\n<li><code>ObjectDecoder和ObjectEncoder</code>：将<code>Java</code>对象序列化为字节数据，并将字节数据反序列化为<code>Java</code>对象。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>线程模型（基于事件驱动的Reactor模型）</p>\n<ol>\n<li><p>Reactor模型</p>\n<ul>\n<li><p>单线程模型：所有IO操作都由同一个NIO线程处理，虽然这种方式并不适合高并发的场景，但是它具有简单、快速的优点，适用于处理<code>I/O</code>操作非常快速的场景，例如传输小文件等</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902105748423.png\" alt=\"image-20230902105748423\"></p>\n</li>\n<li><p>多线程模型：所有的<code>I/O</code>操作都由一组线程来执行，其中一个线程负责监听客户端的连接请求，其他NIO线程负责处理<code>I/O</code>操作。这种方式可以支持高并发，但是线程上下文切换的开销较大，适用于处理<code>I/O</code>操作较为耗时的场景</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902105813701.png\" alt=\"image-20230902105813701\"></p>\n</li>\n<li><p>主从多线程模型：，所有的<code>I/O</code>操作都由一组<code>NIO</code>线程来执行，其中一组NIO线程负责接受请求，一组NIO线程负责处理IO操作。这种方式将接受连接和处理<code>I/O</code>操作分开，避免了线程上下文切换的开销，同时又能支持高并发，适用于处理I/O操作耗时较长的场景</p>\n<p><img src=\"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/image-20230902105834011.png\" alt=\"image-20230902105834011\"></p>\n</li>\n</ul>\n</li>\n<li><p>为每个连接都分配了一个单独的<code>EventLoop</code>线程，负责处理所有与该连接相关的事件，包括数据传输、握手和关闭等。多个连接还可以共享同一个<code>EventLoop</code>线程，从而减少线程的创建和销毁开销，提高资源利用率</p>\n</li>\n<li><p>可以使用不同的<code>EventLoopGroup</code>实现不同的线程模型，如<strong>单线程模型、多线程模型和主从线程模型</strong>等。同时，<strong>还可以设置不同的线程池参数，如线程数、任务队列大小、线程优先级</strong>等，以调整线程池的工作负载和性能表现</p>\n<ol>\n<li><p>单线程模型</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">&#x2F;&#x2F;eventGroup 既负责处理客户端连接，又负责具体的处理\nEventLoopGroup eventGroup &#x3D; new NioEventLoopGroup(1);</code></pre></li>\n<li><p>多线程模型</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">&#x2F;&#x2F; bossGroup处理客户端连接, workerGroup负责具体的处理\nEventLoopGroup bossGroup &#x3D; new NioEventLoopGroup(1);\nEventLoopGroup workerGroup &#x3D; new NioEventLoopGroup();</code></pre></li>\n<li><p>主从多线程模型</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">&#x2F;&#x2F; bossGroup接收客户端连接, workerGroup负责具体的处理\nEventLoopGroup bossGroup &#x3D; new NioEventLoopGroup();\nEventLoopGroup workerGroup &#x3D; new NioEventLoopGroup();</code></pre></li>\n</ol>\n</li>\n<li><p>在实际使用中，<strong>还可以通过优化网络协议、数据结构、业务逻辑等方面来提高Netty的性能</strong>。例如，可以使用零拷贝技术避免数据拷贝，使用内存池减少内存分配和回收的开销，避免使用阻塞IO和同步操作等，从而提高应用的吞吐量和性能表现</p>\n</li>\n</ol>\n</li>\n<li><p>零拷贝</p>\n<ul>\n<li>可以避免在数据传输过程中对数据的多次拷贝操作，如数据在内核空间和用户空间之间的拷贝次数，从而提高数据传输的效率和性能</li>\n<li>实现：<code>Netty</code> 通过使用 <code>Direct Memory</code> 和 <code>FileChannel</code> 的方式实现零拷贝。当应用程序将数据写入 <code>Channel</code> 时，<code>Netty</code> 会将数据直接写入到内存缓冲区中，然后通过操作系统提供的 <code>sendfile</code> 或者 <code>writev</code> 等零拷贝技术，将数据从内存缓冲区中传输到网络中，从而避免了中间的多次拷贝操作</li>\n</ul>\n</li>\n<li><p>客户端与服务端之间的网络连接机制</p>\n<ul>\n<li><p><strong>长连接：</strong>通过 <code>Channel</code> 的 <code>keepalive</code> 选项来保持连接的状态。当启用了 <code>keepalive</code> 选项后，客户端和服务器之间的连接将会自动保持一段时间，如果在这段时间内没有数据交换，客户端和服务器之间的连接将会被关闭，通过以下四种方式来实现</p>\n</li>\n<li><p><strong>心跳机制：</strong>心跳机制可以通过定期向对方发送心跳消息，来检测连接是否正常。如果在一段时间内没有收到心跳消息，就认为连接已经断开，并进行重新连接。Netty提供了一个 IdleStateHandler类，可以用来实现心跳机制。IdleStateHandler可以设置多个超时时间，当连接空闲时间超过设定的时间时，会触发一个事件，可以在事件处理方法中进行相应的处理，比如发送心跳消息</p>\n<ol>\n<li><p>定义心跳消息的类型</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">public class HeartbeatMessage implements Serializable &#123;\n    &#x2F;&#x2F; ...\n&#125;</code></pre></li>\n<li><p>在客户端和服务端的<code>ChannelPipeline</code>中添加<code>IdleStateHandler</code>，用于触发定时任务</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">pipeline.addLast(new IdleStateHandler(0, 0, 60, TimeUnit.SECONDS));</code></pre></li>\n<li><p>在客户端和服务端的业务逻辑处理器中，重写<code>userEventTriggered</code>方法，在触发定时任务时发送心跳包</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">public class MyServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123;\n    @Override\n public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123;\n        if (evt instanceof IdleStateEvent) &#123;\n         IdleStateEvent event &#x3D; (IdleStateEvent) evt;\n            if (event.state() &#x3D;&#x3D; IdleState.READER_IDLE) &#123;\n             &#x2F;&#x2F; 读空闲，发送心跳包\n                ctx.writeAndFlush(new HeartbeatMessage());\n            &#125;\n        &#125; else &#123;\n            super.userEventTriggered(ctx, evt);\n        &#125;\n &#125;\n&#125;</code></pre></li>\n<li><p>在客户端和服务端的业务逻辑处理器中，重写<code>channelRead</code>方法，接收并处理心跳包</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">        public class MyClientHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123;\n            @Override\n               protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception &#123;\n                   if (msg instanceof HeartbeatMessage) &#123;\n                       &#x2F;&#x2F; 收到心跳包，不做处理\n                       return;\n                   &#125;\n                   &#x2F;&#x2F; 处理其他消息\n                   &#x2F;&#x2F; ...\n               &#125;\n           &#125;\n     \n   - **断线重连机制**：在网络不稳定的情况下，连接可能会不可避免地断开。为了避免因为网络异常导致应用程序不能正常工作，可以实现断线重连机制，定期检查连接状态，并在连接断开时尝试重新连接。&#96;Netty&#96;提供了&#96;ChannelFutureListener&#96;接口和&#96;ChannelFuture&#96;对象，可以方便地实现断线重连机制\n   \n   - **基于HTTP&#x2F;1.1协议的长连接**：&#96;HTTP&#x2F;1.1&#96;协议支持长连接，可以在一个&#96;TCP&#96;连接上多次发送请求和响应。在&#96;Netty&#96;中，可以使用&#96;HttpClientCodec&#96;和&#96;HttpObjectAggregator&#96;处理器，实现基于&#96;HTTP&#x2F;1.1&#96;协议的长连接\n   \n   - **WebSocket协议**：&#96;WebSocket&#96;协议也支持长连接，可以在一个&#96;TCP&#96;连接上双向通信，实现实时数据交换。在&#96;Netty&#96;中，可以使用&#96;WebSocketServerProtocolHandler&#96;和&#96;WebSocketClientProtocolHandler&#96;处理器，实现&#96;WebSocket&#96;协议的长连接\n   \n6. **服务端和客户端的启动过程**\n\n   1. 创建 &#96;EventLoopGroup&#96; 对象。&#96;EventLoopGroup&#96; 是&#96;Netty&#96;的核心组件之一，它用于管理和调度事件的处理。&#96;Netty&#96; 通过&#96;EventLoopGroup&#96;来创建多个&#96;EventLoop&#96;对象，并将每个 &#96;EventLoop&#96; 与一个线程绑定。在服务端中，一般会创建两个 &#96;EventLoopGroup&#96; 对象，分别用于接收客户端的连接请求和处理客户端的数据\n\n   2. 创建 &#96;ServerBootstrap&#96; 或 &#96;Bootstrap&#96; 对象。&#96;ServerBootstrap&#96; 和 &#96;Bootstrap&#96; 是 &#96;Netty&#96; 提供的服务端和客户端启动器，它们封装了启动过程中的各种参数和配置，方便使用者进行设置。在创建 &#96;ServerBootstrap&#96; 或 &#96;Bootstrap&#96; 对象时，需要指定相应的 &#96;EventLoopGroup&#96; 对象，并进行一些基本的配置，比如传输协议、端口号、处理器等\n\n      - Bootstrap是一个用于启动和配置Netty客户端和服务器的工具类。它提供了一组简单易用的方法，用于设置服务器或客户端的选项和属性，以及为ChannelPipeline配置handler，以处理传入或传出的数据，使得创建和配置Netty应用程序变得更加容易。主要有两个作用\n        - 作为&#96;Netty&#96;服务器启动的入口点：通过&#96;Bootstrap&#96;启动一个&#96;Netty&#96;服务器，可以在指定的端口上监听传入的连接，并且可以设置服务器的选项和属性\n        - 作为&#96;Netty&#96;客户端启动的入口点：通过&#96;Bootstrap&#96;启动一个&#96;Netty&#96;客户端，可以连接到远程服务器，并且可以设置客户端的选项和属性\n\n   3. 配置&#96;Channel&#96;的参数。&#96;Channel&#96; 是&#96;Netty&#96;中的一个抽象概念，它代表了一个网络连接。在启动过程中，需要对 &#96;Channel&#96; 的一些参数进行配置，比如传输协议、缓冲区大小、心跳检测等\n\n   4. 绑定 &#96;ChannelHandler&#96;。&#96;ChannelHandler&#96; 是 &#96;Netty&#96; 中用于处理事件的组件，它可以处理客户端的连接请求、接收客户端的数据、发送数据给客户端等。在启动过程中，需要将 &#96;ChannelHandler&#96; 绑定到相应的 &#96;Channel&#96; 上，以便处理相应的事件\n   \n   5. 启动服务端或客户端。在完成以上配置后，就可以启动服务端或客户端了。在启动过程中，会创建相应的 &#96;Channel&#96;，并对其进行一些基本的初始化，比如注册监听器、绑定端口等。启动完成后，就可以开始接收客户端的请求或向服务器发送数据了\n\n   6. 服务端代码\n\n      ![image-20230902110031766](https:&#x2F;&#x2F;macro---oss2.oss-cn-beijing.aliyuncs.com&#x2F;img&#x2F;image-20230902110031766.png)\n   \n   7. 客户端代码\n\n      ![image-20230902110047958](https:&#x2F;&#x2F;macro---oss2.oss-cn-beijing.aliyuncs.com&#x2F;img&#x2F;image-20230902110047958.png)\n\n7. IO模型（基于异步的NIO）：一个线程可以处理多个连接的读写事件，Netty的NIO与传统NIO相比有以下不同点\n\n   - &#96;Netty&#96;使用了&#96;Reactor&#96;模式，将&#96;IO&#96;事件分发给对应的&#96;Handler&#96;处理，使得应用程序可以更方便地处理网络事件\n   - &#96;Netty&#96;使用了多线程模型，将&#96;Handler&#96;的处理逻辑和&#96;IO&#96;线程分离，避免了&#96;IO&#96;线程被阻塞的情况\n   - &#96;Netty&#96;支持多种&#96;Channel&#96;类型，可以根据应用场景选择不同的&#96;Channel&#96;类型，如&#96;NIO、EPoll、OIO&#96;等\n\n8. **TCP粘包&#x2F;拆包：**在&#96;TCP&#96;传输过程中，由于&#96;TCP&#96;并不了解上层应用协议的消息边界，**会将多个小消息组合成一个大消息，或者将一个大消息拆分成多个小消息发送**\n\n   1. 解决方式一（**消息定长）**：将消息固定长度发送，例如每个消息都是固定的&#96;100&#96;字节。在接收端，根据固定长度对消息进行拆分\n\n      &#96;&#96;&#96;java\n      &#x2F;&#x2F; 编码器，将消息的长度固定为100字节\n      pipeline.addLast(&quot;frameEncoder&quot;, new LengthFieldPrepender(2));\n      pipeline.addLast(&quot;messageEncoder&quot;, new StringEncoder(CharsetUtil.UTF_8));\n      &#x2F;&#x2F; 解码器，根据固定长度对消息进行拆分\n      pipeline.addLast(&quot;frameDecoder&quot;, new LengthFieldBasedFrameDecoder(100, 0, 2, 0, 2));\n      pipeline.addLast(&quot;messageDecoder&quot;, new StringDecoder(CharsetUtil.UTF_8));</code></pre></li>\n</ol>\n</li>\n</ul>\n<ol start=\"2\">\n<li><p>解决方式二（<strong>消息分隔符）</strong>：将消息以特定的分隔符分隔开，例如以”<code>\\\\r\\\\n</code>“作为分隔符。在接收端，根据分隔符对消息进行拆分</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">&#x2F;&#x2F; 编码器，以&quot;\\\\r\\\\n&quot;作为消息分隔符\npipeline.addLast(&quot;frameEncoder&quot;, new DelimiterBasedFrameEncoder(&quot;\\\\r\\\\n&quot;));\npipeline.addLast(&quot;messageEncoder&quot;, new StringEncoder(CharsetUtil.UTF_8));\n&#x2F;&#x2F; 解码器，根据&quot;\\\\r\\\\n&quot;对消息进行拆分\npipeline.addLast(&quot;frameDecoder&quot;, new DelimiterBasedFrameDecoder(1024, Delimiters.lineDelimiter()));\npipeline.addLast(&quot;messageDecoder&quot;, new StringDecoder(CharsetUtil.UTF_8));</code></pre></li>\n<li><p>解决方式三（<strong>消息头部加长度字段）</strong>：在消息的头部加上表示消息长度的字段，在发送端发送消息时先发送消息长度，再发送消息内容。在接收端，先读取消息头部的长度字段，再根据长度读取消息内容</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">&#x2F;&#x2F; 编码器，将消息的长度加入消息头部\npipeline.addLast(&quot;frameEncoder&quot;, new LengthFieldPrepender(2));\npipeline.addLast(&quot;messageEncoder&quot;, new StringEncoder(CharsetUtil.UTF_8));\n&#x2F;&#x2F; 解码器，先读取消息头部的长度字段，再根据长度读取消息内容\npipeline.addLast(&quot;frameDecoder&quot;, new LengthFieldBasedFrameDecoder(1024, 0, 2, 0, 2));\npipeline.addLast(&quot;messageDecoder&quot;, new StringDecoder(CharsetUtil.UTF_8));</code></pre></li>\n</ol>\n</li>\n<li><p>常用Handler</p>\n<ul>\n<li>实现WebSocket协议：<code>WebSocketServerProtocolHandler</code> 是一个 <code>ChannelHandler</code>，可以将 <code>HTTP</code> 升级为 <code>WebSocket</code> 并处理 <code>WebSocket</code> 帧</li>\n<li>大文件传输：<code>ChunkedWriteHandler</code>是一个编码器，可以将大文件切分成多个<code>Chunk</code>，并将它们以<code>ChunkedData</code>的形式写入管道，这样就可以避免一次性将整个文件读入内存，降低内存占用</li>\n<li>SSL/TLS加密传输：通过 <code>SSLHandler</code>来进行处理。通常情况下，<code>SSLHandler</code> 需要在 <code>ChannelPipeline</code> 中作为最后一个<code>handler</code>添加</li>\n</ul>\n</li>\n<li><p>Netty和Tomcat的区别</p>\n<ul>\n<li>底层网络通信模型不同：<code>Tomcat</code> 是基于阻塞的 <code>BIO（Blocking I/O）</code>模型实现的，而 <code>Netty</code> 是基于 <code>NIO（Non-Blocking I/O）</code>模型实现的</li>\n<li>线程模型不同：<code>Tomcat</code> 使用传统的多线程模型，每个请求都会分配一个线程，而 <code>Netty</code> 使用 <code>EventLoop</code> 线程模型，每个 <code>EventLoop</code> 负责处理多个连接，通过线程池管理 <code>EventLoop</code></li>\n<li>协议支持不同：<code>Tomcat</code> 内置支持 <code>HTTP 和 HTTPS</code> 协议，而 <code>Netty</code> 不仅支持 <code>HTTP 和 HTTPS</code> 协议，还支持 <code>TCP、UDP 和 WebSocket</code> 等多种协议</li>\n<li>代码复杂度不同：由于<code>Tomcat</code>支持的功能比较全面，所以其代码相对较为复杂，而 <code>Netty</code> 的代码相对比较简洁、精简</li>\n<li>应用场景不同：<code>Tomcat</code>适合于处理比较传统的 <code>Web</code> 应用程序，如传统的 <code>MVC</code> 模式<code>Web</code>应用程序；而 <code>Netty</code>更适合于高性能、低延迟的网络应用程序，如游戏服务器、即时通讯服务器等</li>\n</ul>\n</li>\n<li><p>为什么不直接用NIO：NIO编程复杂、不能简单的解决断点重连、包丢失、粘包等问题</p>\n<ul>\n<li>Netty自带编码、解码器用来解决粘包、拆包问题</li>\n<li>自带各种协议栈，支持多种协议，如FTP、HTTP、SSL/TLS以及二进制和基于文本的传统协议</li>\n<li>比直接用Java的核心API有更高的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制</li>\n</ul>\n</li>\n</ol>\n","feature":true,"text":"Dubbo RPC RPC（Remote Procedure Call）全称为远程过程调用，用于解决不同服务器之间两个方法的互相调用问题，并且通过网络编程来传递方法调用所需要的参数，通过RPC来简化底层网络编程（如TCP连接的建立）、规划化参数序列化反序列化等问题，使得这个过程就...","link":"","photos":[],"count_time":{"symbolsCount":"26k","symbolsTime":"23 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Dubbo\"><span class=\"toc-text\">Dubbo</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Zookeeper\"><span class=\"toc-text\">Zookeeper</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Netty\"><span class=\"toc-text\">Netty</span></a></li></ol>","author":{"name":"Dajunnnnnn","slug":"blog-author","avatar":"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/dajunnnnnn_psychedelic_eagle_neon_colors_comic_illustration_1ca2dde3-db58-4c04-b835-42e21feffbe8.PNG","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"MQ","uid":"f8ef4e10e9f952ff4ed3590ecc45cea3","slug":"MQ","date":"2023-08-07T05:45:26.000Z","updated":"2023-09-24T12:17:24.475Z","comments":true,"path":"api/articles/MQ.json","keywords":null,"cover":[],"text":"MQ 中间件（Middleware）：是一类提供系统软件和应用软件之间连接、便于软件各部件之间的沟通的软件，应用软件可以借助中间件在不同的技术架构之间共享信息与资源。常用中间件有消息队列、RPC 框架、分布式组件（分布式事务、分布式Session、分布式Id）、HTTP 服务器（...","link":"","photos":[],"count_time":{"symbolsCount":"21k","symbolsTime":"19 mins."},"categories":[],"tags":[],"author":{"name":"Dajunnnnnn","slug":"blog-author","avatar":"https://macro---oss2.oss-cn-beijing.aliyuncs.com/img/dajunnnnnn_psychedelic_eagle_neon_colors_comic_illustration_1ca2dde3-db58-4c04-b835-42e21feffbe8.PNG","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}