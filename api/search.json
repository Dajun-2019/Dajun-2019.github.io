[{"id":"a978a5e93d8e6628e9f4ee713be55be8","title":"Redis","content":"Redis\n\n\n\n\n\n\n\n\nRedis是一个高性能（内存+Reactor+优化的数据结构）的开源键值数据库，其value支持丰富的数据类型（string、hash、set、list、zset「有序集合」），具有数据可持久化、支持master-slave备份、读写性能高（MySQL的QPS大概1w左右，Redis读11w次/s，写8w次/s）等特点，其单个操作是原子性的，多个连续操作支持事务\n1.高性能1.数据结构\n数据类型及其应用\n\n\n\n类型\n简介\n特性\n场景\n大小\n底层数据结构\n\n\n\nString\n二进制安全\n可以包含任何数据（字符串、整数、浮点数、图片的base64编码、序列化后的对象）\n1.存储数据；2.计数（单位时间请求数，单位时间访问数）\n一个键最大能存储 512MB\nSDS（简单动态字符串）\n\n\nHash\n键值对集合,即编程语言中的Map类型\n适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去)\n1.存储、读取、修改用户属性；2.对象数据存储（用户信息、文章信息、购物车信息）\n每个 hash 可以存储 2^32 -1 键值对（4294967295）\nLinkedLIst ZipList\n\n\nList\n链表(双向链表)，按照插入顺序排序\n增删快，提供了操作某一段元素的API（没法用来做排行榜，分页显示时会有串行的问题，使用Sorted Set的score可以解决）\n1、最新消息排行等功能(比如朋友圈的时间线) 2、消息队列\n列表最多可存储 2^32 - 1 元素（4294967295）\nZipList HashTable；3.2后使用QuickList\n\n\nSet\n哈希表实现，元素不重复，可以求交集、并集、差集\n1、添加、删除、查找的复杂度都是O(1) ；2、为集合提供了求交集、并集、差集等操作；3、数据量大时可以选择一个从库\n1、共同好友 2、利用唯一性，统计访问网站的所有独立ip 3、好友推荐时，根据tag求交集，大于某个阈值就可以推荐\n集合中最大的成员数为 2^32 - 1（4294967295）\nZipList Intset\n\n\nSorted Set\n将Set中的元素增加一个权重参数score,元素按score有序排列\n数据插入集合时，已经进行天然排序。类似于Set，但是多了一个权重参数score，使得集合中的元素能够按score进行有序排列，还可以通过score的范围来获取元素的列表\n1、排行榜 2、带权重的消息队列\n—\nZipList SkipList\n\n\nBitmap\n位存储，支持按位与、或、异或\n存储连续的二进制数字，可以看成是存储0/1的数组，数组下标称为offset（活跃用户统计）\n1、用来做二值统计（元素的取值只有0和1），如签到统计\nGETBIT、SETBIT、BITCOUNT\nString（底层为二进制字节数组）\n\n\nHyperLogLogs\n基数统计，元素数量多时仍可保证消耗的空间是固定的\n基数计数概率算法为了节省内存并不会直接存储元数据，而是通过一定的概率统计方法预估基数值（集合中包含元素的个数）\n主要用于数据量巨大的计数场景（热门网站每日访问ip数统计），只存估计值\nHyperLogLogs只需要12KB内存，可以计算接近 2^64 个元素的基数\n基于概率进行统计，给出的结果有偏差\n\n\nGeospatial\n地理位置\n基于Sorted Set实现，将经纬度信息通过GeoHash算法转换成一个整数，将这个整数作为Sorted Set的score使用（实现附近的人功能）\n主要用于存储地理位置信息\nGEOADD、GEORADIUS（根据输入的经纬度，查找以这个经纬度为中心的一定范围内的其他元素）\nSorted Set + GeoHash编码（二分区间、区间编码）\n\n\n\nHash 类型底层结构什么时候使用压缩列表\n\nHash 集合中写入的元素个数超过了hash-max-ziplist-entries，或者写入的单个元素大小超过了hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表\n一旦从压缩列表转为了哈希表，Hash 类型就会一直用哈希表进行保存，而不会再转回压缩列表了\n\n\n补充数据结构\n\n\n\n类型\n简介\n特性\nAPI\n底层原理\n\n\n\nRedisTimeSeries\n记录时间序列数据\n支持直接在Redis实例上进行聚合计算（求平均、最值、和），其它方案都需要传输数据到客户端上进行聚合计算\nTS.CREATE；TS.ADD；TS.GET；TS.MGET；TS.RANGE（av g、max/min、sum）\nRedis的扩展模块，使用前需要编译成动态链接库 redistimeseries.so，再使用 loadmodule 命令进行加载\n\n\nStreams\n专门为消息队列设计的数据类型\n消息格式是键值对形式，插入的每一条消息自动生成一个全局唯一ID，读取时可以指定读取起始位置。支持创建消费组\nXADD、XREAD、XREADGROUP、SPENDING、XACK\n自动使用内部队列留存消费者读取的消息直到消费者使用SACK命令，重启时使用命令XPENDING继续处理\n\n\n\n自定义数据类型\n\nRedisObject\ntype：表示值的类型，涵盖了前面学习的五大基本类型\nencoding：是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等\nlru：记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对\nrefcount：记录了对象的引用计数\n*ptr：是指向数据的指针，借助*ptr指针，就可以指向不同的数据类型\n\n\n定义一个新的数据类型的步骤（未完待续）\n\n\n\n\n数据结构对应命令\n\n命令行方式（https://www.runoob.com/redis/redis-tutorial.html、https://redis.io/commands/）\n\n\n\n类型\n命令\n\n\n\nstring\nSET、GET、EXIST、STRLEN、DEL、MSET、MGET、INCR、DECR、EXPIRE、SETNX、TTL\n\n\nlist\nLPUSH、LPOP、RPUSH、RPOP、LRANGE（实现分页）、LLEN\n\n\nhash\nHSET、HSETNX、HMSET、HGET、HMGET、HGETALL（所有）、HEXIXTS、HDEL、HLEN、HINCRBY（增加多少）\n\n\nset\nSADD、SMEMBERS（内容）、SCARD（数量）、SISMEMBER（有无）、求交/并/差集（SINTER、SUNION、SDIFF）、SPOP key count、SRANDMEMBER key count\n\n\nzset\nZADD、ZCARD、ZSCORE、ZINTERSTORE（一共三个）、ZRANGE、ZREVRANGE、ZREVRANK\n\n\nkey\nSET key value、DEL key、EXISTS key（seconds）、TTL key、TYPE key\n\n\npub/sub\nPUBLISH channel message、SUBSCRIBE channel、UNSUBSCRIBE channel\n\n\n事务\nMULTI（开始）、EXEC（执行）、DISCARD（取消）、WATCH、UNWATCH\n\n\n其它\nPING、PONG、QUIT、AUTH password、INFO、FLUSHALL、BGSAVE、BGREWRITEAOF\n\n\n\n使用批量操作减少网络传输\n\nRedis每条命令都会通过网络与服务器交互，可以使用批量操作命令（mget、hmget），但是在Redis Cluster下无法保证所有key都在同一个hash slot上，但仍个减少网络交互耗时\n对于不支持批量操作的命令，可以用pipeline将一批Redis命令封装成一组（非原子操作），但是需要控制批量传输的元素个数，避免网络传输的数据量大，同前一点一样，在Redis Cluster会有问题\n\n\nJedis方式（https://redis.io/commands/）\nimport redis.clients.jedis.Jedis;\n \n import java.util.List;\n import java.util.Set;\n \n public class redisDemo &#123;\n     public static void main(String[] args) &#123;\n         Jedis jedis &#x3D; new Jedis(&quot;localhost&quot;,6379);\n         &#x2F;&#x2F;ping下，看看是否通的\n &#x2F;&#x2F;        System.out.println(&quot;Server is running: &quot; + jedis.ping());\n         &#x2F;&#x2F;String\n         jedis.set(&quot;foo&quot;, &quot;bar&quot;);\n         String value &#x3D; jedis.get(&quot;foo&quot;);\n         &#x2F;&#x2F;List，双端队列可设置为阻塞获取，可返回&#x2F;删除一个范围内的元素，可通过索引设置元素\n         jedis.lpush(&quot;mylist&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         jedis.rpush(&quot;mylist&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         List&lt;String&gt; mylist &#x3D; jedis.lrange(&quot;mylist&quot;, 0, -1);&#x2F;&#x2F;0表示第一个，-1表示最后一个\n         System.out.println(mylist);\n         &#x2F;&#x2F;Set\n         jedis.sadd(&quot;myset&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         jedis.sadd(&quot;myset2&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;);\n         Set&lt;String&gt; setdiff &#x3D; jedis.sdiff(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;差集\n         Set&lt;String&gt; setinter &#x3D; jedis.sinter(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;交集\n         Set&lt;String&gt; sunion &#x3D; jedis.sunion(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;并集\n         &#x2F;&#x2F;Hash\n         jedis.hset(&quot;myhash&quot;, &quot;a&quot;, &quot;b&quot;);\n &#x2F;&#x2F;        jedis.hincrBy(&quot;myhash&quot;, &quot;a&quot;, 1);&#x2F;&#x2F;a的值加1\n         &#x2F;&#x2F;Sorted Set\n         jedis.zadd(&quot;myzset&quot;, 1, &quot;a&quot;);\n         jedis.zadd(&quot;myzset&quot;, 2, &quot;b&quot;);\n         jedis.zadd(&quot;myzset&quot;, 3, &quot;c&quot;);\n         jedis.zlexcount(&quot;myzset&quot;, &quot;-&quot;, &quot;+&quot;);&#x2F;&#x2F;返回有序集合中指定区间内成员的数量\n         jedis.zlexcount(&quot;myzset&quot;, &quot;[b&quot;, &quot;[c&quot;);&#x2F;&#x2F;返回有序集合中指定区间(b到c)内成员的数量\n \n     &#125;\n &#125;\n\n\n底层原理\n\n全局哈希：实现从键到值的访问，具体的数据再根据值的类型不同进行不同的查找（默认使用两个全局哈希表）\n\n哈希冲突解决方法：拉链法，增加next指针，缺点是冲突越多在链上的查找越慢\nrehash：增加hash表长度，减少单个桶中的元素数量，rehash的过程包括以下三步\n给hash表2分配更大的空间\n把hash表1的数据重新映射到hash表2，会产生大量的数据拷贝，导致Redis的线程阻塞，所以使用渐进式rehash\nRedis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries\n处理请求：渐进式rehash过程中，使用两个hash表，t1和t2。针对查找操作，先在t1里面查找，如果没找到就去t2里查找；针对插入操作，一律保存到t2里，保证t1数据只减不增\n\n\n释放hash表1的空间，表1留作下一次rehash使用\n\n\n\n\n\n压缩列表：基于压缩列表实现了List、Hash、Set、Sorted Set，节省了dictEntry数量，好多个值共用一个dictEntry\n\n类似于一个数组，不同之处是在表头有三个字段lbytes、zltail和zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束\n\n查找：查找头尾元素复杂度是O(1)，查找其它元素复杂度是O(N)\n\n\n\n节省内存：使用一系列entry保存数据，每个entry包括以下几部分，通过连续存储不使用指针连接来节省指针占用的空间\n\nprev_len，表示前一个 entry 的长度，prev_len 有两种取值情况1 字节或 5 字节\n\n取值 1 字节时，表示上一个 entry 的长度小于 254 字节。虽然 1 字节的值能表示的数值范围是 0 到 255，但是压缩列表中 zlend 的取值默认是 255，因此，就默认用 255 表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。所以，当上一个 entry 长度小于 254 字节时prev_len 取值为 1 字节，否则，就取值为 5 字节\n\n\nlen：表示自身长度，4 字节\n\nencoding：表示编码方式，1 字节\n\ncontent：保存实际数据\n\n代码（byte==B），一个存储Long类型的entry占用1+4+1+8（向上取整为16）字节\ntypedef struct zlentry &#123;\n    unsigned int prevrawlensize; &#x2F;* Bytes used to encode the previous entry len*&#x2F;\n    unsigned int prevrawlen;     &#x2F;* Previous entry len. *&#x2F;\n    unsigned int lensize;        &#x2F;* Bytes used to encode this entry type&#x2F;len.\n                                    For example strings have a 1, 2 or 5 bytes\n                                    header. Integers always use a single byte.*&#x2F;\n    unsigned int len;            &#x2F;* Bytes used to represent the actual entry.\n                                    For strings this is just the string length\n                                    while for integers it is 1, 2, 3, 4, 8 or\n                                    0 (for 4 bit immediate) depending on the\n                                    number range. *&#x2F;\n    unsigned int headersize;     &#x2F;* prevrawlensize + lensize. *&#x2F;\n    unsigned char encoding;      &#x2F;* Set to ZIP_STR_* or ZIP_INT_* depending on\n                                    the entry encoding. However for 4 bits\n                                    immediate integers this can assume a range\n                                    of values and must be range-checked. *&#x2F;\n    unsigned char *p;            &#x2F;* Pointer to the very start of the entry, that\n                                    is, this points to prev-entry-len field. *&#x2F;\n&#125; zlentry;\n\n\n二级编码技巧\n\n使用集合类型保存单值的键值对，把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value\n以图片 ID 1101000060 和图片存储对象 ID 3302000080 为例，可以把图片 ID 的前 7 位（1101000）作为 Hash 类型的键，把图片 ID 的最后 3 位（060）和图片存储对象 ID 分别作为 Hash 类型值中的 key 和 value\n\n\n\n\n跳表\n\n在链表的基础上，增加了多级索引，通过索引位置的几个跳转就可以实现数据的快速定位，查找的复杂度是O(logN)\n\n\nString的底层实现\n\n相比于c语言的字符串\n\n拼接时会先考虑内存空间，防止内存溢出\n使用len保存了当前字符串的长度，计算长度的时间复杂度是O(1)的\n减少内存分配次数：为了避免修改（增加/减少）字符串时，每次都需要重新分配内存（C 语言的字符串是这样的），SDS 实现了空间预分配和惰性空间释放两种优化策略。当 SDS 需要增加字符串时，Redis 会为 SDS 分配好内存，并且根据特定的算法分配多余的内存，这样可以减少连续执行字符串增长操作所需的内存重分配次数。当 SDS 需要减少字符串时，这部分内存不会立即被回收，会被记录下来，等待后续使用（支持手动释放，有对应的 API）\n二进制安全：C 语言中的字符串以空字符\\\\\\\\0作为字符串结束的标识，这存在一些问题，像一些二进制文件（比如图片、视频、音频）就可能包括空字符，C 字符串无法正确保存。SDS 使用 len 属性判断字符串是否结束，不存在这个问题\n\n\nString 还是 Hash 存储对象数据更好\n\nString 存储的是序列化后的对象数据，存放的是整个对象。Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改或者添加部分字段，节省网络流量。如果对象中某些字段需要经常变动或者经常需要单独查询对象中的个别字段信息，Hash 就非常适合\nString 存储相对来说更加节省内存，缓存相同数量的对象数据，String 消耗的内存约是 Hash 的一半。并且，存储具有多层嵌套的对象时也方便很多。如果系统对性能和资源消耗非常敏感的话，String 就非常适合\n在绝大部分情况，使用 String 来存储对象数据即可\n\n\nString内存占用：以key和value都为10位的整数为例，key16B、value16B、dictEntry32B，合计64B\n\n针对初始化的长度决定用多少字节的struct（支持1、2、4、8），可以减少内存的使用，数据用char buf[]存储\n\nString本身空间占用\n\nint编码：当保存的是 Long 类型整数（8B）时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了\n\nembstr编码：当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片\n\nraw编码模式：当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构\n\n\n\n\n全局哈希的dictEntry结构\n\ndictEntry结构：key指针、value指针、next指针分别为8B\n\n\nRedis的内存分配库jemalloc：分配比所需空间大的最小2次幂走位分配空间，减少分配次数，例如上面的dictEntry结构占用24字节空间\n\n\n\n\n\n\n\n\n\n\n\n2.线程模型\n单线程机制：多线程机制会带来不必要的开销，出现并行变串行的情况；通过优化（内存上进行操作、高效的数据结构、多路复用机制）单线程提高性能\n\n阻塞点\n\n客户端：网络 IO（多路复用机制优化），键值对增删改查操作（O(N)操作会阻塞，如全量查询、聚合统计），数据库操作\n\n查询：keys * （获取所有的 key 操作）、Hgetall（返回哈希表中所有的字段和）、smembers（返回集合中所有成员）\n\n优化：可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合计算\n\n\nbigkey删除：短时释放大量内存，删除操作需要释放内存，将空闲内存插入到空闲内存块链表，内存块过多会影响链表操作时间，从而造成Redis主线程的阻塞。bigkey删除即删除包含大量元素的集合，其不同类型常见耗时如下：\n\n优化1：从Redis4.0开始，当集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，建议使用 UNLINK 命令\n优化2：4.0之前，先使用集合类型提供的 SCAN 命令读取数据，然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避免一次性删除大量 key 给主线程带来的阻塞\n\n\n\n清空数据库：如 FLUSHDB 和 FLUSHALL 操作原理同上bigkey删除\n\n优化：从Redis4.0开始，可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库\n\n\n\n\n磁盘：生成 RDB 快照（子进程），记录 AOF 日志，AOF 日志重写（子进程）\n\n记录 AOF 日志：会根据不同的写回策略对数据做落盘保存。一个同步写磁盘的操作的耗时大约是 1～2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程\n\n\n主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件\n\n接收RDB文件：主库在复制的过程中，创建和传输 RDB 文件都是由子进程来完成的，不会阻塞主线程。但是，对于从库来说，它在接收了 RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库，这会阻塞主线程\n加载RDB文件：从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和 RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢\n优化：把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以较快的速度加载\n\n\n\n\n切片集群实例：向其他实例传输哈希槽信息，数据迁移\n\nRedsi动态扩缩容时，为保证数据一致性，迁移操作都是同步操作，当没有 bigkey 时，切片集群的各实例在进行交互时不会阻塞主线程，一旦有bigkey且内存占用过大时，会触发集群内的故障转移，造成不必要的切换\n\n\n异步子线程机制：除了查询和加载RDB文件这两个读操作，都可以使用异步子线程机制\n\nRedis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行\n惰性删除（Redis4.0）：主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成，数据会在子线程获取任务后才开始删除（使用UNLINK而不是DEL）\nAOF日志的everysec选项：主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了\n\n\n\n\n6.0版本的多线程：为了提高网络IO读写性能，这部分时Redis的一个性能瓶颈，多线程默认是禁用的，不建议开启\n\nredis性能变慢的检测方法\n\n基于当前环境下的Redis基线性能判定Redis是否真的变慢（./redis-cli --intrinsic-latency 120    ）\n\n系统排查及应对方案\n\n自身操作特性：看日志是否有慢查询命令、看是否有key集中过期的情况（EXPIREAT、EXPIRE）、是否存在bigkey、是否在进行自动内存整理\n\n操作系统：Redis是内存数据库，操作系统的内存机制会直接影响Redis的内存效率，如swap机制、内存大页机制\n\n触发swap的原因：物理机器内存不足，Redis实例使用了大量内存、机器上其它进程进行读写操作占用内存，解决方法为，增加机器的内存或使用Redis集群\n$ redis-cli info | grep process_id\nprocess_id: 5332\n$ cd &#x2F;proc&#x2F;5332\n$cat smaps | egrep &#39;^(Swap|Size)&#39;\nSize: 584 kB #一块内存大小\nSwap: 0 kB #有多少内存被swap到磁盘上\nSize: 4 kB\nSwap: 4 kB\nSize: 462044 kB\nSwap: 462008 kB #出现几百MB时，表明Redis实例的内存压力很大，很可能变慢\n大量短连接请求：Redis处理大量短连接请求，TCP三次握手和四次挥手也会增加耗时，可以使用长连接操作Redis\n\n内存大页机制：持久化时通过写时复制机制保证继续响应请求，即使只改小部分数据也需要拷贝整个大页，影响Redis正常的访存操作，所以一般关闭内存大页机制\necho never &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n\n\n文件系统：Redis会持久化到磁盘，文件系统写磁盘的机制会影响Redis持久化的效率，如AOF模式不同的写回策略会导致不同的延迟（检查配置）\n\nAOF日志提供了三种日志写回策略no、everysec、always，依赖底层的系统调用write和fsync，后两种写回策略都使用了fsync，但是everysec使用了后台子线程异步完成fsync操作而always没有使用，fsync需要等写回磁盘才返回\nAOF重写会进行大量的IO操作，阻塞fsync操作（等待写完磁盘才返回），主线程虽不等待fsync操作，但是会导致主线程的下一次fsync操作被阻塞（等待上一次的fsync），从而阻塞主线程\n是否运行了 Redis 主从集群，如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞\n\n\n\n\n\n\n\n\n基于多路复用的高性能I/O模型（epoll网络框架）\n\n传统做法：阻塞IO模型\n为了处理一个 Get 请求，需要监听客户端请求（bind/listen），和客户端建立连接（accept），从 socket 中读取请求（recv），解析客户端发送请求（parse），根据请求类型读取键值数据（get），最后给客户端返回结果，即向 socket 中写回数据（send）\n其中的accept、recv操作都是潜在的阻塞点，如果发生阻塞，Redis就不能再响应其它请求\n\n\n基于多路复用的高性能I/O模型（select/epoll）\nRedis向内核注册事件和对应的事件回调函数，由内核来同时保存并监听多个套接字（FD）上的连接请求或数据请求，一旦有请求到达，通过select/epoll提供的基于事件的回调机制（不同事件的不同处理函数）来实现。select/epoll在检测到FD上有请求到达时（事件发生），就将对应事件插入到事件队列中，Redis一直在对事件队列进行处理（如调用epoll_wait函数取事件队列的数据），这样就不会阻塞在某一具体的请求上了\n\n\n\n\n事务：不支持原子性、不支持回滚、每条命令都与服务器交互，所以不推荐使用Redis的事务\n\nRedis 可以通过MULTI（开始事务），EXEC（执行事务），DISCARD（取消事务） 和 WATCH 等命令来实现事务功能\nWATCH 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行\n通过WATCH命令监听指定的 Key，当调用 EXEC命令执行事务时，如果一个被 WATCH命令监视的 Key 被 其他客户端/Session 修改的话，整个事务都不会被执行\n不过，如果 WATCH与 事务在同一个 Session 里，并且被 WATCH监视的 Key 被修改的操作发生在事务内部，这个事务是可以被执行成功\n\n\nRedis事务不支持原子性：Redis 事务在运行错误的情况下，除了执行过程中出现错误的命令外，其他命令都能正常执行。并且，Redis 事务是不支持回滚（roll back）操作的。因此，Redis 事务其实是不满足原子性的（而且不满足持久性）\n如果事务中使用的命令语法没问题时，可以保证原子性，所以需要严格按照 Redis 的命令规范进行程序开发，并且通过 code review 确保命令的正确性\n\n\n一致性（支持）：在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的\n实例发生故障时，如果有RDB则可以保证一致性；如果有AOF也可以保证一致\n\n\n隔离性（支持）\n并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证\n并发操作在 EXEC 命令后执行，此时，隔离性可以保证\n\n\n持久性：AOF的三种配置会导致数据丢失、RDB快照间隙宕机也会丢失数据\n除了不满足原子性之外，事务中的每条命令都会与 Redis 服务器进行网络交互，这是比较浪费资源的行为。明明一次批量执行多个命令就可以了，这种操作实在是看不懂。因此，Redis 事务是不建议在日常开发中使用的\n\n\n并发安全性：针对读-改-写操作\n\n原子操作\n\n方法一：Redis每个命令是原子性的，可以通过把多个操作在 Redis 中实现成一个操作，实现单命令操作\n如：数据修改涉及读-改-写三个步骤，可以通过INCR/DECR命令可以对数据进行增值 / 减值操作\n\n\n方法二：使用Lua脚本，用于Redis没有提供原子命令的情况\n\n\n加分布式锁\n\n缺点：将低并发安全性，分布式锁的实现困难\n\n单个Redis实现分布式锁\n\n实现：赋予锁变量一个变量名，把这个变量名作为键值对的键，而锁变量的值，则是键值对的值\n\n原子操作\n\n使用SETNX命令实现加锁操作：执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置\n释放锁：可以在执行完业务逻辑后，使用 DEL 命令删除锁变量\n\n\n问题\n\n释放失败：给锁加一个过期时间，即使持有锁的客户端发生了异常，无法主动地释放锁，Redis 也会根据锁变量的过期时间，在锁变量过期后，把它删除\n\n加锁后被另一客户端误删再创建新锁：区分来自不同客户端的锁操作，让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端\n\n优化后的实现\n# 加锁, unique_value作为客户端唯一性的标识\n# 使用了 NX 选项，SET 命令只有在键值对不存在时，才会进行设置，否则不做赋值操作\n# PX 10000 则表示 lock_key 会在 10s 后过期，以免客户端在这期间发生异常而无法释放锁\nSET lock_key unique_value NX PX 10000\n\n\n# 释放锁 比较unique_value是否相等，避免误释放，使用的Lua脚本实现的释放锁操作的伪代码\nif redis.call(&quot;get&quot;,KEYS[1]) &#x3D;&#x3D; ARGV[1] then\n    return redis.call(&quot;del&quot;,KEYS[1])\nelse\n    return 0\nend\n## 执行上面的脚本\nredis-cli  --eval  unlock.script lock_key , unique_value \n\n\n\n\n多个Redis实现分布式锁（Redlock）\n\n算法思路：让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失\n算法执行步骤\n客户端获取当前时间\n客户端按顺序依次向 N 个 Redis 实例执行加锁操作\n使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识\n客户端获取锁的总耗时没有超过锁的有效时间\n\n\n一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时\n需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况\n\n\n\n\n\n\n\n\n\n\n其它\n\n\n3.内存管理\n内存管理\n\n设置过期时间：内存有限，保存所有数据迟早会OOM\n\nRedis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间\n很多时候业务场景就是需要某个数据只在某一时间段内存在，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效，使用传统的数据库来处理的话，一般都是自己判断过期，这样更麻烦并且性能要差很多\n\n\n如何判断数据过期（过期字典）\n\n通过过期字典（可看作hash表）来保存数据过期的时间，过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）\n过期数据的删除策略：Redis采用定期删除+惰性删除，对于漏掉的过期key使用内存淘汰机制\n惰性删除 ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。\n定期删除 ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响\n\n\n大量key集中过期的问题\n给 key 设置随机过期时间\n开启 lazy-free（惰性删除/延迟释放） 。lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程（不建议使用）\n\n\n\n\nRedis 内存淘汰机制：干净数据直接删除，脏数据需要写回数据库\n\n不进行数据淘汰\nno-eviction：==禁止驱逐数据==，也就是说当内存不足以容纳新写入数据时，新写入操作会报错，很少用\n\n\n在设置了过期时间（EXPIRE命令）的数据中进行淘汰\nvolatile-lru（least recently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰，LRU策略的实现如下：\nRedisObject 结构来保存数据的，RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳\n并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选\n\n\nvolatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰\nvolatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰\nvolatile-lfu（4.0版 least frequently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰\n缓存污染问题：大量不再访问的数据滞留在缓存中，影响应用的性能；所以需要在写满之前就经常淘汰数据\nLFU策略\n从两个维度筛选并淘汰数据，数据访问的时效性（访问时间离当前时间的远近）和数据的被访问次数\nLFU 缓存策略的优化：在LRU策略基础上，为每个数据增加一个计数器，来统计这个数据的访问次数。筛选时先淘汰访问次数少的，访问次数相同时再淘汰掉距离上一次访问时间更久的数据\nLFU具体实现：把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分，ldt 值（前 16bit，表示数据的访问时间戳）；counter 值（后 8bit，表示数据的访问次数）\nLFU计数规则：每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1，可以减慢counter值达到255的速度\nLFU衰减机制：使用衰减因子配置项 lfu_decay_time 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以lfu_decay_time值，所得的结果就是数据 counter 要衰减的值\n\n\n\n\n\n\n在所有数据中进行淘汰\nallkeys-lru（least recently used）：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（==这个是最常用的==）\nallkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰，适用于没有明显冷热数据的情况\nallkeys-lfu（4.0版 least frequently used）：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key\n\n\n\n\n内存碎片\n\n产生的原因\n\nOS的内存分配机制：默认使用jemalloc内存分配器，其按照2的幂次大小来分配内存空间，减少分配次数的情况下产生了内部碎片\nRedis的负载特征：频繁修改 Redis 中的数据，当 Redis 中的某个数据删除时，Redis 通常不会轻易释放内存给操作系统\n\n\n查看内存碎片率（&gt;1.5才需要清理）\n\n内存碎片率：mem_fragmentation_ratio = used_memory_rss / used_memory\n\nredis-cli -p 6379 info | grep mem_fragmentation_ratio\n如何清理：Redis4.0-RC3 版本以后自带了内存整理，可以避免内存碎片率过大的问题（之前版本可以直接重启）\nconfig set activedefrag yes #启用了自动清理功能\n\n###具体清理\n# 内存碎片占用空间达到 500mb 的时候开始清理\nconfig set active-defrag-ignore-bytes 500mb\n# 内存碎片率大于 1.5 的时候开始清理\nconfig set active-defrag-threshold-lower 50\n\n###减少对Redis性能的影响\n# 内存碎片清理所占用 CPU 时间的比例不低于 20%\nconfig set active-defrag-cycle-min 20\n# 内存碎片清理所占用 CPU 时间的比例不高于 50%\nconfig set active-defrag-cycle-max 50\n\n\n缓冲区\n\n用一块内存空间来暂时存放命令数据，以免出现因为数据和命令的处理速度慢于发送速度而导致的数据丢失和性能问题，当缓冲区占用的内存超出了设定的上限阈值时，就会出现缓冲区溢出（bigkey/大RDB、处理慢、缓冲区小）\n\n客户端输入和输出缓冲区：输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端\n\n输入缓冲区溢出时\n\n发生场景：写入bigkey、服务端处理请求慢，通过CLIENT LIST命令可以查看客户端的输入缓冲区（qbuf）使用情况\n解决办法：避免客户端写入bigkey、避免Redis主线程阻塞\n\n\n输出缓冲区溢出时\n\n发生场景：返回bigkey、执行了MONITOR命令、缓冲区大小设置不合理\n\nMONITOR 命令是用来监测 Redis 执行的，执行后会持续输出监测到的各个命令操作，持续占用输出缓冲区\n\n缓冲区大小：与输入缓冲区不同输出缓冲区可以设置大小（client-output-buffer-limit）\n# normal 表示当前设置的是普通客户端，第 1 个 0 设置的是缓冲区大小限制\n# 第 2 个 0 和第 3 个 0 分别表示缓冲区持续写入量限制和持续写入时间限制\nclient-output-buffer-limit normal 0 0 0\n\n\n\n\n主从集群中的缓冲区\n\n全量复制：主节点向从节点传输RDB文件时，持续接受客户端发送的写命令请求，并保存在复制缓冲区中，主节点为每一个客户端维护一个复制缓冲区，RDB传输的慢就会导致复制缓冲区溢出，主节点会结束该连接导致全量复制失败\n建议把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令\n使用 client-output-buffer-limit 配置项，来设置合理的复制缓冲区大小\n控制从节点数量：主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和\n\n\n增量复制：主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区（主从同步中的repl_backlog_buffer）中，读取断连期间主节点接收到的写命令，进而进行增量同步\n环形缓冲区：一旦发生溢出，新写入的命令数据就会覆盖旧的命令数据，导致旧命令数据的丢失，进而导致主从节点重新进行全量复制\n\n\n\n\n\n\n\n\n三种常用的缓存读写策略：旁路缓存、读写穿透、异步缓存\n\nCache Aside Pattern（旁路缓存）：同时维护db和cache，并且以db的结果为准\n\n读取数据流程\n\n\n写数据中的问题\n\n正确方式：先更新db，在直接删除cache\n\n问题一：在写数据的过程中，可以先删除 cache ，后更新 db 么？\n\n回答：会有数据不一致的问题，在删除cache和更新db的过程中，如果有请求从db读取，会读到旧数据\n\n\n问题二：在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？\n\n在请求读取数据后，将新数据写入到缓存这个过程中，如果有请求更新db，那么读取数据的请求插入到cache中的就是旧数据\n\n\n\n\n\n旁路缓存模式的缺陷：\n\n首次请求数据一定不在cache中：提前缓存热点数据\n写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率\n数据库和缓存数据强一致场景 ：更新 db 的时候同样更新 cache，不过需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题\n可以短暂地允许数据库和缓存数据不一致的场景 ：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小\n\n\n保证缓存和数据库数据的一致性（更新数据库成功，但删除缓存这一步失败的情况）\n增加 cache 更新重试机制： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可\n\n\n\n\n\n\nRead/Write Through Pattern（同步直写）：将cache视为主要数据存储，cache服务负责将数据读取和写入db（很少用），对于首次请求不在cache中的问题，可以提前缓存热点数据\n\n写\n\n\n读\n\n\n\n\nWrite Behind Pattern（异步缓存写入）\n\n与读写穿透类似，都是cache负责db的读写，但是读写穿透是同步更新cache和db，而异步缓存写入更新缓存后，不直接更新db，改为异步批量的方式更新db\n开发中很少见，因为会有数据一致性的问题（没写入db就丢失），应用场景主要是消息队列中消息的异步写入磁盘、MySQL的Innodb Buffer Pool机制\n异步缓存写入下db的写性能非常高，非常适合一些数据经常变化又对数据一致性要求不高的场景，比如浏览量、点赞量\n\n\n\n\n缓存相关问题\n\n缓存雪崩\n\n缓存在同一时间大面积的失效，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力\n预防机制：构建Redis缓存高可靠集群，如果Redis缓存的主节点故障宕机了，可以进行主从切换\n\n\n原因一：缓存中有大量数据同时过期，导致大量请求无法得到处理\n避免给大量的数据设置相同的过期时间，在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟）\n通过服务降级，来应对缓存雪崩，即针对不同的数据采取不同的处理方式\n针对非核心数据请求，停止从缓存中查询这些数据，直接返回预定义的信息、空值或错误信息\n针对核心数据请求，仍然允许查询缓存，缓存缺失时继续通过数据库读取\n\n\n\n\n原因二：Redis缓存实例发生故障宕机，无法处理请求\n服务熔断：为了防止连锁的数据库雪崩，暂停应用对缓存系统的接口访问，会影响整个业务应用的运行\n请求限流：只允许通过一小部分请求，避免大量并发请求压力传递到数据库层\n\n\n\n\n缓存击穿\n\n请求的 key 对应的是热点数据，该数据存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期），这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了\n如：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n解决办法\n设置热点数据永不过期或者过期时间比较长\n针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期\n请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力\n\n\n\n\n缓存穿透\n\n大量请求的 key 是不合理的，根本不存在于缓存中，也不存在于数据库中。这就导致这些请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了\n\n业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据\n恶意攻击：专门访问数据库中没有的数据\n\n\n解决一：做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等\n\n解决二：布隆过滤器，非常方便的判断一个给定的数据是否存在于海量数据中\n\n布隆过滤器原理    \n\n首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值\n然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置\n最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作\n\n\n把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程\n\n误判：布隆过滤器说存在，则可能不存；但是说不存在则一定不存在\n\n计算哈希值，将位数组中对应下标设置为1；判断时检查位数组对应值是否是1（不同字符串可能哈希出来的位置相同）\n\n\n使用**docker redis bloomfilter**\n➜  ~ docker run -p 6379:6379 --name redis-redisbloom redislabs&#x2F;rebloom:latest\n➜  ~ docker exec -it redis-redisbloom bash\nroot@21396d02c252:&#x2F;data# redis-cli\n127.0.0.1:6379&gt;\n\n\n解决三：在请求入口的前端进行请求检测，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库\n\n\n\n\n\n\n2.高可靠1.数据持久化\nAOF\n写后日志：首先执行命令写入内存，然后再将命令记录到日志中。与传统数据库的写前（WAL）日志相比，避免了额外的检查开销，并且不会阻塞当前的命令（但会阻塞后一条命令）。但是在写入日之前宕机会丢失日志\n写回策略：控制一个写命令执行完后AOF日志写回磁盘的时机，即appendfsync配置项的三个可选值\nAlways，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；可以保证不丢失数据，但是回影响主线程性能\nEverysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘\nNo，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘，会有数据丢失的风险\n\n\nAOF重写机制：防止AOF文件过大，故障恢复时恢复过程缓慢\nRedis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入（将多个命令合并成一个命令）\nAOF使用后台子线程bgrewriteaof来完成，避免阻塞主线程：重写时fork出后台bgrewriteaof子线程，通过拷贝父进程的页表的方式共享父进程的内存数据的方式来共享父进程的数据\n写时复制：避免一次性大量拷贝给子进程造成的长时间阻塞问题，在父进程写入操作是一个已经存在的key时，父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间（否则子进程读完，父进程修改，就会丢失这一次修改的数据）\n\n\n两次日志写入：在重写过程中，新请求会先写入到原AOF文件的缓冲区中，然后写入到重写日志的缓冲区，在重写机制结束后再合并到AOF重写日志中（但是需要上面的写时复制来保证数据不会丢失修改）\n\n\n\n\nRDB\nAOF在故障恢复的时候需要逐一执行命令，恢复时间长，所以提出了RDB内存快照的方式来高效的恢复，提供了两个命令\nsave：在主线程中执行，会导致阻塞\nbgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置\n\n\n写时复制：在执行快照的同时，正常处理写操作\n由父进程fork出bgsave子进程，然后开始读取主线程的内存数据，并写入到RDB文件中，在主线程有写入请求时，这块数据会被复制一份，然后主线程在数据副本上进行修改，bgsave子进程继续将原来的数据写入RDB文件\n\n\n优化\n增量快照：一直做全量快照，虽然bgsave执行时不阻塞主线程，但是会对磁盘造成压力，而且fork操作本身也会阻塞主线程。通过记录修改的元数据信息来做增量快照，但是又会产生大量的额外空间开销\nRedis 4.0 中提出了一个混合使用 AOF 日志和内存快照：内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作\n\n\n\n\n如何选择 RDB 和 AOF？\nRDB 比 AOF 优秀的地方\nRDB 文件存储的内容是经过压缩的二进制数据， 保存着某个时间点的数据集，文件很小，适合做数据的备份，灾难恢复。AOF 文件存储的是每一次写命令，类似于 MySQL 的 binlog 日志，通常会必 RDB 文件大很多。当 AOF 变得太大时，Redis 能够在后台自动重写 AOF。新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。不过， Redis 7.0 版本之前，如果在重写期间有写入命令，AOF 可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。\n使用 RDB 文件恢复数据，直接解析还原数据即可，不需要一条一条地执行命令，速度非常快。而 AOF 则需要依次执行每个写命令，速度非常慢。也就是说，与 AOF 相比，恢复大数据集的时候，RDB 速度更快。\n\n\nAOF 比 RDB 优秀的地方\nRDB 的数据安全性不如 AOF，没有办法实时或者秒级持久化数据。生成 RDB 文件的过程是比较繁重的， 虽然 BGSAVE 子进程写入 RDB 文件的工作不会阻塞主线程，但会对机器的 CPU 资源和内存资源产生影响，严重的情况下甚至会直接把 Redis 服务干宕机。AOF 支持秒级数据丢失（取决 fsync 策略，如果是 everysec，最多丢失 1 秒的数据），仅仅是追加命令到 AOF 文件，操作轻量。\nRDB 文件是以特定的二进制格式保存的，并且在 Redis 版本演进中有多个版本的 RDB，所以存在老版本的 Redis 服务不兼容新版本的 RDB 格式的问题。\nAOF 以一种易于理解和解析的格式包含所有操作的日志。你可以轻松地导出 AOF 文件进行分析，你也可以直接操作 AOF 文件来解决一些问题。比如，如果执行FLUSHALL命令意外地刷新了所有内容后，只要 AOF 文件没有被重写，删除最新命令并重启即可恢复之前的状态\n\n\n由于 RDB 和 AOF 各有优势，于是，Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差\n\n\n\n2.主从复制\n使用主从库模式，通过增加副本冗余量，将一份数据同时保存在多个实例上。主从库之间采用读写分离的方式，主库和从库同时支持读操作，写操作通过主库执行然后同步到从库\n不采用主从库读写分离：需要加锁或实例间协商的方式完成修改，带来更大的开销\n\n\n主从库模式的建立\n启动多个Redis实例时，他们相互之间通过replicaof命令形成主库和从库的关系，按照三个阶段完成第一次同步\n第一阶段：从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了\n具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID（实例的随机ID，第一次设置为？）和复制进度offset（-1表示第一次复制）两个参数\n主库收到 psync 命令后，会用 FULLRESYNC 响应命令（全量复制）带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。\n\n\n第二阶段：主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的RDB文件\n主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空\n在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作\n\n\n第三阶段：主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了\n\n\n\n\n优化\n主从级联模式分担全量复制时的主库压力：主库需要进行两个耗时操作，生成RDB文件和传输RDB文件，如果从库数量过多就会忙于fork子进程生成RDB文件，通过主-从-从模式将主库的压力分担下去，让一些从库不再和主库交互，只和级联的从库进行写操作同步，减轻主库上的压力\n基于长连接的命令传播：主从库完成全量复制后，会一直维护一个网络连接，主库通过这个连接将后续命令同步给从库\n增量复制：主从库间网络断了，2.8之前进行全量复制，2.8之后采用部分增量复制仍需全量同步，4.0版本后进行增量同步\nrepl_backlog_buffer缓冲区：repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置\n当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区\n主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距\n\n\n注意事项\n通过reolid和replid2来判断主从切换的时候，新的master和slave是否曾经属于同一个主库，如果属于可进行增量同步的尝试\nmaster同步速度必须比slave快，且不能超过环形缓冲区大小，否则还是要进行全量同步操作\nrepl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致，可以通过调整 repl_backlog_size 这个参数来避免\n一个从库如果和主库断连时间过长，造成它在主库repl_backlog_buffer的slave_repl_offset位置上的数据已经被覆盖掉了，此时从库和主库间将进行全量复制\n每个从库会记录自己的slave_repl_offset，每个从库的复制进度也不一定相同。在和主库重连进行恢复时，从库会通过psync命令把自己记录的slave_repl_offset发给主库，主库会根据从库各自的复制进度，来决定这个从库可以进行增量复制，还是全量复制\n\n\n\n\n\n\n\n3.哨兵机制\n哨兵机制：在Redis主从集群中，实现主从库自动切换的机制，有效地解决了三个问题（主库判活、从库升级为主库、新主库同步消息到从库和客户端），即监控、选主和通知三个任务\n\n监控：哨兵在运行时周期性的给所有的主从库发送PING命令，检测他们是否仍在运行，如果从库规定时间内没响应，则标记为下线状态（主观下线）；如果主库规定时间内也没有响应，则开始自动切换主库的流程（主观下线）\n客观下线：主观下线如果是误判（网络压力大、主库压力大），会产生额外的通信和计算开销，所以选择多哨兵实例的哨兵集群的方式来减少误判率\n客观下线的标准：当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”，但是这个数量标准可以通过设置来指定\n哨兵领导者：哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换\n\n\n选主：在已有从库中，通过一定的规则（筛选+打分）选择一个从库实例，将其升级为主库\n筛选\n判断从库的当前在线状态：从库仍在运行\n判断之前的网络状态：使用配置项down-after-milliseconds * 10，down-after-milliseconds是从库断连的最大连接超时时间，如果down-after-milliseconds内从库都没有连接上则认为主从节点断连，如果从库从运行到现在一共断连次数超过10次，则认为从库网络状况不好（在sentinel.conf中配置）\n\n\n打分：按照三个规则依次打分（从库优先级、从库复制进度以及从库 ID 号），只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。\n第一轮：优先级最高的从库得分高，通过 slave-priority 配置项配置\n第二轮：和旧主库同步程度最接近的从库得分高，repl_backlog_buffer缓冲区的位置，主为master_repl_offset，副为slave_repl_offset，选择复制最快的（选slave_repl_offset最大的）\n第三轮：ID 号小的从库得分高，每个实例都会有一个 ID，类似于从库的编号\n\n\n\n\n通知：在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让他们执行repliicaof命令，和新主库建立连接并进行数据复制。同时，哨兵还会把新主库的连接信息发送给客户端，让它们把请求操作发到新主库上\n通知客户端：哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端\n如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。 所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址\n\n\n\n\n哨兵集群\n\n基于 pub/sub 机制的哨兵集群组成：不同哨兵通过___sentine__:hello频道来相互发现、实现互相通信\n\n主库和哨兵：哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口\n从库和哨兵：哨兵向主库发送 INFO 命令后可以知道从库的IP地址和端口\n\n\n基于 pub/sub 机制的客户端事件通知\n\n从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件\n\n重要频道使用：客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，可以在客户端执行订阅命令（SUBSCRIBE [下面的频道]），来获取不同的事件消息\n\n\n\n\n由哪个哨兵执行主从切换\n\n任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票\n要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds\n\n\n一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的\n需要同时满足：拿到半数以上的赞成票（选举Leader），并且票数需要大于quorum值（判读客观下线）\n\n\n此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader\n如果一轮投票没选出来Leader，哨兵集群就等待一段时间（哨兵故障转移超时时间的2倍），再重新选举\n\n\n\n\n\n\n相关问题：\n\n哨兵机制能防止脑裂吗\nmaster和两个slave节点因网络问题被隔离时，所有写入到master的数据都会丢失（网络恢复后master节点会变为新master的slave）\n解决办法\nmin-replicas-to-write 1：配置写master至少写入的slave数量，0表示关闭此功能，3个节点的情况下，可以配置为1\nmin-replicas-max-lag 10：配制master多长时间无法得到从节点的响应，就认为这个节点失联，失联则停止新的写入命令请求\n\n\n\n\nRaft协议\n\n\n\n3.高可扩展1.数据分片\n单实例：使用RDB进行持久化时，fork子进程的用时与Redis数据量是正相关的，所以采用切片/分片集群，同时启动多个Redis实例组成一个集群，然后按照一定的规则，把收到的数据划分为多份，每一份用一份实例来保存\n\nRedis在单机情况下支持多个数据库（同一个访问密码，FLUSHALL可以同时清空所有数据），每个数据库对外都是一个从0开始的递增数字命名，Redis默认支持16个数据库。并且可以随时使用SELECT命令更换数据库\n\n\nRedis Cluster：用于实现切片集群的方案，方案中规定了数据分片和实例的对应关系\n\n哈希槽：一个切片集群共有16384个哈希槽，根据键值对的key，按照CEC16算法计算一个16bit的值，然后与16384取模确定对应的哈希槽。哈希槽默认被均分到Redis实例上，也可以通过命令来配置（cluster meet、cluster addslots）\n\n为什么Redis Cluster的哈希槽是16384个：CRC16算法可以产生16位（65536），但是只用了14位（16384）\n通过bitmap来维护哈希槽信息，如果该位为1，则表示这个哈希槽属于这个节点，哈希槽长度为2048（16384/8）。哈希槽总数越少，bitmap填充率越小，压缩效果越好\n正常的心跳包会携带一个节点的完整配置，也就是说会包含当前节点负责的哈希槽的信息，如果是65536则需要8k的空间，内存占用过高\n\n\n\n\n客户端如何定位数据所在实例\n\nRedis会把自己的哈希槽发给和他相连接的其他实例，来完成哈希槽分配信息的扩散，客户端会把哈希槽信息缓存在本地\n127.0.0.1:6379&gt; cluster slots\n1) 1) (integer) 0\n   2) (integer) 4095\n   3) 1) &quot;192.168.10.3&quot;\n      2) (integer) 6379\n2) 1) (integer) 12288\n   2) (integer) 16383\n   3) 1) &quot;192.168.10.5&quot;\n      2) (integer) 6379\n变化：集群中增减Redis实例、为了负载均衡重新划分，重新划分完后实例间使用上面的方式扩散信息\n\nCLUSTER SETSLOT：使用不同的选项进行三种设置，分别是设置 Slot 要迁入的目标实例，Slot 要迁出的源实例，以及 Slot 所属的实例\n\nCLUSTER GETKEYSINSLOT：获取某个 Slot 中一定数量的 key\n\nMIGRATE：把一个 key 从源实例实际迁移到目标实例\n#假设要把 Slot 300 从源实例（ID 为 3）迁移到目标实例（ID 为 5）\n#第 1 步，我们先在目标实例 5 上执行下面的命令，将 Slot 300 的源实例设置为实例 3，表示要从实例 3 上迁入 Slot 300\nCLUSTER SETSLOT 300 IMPORTING 3\n#第 2 步，在源实例 3 上，我们把 Slot 300 的目标实例设置为 5，这表示，Slot 300 要迁出到实例 5 上，如下所示：\nCLUSTER SETSLOT 300 MIGRATING 5\n#第 3 步，从 Slot 300 中获取 100 个 key。因为 Slot 中的 key 数量可能很多，所以我们需要在客户端上多次执行下面的这条命令，分批次获得并迁移 key。\nCLUSTER GETKEYSINSLOT 300 100\n#第 4 步，我们把刚才获取的 100 个 key 中的 key1 迁移到目标实例 5 上（IP 为 192.168.10.5），同时把要迁入的数据库设置为 0 号数据库，把迁移的超时时间设置为 timeout。我们重复执行 MIGRATE 命令，把 100 个 key 都迁移完。\nMIGRATE 192.168.10.5 6379 key1 0 timeout\n#最后，我们重复执行第 3 和第 4 步，直到 Slot 中的所有 key 都迁移完成。\n\n#从 Redis 3.0.6 开始，你也可以使用 KEYS 选项，一次迁移多个 key（key1、2、3），这样可以提升迁移效率。\nMIGRATE 192.168.10.5 6379 &quot;&quot; 0 timeout KEYS key1 key2 key3\n\n\n\n\n重定向\n\n当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回MOVED命令响应结果，这个结果中就包含了新实例的访问地址\n数据迁移过程中的请求：如果不在本地，则返回ASK报错信息返回新地址，客户端给新地址发送ASKING命令在发送数据请求命令（如果不发ASKING直接请求则会报错，因为新实例上还没有管理这个槽位）\n\n\n\n\nGossip协议：Redis Cluster中的节点的通信方式，cluster.h定义了所有消息类型和消息结构\n\nGossip 协议的工作原理可以概括成两点\n\n每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表\n一个实例在接收到 PING 消息后，会给发送 PING 消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样\n\n\nGossip 消息大小：clusterMsgDataGossip * 集群中实例个数 + 16384bit的Bitmap（slot信息）\ntypedef struct &#123;\n    char nodename[CLUSTER_NAMELEN];  &#x2F;&#x2F;40字节\n    uint32_t ping_sent; &#x2F;&#x2F;4字节\n    uint32_t pong_received; &#x2F;&#x2F;4字节\n    char ip[NET_IP_STR_LEN]; &#x2F;&#x2F;46字节\n    uint16_t port;  &#x2F;&#x2F;2字节\n    uint16_t cport;  &#x2F;&#x2F;2字节\n    uint16_t flags;  &#x2F;&#x2F;2字节\n    uint32_t notused1; &#x2F;&#x2F;4字节\n&#125; clusterMsgDataGossip; &#x2F;&#x2F;104字节 一个实例状态信息大小\n实例间通信频率\n\nRedis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法\n为了避免有实例一直没有被发送PING信息：Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现有实例最近一次接收 PONG 消息的时间，已经大于配置项 cluster-node-timeout 的一半了（cluster-node-timeout/2），就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息\n\n\n\n\n\n2.负载均衡附录\n如何使用慢查询日志和 latency monitor 排查执行慢的操作（也可以使用监控工具latency monitor）\n\n设置参数\n\nslowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录\nslowlog-max-len：慢查询日志最多能记录多少条命令记录（队列）\n\n\n使用SLOWLOG GET命令查看慢查询日志中记录的命令操作\nSLOWLOG GET 1\n1) 1) (integer) 33           &#x2F;&#x2F;每条日志的唯一ID编号\n   2) (integer) 1600990583   &#x2F;&#x2F;命令执行时的时间戳\n   3) (integer) 20906        &#x2F;&#x2F;命令执行的时长，单位是微秒\n   4) 1) &quot;keys&quot;               &#x2F;&#x2F;具体的执行命令和参数\n      2) &quot;abc*&quot;\n   5) &quot;127.0.0.1:54793&quot;      &#x2F;&#x2F;客户端的IP和端口号\n   6) &quot;&quot;                     &#x2F;&#x2F;客户端的名称，此处为空\n\n\n如何排查Redis的bigkey：./redis-cli  --bigkeys\n\n在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行\n可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能\n缺点：只能返回最大的那个bigkey，只统计个数不统计实际占用的内存量\n\n$ .&#x2F;redis-cli  --bigkeys\n\n-------- summary -------\nSampled 32 keys in the keyspace!\nTotal key length in bytes is 184 (avg len 5.75)\n\n&#x2F;&#x2F;统计每种数据类型中元素个数最多的bigkey\nBiggest   list found &#39;product1&#39; has 8 items\nBiggest   hash found &#39;dtemp&#39; has 5 fields\nBiggest string found &#39;page2&#39; has 28 bytes\nBiggest stream found &#39;mqstream&#39; has 4 entries\nBiggest    set found &#39;userid&#39; has 5 members\nBiggest   zset found &#39;device:temperature&#39; has 6 members\n\n&#x2F;&#x2F;统计每种数据类型的总键值个数，占所有键值个数的比例，以及平均大小\n4 lists with 15 items (12.50% of keys, avg size 3.75)\n5 hashs with 14 fields (15.62% of keys, avg size 2.80)\n10 strings with 68 bytes (31.25% of keys, avg size 6.80)\n1 streams with 4 entries (03.12% of keys, avg size 4.00)\n7 sets with 19 members (21.88% of keys, avg size 2.71)\n5 zsets with 17 members (15.62% of keys, avg size 3.40)\n\n","slug":"Redis","date":"2023-05-04T10:00:34.000Z","categories_index":"","tags_index":"database","author_index":"Dajunnnnnn"},{"id":"5df101b423a7ec5d76d6e555835fbe7b","title":"IO模型","content":"IO模型1.IO多路复用2.Java的IO模型3.其它1.Redis的IO模型2.Nginx的IO模型3.","slug":"IO模型","date":"2023-05-04T07:19:59.000Z","categories_index":"","tags_index":"","author_index":"Dajunnnnnn"},{"id":"8aa9bb0438939ce2b4a00f1a6ea1e9e5","title":"Go","content":"GO1.运行前准备\n源码结构\n\nGOROOT：Go 语言安装根目录的路径，也就是 GO 语言的安装路径\nGOPATH：若干工作区目录的路径。是我们自己定义的工作空间（workspace），go源码文件（.go）、归档文件（.a）、可执行文件都存在此处\ngo源码文件需要保存在GOPATH包含的某个工作区（目录）中的src目录下的某个代码包（目录）中，可执行文件放在该工作区的bin子目录，归档文件（.a）放在pkg子目录\n安装某个代码包而产生的归档文件是与这个代码包同名的\n构建和安装Go程序的过程：构建使用命令go build，安装使用go install。构建和安装代码的时候都会执行编译、打包等操作，并且这些操作生成的任何文件都会先保存到某个临时的目录中\n\n\nGOBIN：GO 程序生成的可执行文件（executable file）的路径\n添加export PATH=$PATH:/usr/local/go/bin到~/.bash_profile 或 /etc/profile，然后执行source ~/.bash_profile或source /etc/profile\n\n\n\n\n源码文件\n\n命令源码文件：可以使用 go run 命令启动，是程序的运行入口，每个可独立运行的程序必须拥有的。可以通过构建或安装，生成与其对应的可执行入口，后者一般会与该命令源码文件的直接父目录同名。例如存在demo.go文件中的hello world示例程序，可以通过go run demo.go命令来执行，在标准输出（屏幕）上看到hello world。（模块化编程时，也只有一个命令源码文件）\npackage main\n\nimport (\n\t&quot;flag&quot;&#x2F;&#x2F;专门用于接收和解析命令参数\n  &quot;fmt&quot;\n)\nvar name string\nfunc init() &#123;\n  &#x2F;&#x2F;四个参数：存命令参数值的地址、命令参数的名称、默认值、该命令参数的简短说明\n  flag.StringVar(&amp;name,&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n  &#x2F;&#x2F;var name &#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#125;\nfunc main() &#123;\n  &#x2F;&#x2F;对该函数的调用必须在所有命令参数存储载体的声明（这里是对变量name的声明）和\n  &#x2F;&#x2F;设置（这里是在[2]处对flag.StringVar函数的调用）之后\n  flag.Parse()&#x2F;&#x2F;用于真正解析命令参数，并把它们的值赋给相应的变量\n  fmt.Printf(&quot;Hello, %s!\\\\n&quot;, name)\n&#125;\n\n&#x2F;&#x2F;查看参数的使用说明\n$ go run demo.go -name-&quot;Robert&quot;\n#输出\nHello,Robert!\n#查看命令源码文件的参数说明\n$ go build demo.go\n$ .&#x2F;demo --help\n#go run命令构建上述命令源码文件时临时生成的可执行文件的完整路径,即go build demo.go\nUsage of .&#x2F;demo:\n -name string\n    The greeting object. (default &quot;everyone&quot;)\n库源码文件：库源码文件是不能被直接运行的源码文件，仅用于存放程序实体，这些程序实体可以被其他代码使用。（程序实体是变量、常量、函数、结构体和接口的统称，总是先声明再使用，程序实体的名字统称为标识符）这里的“其他代码”可以与被使用的程序实体在同一源码文件内，也可以在其他源码文件，甚至其他代码包中\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;demo.go&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npackage main\n\nimport (\n  &quot;flag&quot;\n)\n\nvar name string\n\nfunc init() &#123;\n  flag.StringVar(&amp;name, &quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#125;\n\nfunc main() &#123;\n  flag.Parse()\n  hello(name)&#x2F;&#x2F;变化位置\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;demo_lib.go&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npackage main\n\nimport &quot;fmt&quot;\n\nfunc hello(name string) &#123;\n  fmt.Printf(&quot;Hello, %s!\\\\n&quot;, name)\n&#125;\n什么样的程序实体才可以被当前包外的代码引用\n\n包级私有：首字母小写的程序实体（如函数名）\n公开：首字母大写的程序实体（如函数名）\n模块级私有：通过创建internal代码包（目录名为internal）让一些程序实体仅仅能被当前模块中的其他代码引用。具体规则是，internal代码包中声明的公开程序实体仅能被该代码包的直接父包及其子包中的代码引用。当然，引用前需要先导入这个internal包。对于其他代码包，导入该internal包都是非法的，无法通过编译\n\n\n\n\n补充\n\n\n2.基础语法\n程序实体\n\n关键字\n\n\n\n结构\n函数\n特殊\n\n\n\nif\nimport\ngo\n\n\nelse\npackage\nselect\n\n\nfor\nfunc\nchan\n\n\nswitch\nreturn\nrange\n\n\ncase\nstruct\nmap\n\n\nbreak\ninterface\nconst（标量）\n\n\ncontinue\ntype（别名）\ndefer\n\n\ndefault\nvar\ngoto\n\n\nfallthrough\n\n\n\n\n&#x2F;&#x2F;在声明name时，还为它赋了值，而没有显示指定name的类型，利用Go语言自身的类型推断，省去了声明\nvar name &#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#x2F;&#x2F;短变量声明，类型推断加上语法糖，只能在函数体内部使用，用来声明一些临时的变量\nname :&#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\nconst LENGTH int &#x3D; 10\n\nfor i :&#x3D; range numbers1 &#123;\n\tif i &#x3D;&#x3D; 3 &#123;\n\t\tnumbers1[i] |&#x3D; i\n\t&#125;\n&#125;\n\ntype struct_variable_type struct &#123;\n   member definition\n   member definition\n   ...\n   member definition\n&#125;\n预定义标识符\n\n\n\n整数\n无符号整数\n小数\n其它数\n函数\n特性\n\n\n\nint\nuint\nfloat32\nuintptr\nappend\nnew\n\n\nint8\nuint8\nfloat64\nbool\ncopy\nmake\n\n\nint16\nuint16\ncomplex64\ntrue\ncap\nclose\n\n\nint32\nuint32\ncomplex128\nfalse\nlen\niota\n\n\nint64\nuint64\ncomplex\nstring\nprint\npanic\n\n\n\nbyte\nreal / imag\nnil\nprintln\nrecover\n\n\n\n内置函数\n\nfunc append(slice []Type, elems ...Type) []type：在切片变量的后面追加新的数据，然后返回新的切片变量，可用使用slice接收\n func copy(dst, src [] Type) int：把slice源的数据复制到目的切片中，目的slice填满后舍弃超出的部分\nfunc delete(m map[Type]Type1, key Type)：delete函数用于删除map中对应key的键值对，如果map中不存在该key，则什么也不做\nfunc new(Type) *Type：用来创建某一个类型的指针型对象，返回值是一个指向新分配的type类型的零值的指针\n使用new创建chan类型的指针对象，在使用之前仍然需要使用make函数来初始化chan的容量\nnew函数创建对象与直接使用struct{}创建的对象的区别是，前者为指向对象的一个指针，后者创建的是对象引用本身\n\n\nfunc make(t Type, size ...IntegerType) Type：只能用于chan，map和切片三种类型的创建，返回值就是对象本身，这三类数据结构本身就是引用类型，必须要初始化\n\n\n重声明：允许在使用短变量声明时不用理会被赋值的多个变量中是否包含旧变量\n\n由于变量的类型在其初始化时就已经确定了，所以对它再次声明时赋予的类型必须与其原本的类型相同，否则会产生编译错误\n\n变量的重声明只可能发生在某一个代码块中。如果与当前的变量重名的是外层代码块中的变量，那么就是另外一种含义了\n\n变量的重声明只有在使用短变量声明时才会发生，否则也无法通过编译。如果要在此处声明全新的变量，那么就应该使用包含关键字var的声明语句，但是这时就不能与同一个代码块中的任何变量有重名了\n\n被“声明并赋值”的变量必须是多个，并且其中至少有一个是新的变量。这时我们才可以说对其中的旧变量进行了重声明\n\n示例\nvar err error\n&#x2F;&#x2F; 这里对err进行了重声明\nn, err :&#x3D; io.WriteString(os.Stdout, &quot;Hello, everyone!\\\\n&quot;)\n\n\n类型\n\n类型断言：表达式x.(T)，x代表要判断类型的值，必须是接口类型\nvar container &#x3D; []string&#123;&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;&#125;\ncontainer :&#x3D; map[int]string&#123;0: &quot;zero&quot;, 1: &quot;one&quot;, 2: &quot;two&quot;&#125;\n\n&#x2F;&#x2F;interface&#123;&#125;(container)用来把container变量的值转换为空接口值，\n&#x2F;&#x2F;interface&#123;&#125;代表空接口，任何类型都是它的实现类型，&#123;&#125;要么是空代码块，要么表示不包含任何内容的数据结构，即空的接口类型\n&#x2F;&#x2F;用于判断前者的类型是否为切片类型[]string的.([]string)\n&#x2F;&#x2F;ok是布尔类型，代表类型判断的结果，true或false。如果是true，那么被判断的值将会被自动转换为[]string类型的值，\n&#x2F;&#x2F;并赋给变量value，否则value将被赋予nil（即“空”）。\nvalue, ok :&#x3D; interface&#123;&#125;(container).([]string)\n类型转换：语法形式是T(x)，注意事项如下：可表示范围相同就可以，可表示范围变窄时则截断高位\n\n整数值转换成string类型时，如果是无效的Unicode代码点，则结果是”�”（Unicode标准中定义的Replacement Character，用于替换未知的字符）组成的字符串\n一个值在从string类型向[]byte类型转换时代表着以 UTF-8 编码的字符串会被拆分成零散、独立的字节\n一个值在从string类型向[]rune类型转换时代表着字符串会被拆分成一个个 Unicode 字符\n\n\n别名类型：type MyString = string，byte是uint8的别名类型，而rune是int32的别名类型\n\n类型再定义：type MyString2 string，MyString2和string就是两个不同的类型了。这里的MyString2是一个新的类型，不同于其他任何类型。把string类型再定义成了另外一个类型MyString2\n\n潜在类型：某个类型在本质上是哪个类型，潜在类型相同的不同类型的值之间是可以相互进行类型转换的。但这种说法对集合类的类型却不合法\n\n\n\n\n\n容器\n\n数组&amp;切片：切片是引用类型，而数组是值类型\n\narray：长度是固定的，长度是类型的一部分，不同长度是两个不同的数组类型。数组是切片的底层结构，属于值类型，同属值类型的游基础数据结构以及结构体类型。\nslice：是可变长度的，切片的长度可以自动地随元素数量的增长而增长，但不会减小。切片是对数组的封装，可以看作是对数组的某个连续片段的引用（其他引用类型：字典、通道、函数）\n\npackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n  &#x2F;&#x2F; 用make声明两个不同切片\n  s1 :&#x3D; make([]int, 5)&#x2F;&#x2F;如果不指明其容量，那么它就会和长度一致\n  fmt.Printf(&quot;The length of s1: %d\\\\n&quot;, len(s1))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The capacity of s1: %d\\\\n&quot;, cap(s1))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The value of s1: %d\\\\n&quot;, s1)&#x2F;&#x2F;[0 0 0 0 0]\n  s2 :&#x3D; make([]int, 5, 8)&#x2F;&#x2F;切片的容量实际上代表了它的底层数组的长度，这里是8，但是只能看到5\n  fmt.Printf(&quot;The length of s2: %d\\\\n&quot;, len(s2))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The capacity of s2: %d\\\\n&quot;, cap(s2))&#x2F;&#x2F;8\n  fmt.Printf(&quot;The value of s2: %d\\\\n&quot;, s2)&#x2F;&#x2F;[0 0 0 0 0]\n\t&#x2F;&#x2F;切片的容量可以看作是透过这个窗口最多可以看到的底层数组（即s3）中元素的个数，而且底层数组\n\t&#x2F;&#x2F;不变的情况下，可以向右扩展，直至底层数组的末尾\n\ts3 :&#x3D; []int&#123;1, 2, 3, 4, 5, 6, 7, 8&#125;\n\ts4 :&#x3D; s3[3:6]&#x2F;&#x2F;[3:6)\n\tfmt.Printf(&quot;The length of s4: %d\\\\n&quot;, len(s4))&#x2F;&#x2F;3\n\tfmt.Printf(&quot;The capacity of s4: %d\\\\n&quot;, cap(s4))&#x2F;&#x2F;5\n\tfmt.Printf(&quot;The value of s4: %d\\\\n&quot;, s4)&#x2F;&#x2F;[4 5 6]\n\tfmt.Println()\n\t&#x2F;&#x2F;把切片的窗口向右扩展到最大的方法\n\ts5 :&#x3D; s4[:cap(s4)]&#x2F;&#x2F;[4 5 6 7 8]\n&#125;\n链表\n\nlist包含的方法中，用于插入新元素的方法都只接受interface{ }类型的值，这些方法在内部会使用Element值，包装接受到的新元素。可以避免直接使用我们自己生成的元素，主要原因是避免链表的内部关联找到破坏\n\n开箱即用：经过语句var l list.List声明的变量l可以直接使用，因为有延迟初始化的机制（把初始化操作延后，分散初始化操作带来的计算量和存储空间的消耗），常用方法有以下几种：\n&#x2F;&#x2F;求长度\nfunc (l *List) Len() int &#123; return l.len &#125;\n&#x2F;&#x2F;移除元素\nfunc (l *List) Remove(e *Element) any &#123;&#125;\n&#x2F;&#x2F; 在最前&#x2F;后端插入新元素\nfunc (l *List) PushFront(v any) *Element &#123;&#125;\nfunc (l *List) PushBack(v any) *Element &#123;&#125;\n&#x2F;&#x2F;在元素之前&#x2F;后插入新元素\nfunc (l *List) InsertBefore(v any, mark *Element) *Element &#123;&#125;\nfunc (l *List) InsertAfter(v any, mark *Element) *Element &#123;&#125;\n&#x2F;&#x2F;将元素移到链表的最前&#x2F;后面\nfunc (l *List) MoveToFront(e *Element) &#123;&#125;\nfunc (l *List) MoveToBack(e *Element) &#123;&#125;\n&#x2F;&#x2F;将元素移到另一元素的前&#x2F;后面\nfunc (l *List) MoveBefore(e, mark *Element) &#123;&#125;\nfunc (l *List) MoveAfter(e, mark *Element) &#123;&#125;\n&#x2F;&#x2F;在链表后&#x2F;前插入另一链表的副本\nfunc (l *List) PushBackList(other *List) &#123;&#125;\nfunc (l *List) PushFrontList(other *List) &#123;&#125;\nRing和List的区别：其实List在内部就是一个循环链表。它的根元素永远不会持有任何实际的元素值，而该元素的存在就是为了连接这个循环链表的首尾两端，主要区别如下\n\n在创建并初始化一个Ring值的时候，可以指定它包含的元素的数量，但是对于一个List值来说却不能这样做（也没有必要这样做）。循环链表一旦被创建，其长度是不可变的\n仅通过var r ring.Ring语句声明的r将会是一个长度为1的循环链表，而List类型的零值则是一个长度为0的链表\nRing值的Len方法的算法复杂度是 O(N) 的，而List值的Len方法的算法复杂度则是 O(1) 的\n\n\n\n\n字典（map）\n\n键不可以是函数、字典、切片类型（需要能使用操作符==和!=），值可以是任何类型。map中不存储键的值，而是使用其hash值代表\n如果键的类型是接口类型的，那么键值的实际类型也不能是上述三种类型，否则在程序运行过程中会引发 panic\n如果键的类型是数组类型，那么还要确保该类型的元素类型不是函数类型、字典类型或切片类型\n如果键的类型是结构体类型，那么还要保证其中字段的类型的合法性\n\n\n注意事项\n优先考虑哪些类型作为字典的键类型：求哈希和判等操作的速度越快，越适合，优先使用数值和指针类型（因为求hash值的速度与类型的宽度成正比）\n在值为nil的字典上执行读/写操作：由于字典是引用类型，所以当仅声明而不初始化一个字典类型的变量的时候，它的值会是nil，除了对nil的字典添加键值对操作外，其它操作都不会跑出panic\n\n\n\n\n\n\n程序结构\n\n函数\n\n定义方式：func function_name( [parameter list] ) [return_types] &#123;...&#125;\n\n参数是数组并且在函数内更改了参数：原数组不会改变，所有传给函数的参数值都会被复制，函数在其内部使用的并不是参数值的原值，而是他的副本（数值）\n\n参数是切片并且在函数内更改了参数：原切片会改变，对于切片、字典、通道，只会拷贝他们本身，并不会拷贝他们引用的底层数据（引用）\n\n\nfunc modifyComplexArray(a [3][]string) [3][]string &#123;\n\ta[1][1] &#x3D; &quot;s&quot;&#x2F;&#x2F;a和s都改变，因为是引用类型的内部\n\ta[2] &#x3D; []string&#123;&quot;o&quot;, &quot;p&quot;, &quot;q&quot;&#125;&#x2F;&#x2F;只会改变a，s不变，因为改变的是数组\n\treturn a\n&#125;\n高阶函数：传入参数或返回参数\ntype operate func(x,y int) int\nfunc calculate(x int ,y int, op operate)(int ,error)&#123;\n  if op &#x3D;&#x3D; nil&#123;&#x2F;&#x2F;卫述语句：检查先决条件的合法性，如果未通过立即终止当前代码执行的语句\n    return 0,errors.New(&quot;invalid operation&quot;)\n  &#125;\n  return op(x, y), nil\n&#125;\n\nfunc main() &#123;\n  x, y &#x3D; 56, 78\n  op :&#x3D; func(x, y int) int&#123;\n    return x + y\n  &#125;\n  result, err &#x3D; calculate(x, y, op)\n&#125;\n闭包：内部逻辑并不完整，有一部分逻辑需要外来标识符参与完成，而此标识在函数定义时是未知的。表面上是延迟实现部分逻辑，实际上是在动态地生成那部分程序逻辑，类似于模版方法\n&#x2F;&#x2F;genCalculator就是高阶函数\nfunc genCalculator(op operate) calculateFunc &#123;\n  &#x2F;&#x2F;匿名的calculateFunc类型的闭包函数，内部需要op来实现，在调用时才知道是什么\n  return func(x int, y int) (int, error) &#123;\n    if op &#x3D;&#x3D; nil &#123;\n      return 0, errors.New(&quot;invalid operation&quot;)\n    &#125;\n    return op(x, y), nil\n  &#125;\n&#125;\n\n\n结构体\n&#x2F;&#x2F; AnimalCategory 代表动物分类学中的基本分类法。\ntype AnimalCategory struct &#123;\n  kingdom string &#x2F;&#x2F; 界。\n  phylum string &#x2F;&#x2F; 门。\n  class  string &#x2F;&#x2F; 纲。\n  order  string &#x2F;&#x2F; 目。\n  family string &#x2F;&#x2F; 科。\n  genus  string &#x2F;&#x2F; 属。\n  species string &#x2F;&#x2F; 种。\n&#125;\n&#x2F;&#x2F;嵌入字段\ntype Animal struct &#123;\n  scientificName string &#x2F;&#x2F; 学名。\n  AnimalCategory    &#x2F;&#x2F; 动物基本分类。\n&#125;\n&#x2F;&#x2F;方法名为String，接受者声明为AnimalCategory类型的ac，以在其中引用到当前值的任何一个字段，或者调用\n&#x2F;&#x2F;到当前值的任何一个方法（也包括String方法自己）,相当于java的toString\nfunc (ac AnimalCategory) String() string &#123;\n  return fmt.Sprintf(&quot;%s%s%s%s%s%s%s&quot;,\n    ac.kingdom, ac.phylum, ac.class, ac.order,\n    ac.family, ac.genus, ac.species)\n&#125;\nfunc (a Animal) Category() string &#123;\n  return a.AnimalCategory.String()\n&#125;\n接口\n\n无侵入式的接口实现方法：只要实现接口中的所有方法（方法签名一致+方法名一样）就一定是这个接口的实现类型（Duck typing）\n为接口变量赋值：当接口被赋值时，接口会获得动态值和动态类型（原来只具有静态类型），一起被存储在一个专用的数据结构中，叫做iface。iface实例会包含两个指针，一个是指向类型信息的指针，另一个是指向动态值的指针。这里的类型信息是由另一个专用数据结构的实例承载的，其中包含了动态值的类型，以及使它实现了接口的方法和调用它们的途径\n接口之间的组合：同名的方法会产生冲突，无法通过编译，推荐使用体量较小的接口\n\npackage main\n\nimport (\n    &quot;fmt&quot;\n)\n\ntype Phone interface &#123;\n    call()\n&#125;\n\ntype NokiaPhone struct &#123;\n&#125;\n\nfunc (nokiaPhone NokiaPhone) call() &#123;\n    fmt.Println(&quot;I am Nokia, I can call you!&quot;)\n&#125;\n\ntype IPhone struct &#123;\n&#125;\n\nfunc (iPhone IPhone) call() &#123;\n    fmt.Println(&quot;I am iPhone, I can call you!&quot;)\n&#125;\n\nfunc main() &#123;\n    var phone Phone\n\n    phone &#x3D; new(NokiaPhone)\n    phone.call()\n\n    phone &#x3D; new(IPhone)\n    phone.call()\n\n&#125;\n\n\n\n3.进阶语法\n多线程（go、chan、select）\n\nchan\n\nGo语言自带的唯一可以满足并发安全性的类型，示例如下\nfunc main() &#123;\n  &#x2F;&#x2F;chan 代表通道类型的关键字，int代表该通道类型的元素类型，3为通道容量，即通道可以缓存多少了元素值\n  &#x2F;&#x2F;通道长度为0时，称为非缓冲通道，也就是不带缓冲的通道。通道相当于先进先出的队列\n  ch1 :&#x3D; make(chan int, 3)\n  ch1 &lt;- 2&#x2F;&#x2F;接受和发送都需要&lt;-，形象的表示了元素值的传输方向\n  ch1 &lt;- 1\n  ch1 &lt;- 3\n  elem1 :&#x3D; &lt;-ch1\n  fmt.Printf(&quot;The first element received from channel ch1: %v\\\\n&quot;,elem1)&#x2F;&#x2F;2\n&#125;\n通道的特点\n\n对于同一个通道，发送操作之间是互斥的，接收操作之间也是互斥的\n发送操作和接收操作中对元素值的处理都是不可分割的，并且进通道和出通道的都是副本\n发送操作在完全完成之前会被阻塞。接收操作也是如此\n\n\n注意事项\n\n针对非缓冲通道，无论是发送还是接受操作，一开始执行就会被阻塞，直到配对的操作也开始执行，才会继续传递。由此可见，非缓冲通道是在用同步的方式传递数据。也就是说，只有收发双方对接上了，数据才会被传递\n对于值为nil的通道，不论它的具体类型是什么，对它的发送操作和接收操作都会永久地处于阻塞状态。由于通道类型是引用类型，所以它的零值就是nil，所以不要忘记初始化通道\n对于一个已初始化，但并未关闭的通道来说，收发操作一定不会引发 panic。但是通道一旦关闭，再对它进行发送操作，就会引发 panic。另外，如果我们试图关闭一个已经关闭了的通道，也会引发 panic\n当我们把接收表达式的结果同时赋给两个变量时，第二个变量的类型就是一定bool类型。它的值如果为false就说明通道已经关闭，并且再没有元素值可取了。如果通道关闭时，里面还有元素值未被取出，那么接收表达式的第一个结果，仍会是通道中的某一个元素值，而第二个结果值一定会是true。因此，通过接收表达式的第二个结果值，来判断通道是否关闭是可能有延时的\n\n\n单向通道：只能发不能收，或者只能收不能发的通道，var uselessChan = make(chan&lt;- int, 1)，紧挨在关键字chan右边的那个&lt;-，这表示了这个通道是单向的，并且只能发而不能收；类似的，如果这个操作符紧挨在chan的左边，那么就说明该通道只能收不能发，应用主要是约束代码行为（一般用在接口类型声明中的方法定义上，可以传入双向通道，会自动转换为单向）\n\n\n\nselect：只能与通道联用，一般由若干个分支组成，每次执行只有一个分支的代码会被执行。分支分为两种，一种叫做候选分支，另一种叫做默认分支。候选分支总是以关键字case开头，后跟一个case表达式和一个冒号，然后我们可以从下一行开始写入当分支被选中时需要执行的语句；默认分支其实就是 default case，因为，当且仅当没有候选分支被选中时它才会被执行，所以它以关键字default开头并直接后跟一个冒号\n&#x2F;&#x2F; 准备好几个通道\nintChannels :&#x3D; [3]chan int&#123;\n  make(chan int, 1),\n  make(chan int, 1),\n  make(chan int, 1),\n&#125;\n&#x2F;&#x2F; 随机选择一个通道，并向它发送元素值\nindex :&#x3D; rand.Intn(3)\nfmt.Printf(&quot;The index: %d\\\\n&quot;, index)\nintChannels[index] &lt;- index\n&#x2F;&#x2F; 哪一个通道中有可取的元素值，哪个对应的分支就会被执行。仅当select语句中的所有case表达式都被求值完毕(从上到下)后，它才会开始选择候选分支\nselect &#123;\ncase &lt;-intChannels[0]:\n  fmt.Println(&quot;The first candidate case is selected.&quot;)\ncase &lt;-intChannels[1]:\n  fmt.Println(&quot;The second candidate case is selected.&quot;)\ncase elem :&#x3D; &lt;-intChannels[2]:\n  fmt.Printf(&quot;The third candidate case is selected, the element is %d.\\\\n&quot;, elem)\ndefault:\n  fmt.Println(&quot;No candidate case is selected!&quot;)\n&#125;\n\n&#x2F;&#x2F;通过接收表达式的第二个结果值来判断通道是否已经关闭\nintChan :&#x3D; make(chan int, 1)\n&#x2F;&#x2F; 一秒后关闭通道。\ntime.AfterFunc(time.Second, func() &#123;\n  close(intChan)\n&#125;)\nselect &#123;\ncase _, ok :&#x3D; &lt;-intChan:\n  if !ok &#123;\n    fmt.Println(&quot;The candidate case is closed.&quot;)\n    break\n  &#125;\n  fmt.Println(&quot;The candidate case is selected.&quot;)\n&#125;\ngo\n\n并发模型：（G（goroutine 的缩写）、P（processor 的缩写）和 M（machine 的缩写））\n\n\nGo语句执行时，Go语言运行时系统会先试图从存放空闲G的队列中获取一个G（即goroutine），如果没有才会创建一个新的，然后包装将要执行的代码，并且加入到待执行的队列中，虽然很快就能执行，但是依旧会耗时。所以在go语言本身执行完毕后，Go程序完全不会等待go函数的执行，而是立刻执行后边的语句，即异步并发地执行。在执行完主goroutine的所有go语句后，主goroutine就会结束运行\npackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n  for i :&#x3D; 0; i &lt; 10; i++ &#123;\n    go func() &#123;\n      fmt.Println(i)&#x2F;&#x2F;不会打印任何东西\n    &#125;()\n  &#125;\n&#125;\n&#x2F;&#x2F;因为主goroutine先结束，而其他goroutine内部的语句还没开始执行\n让主goroutine等待其他goroutine：让主goroutine执行time.Sleep(time.Millisecond * 500) 或 利用通道\nfunc main() &#123;\n\tnum :&#x3D; 10\n  &#x2F;&#x2F;struct&#123;&#125;类似于空接口类型interface&#123;&#125;，代表既不包含任何字段也不拥有任何方法的空结构体类型\n  &#x2F;&#x2F;struct&#123;&#125;类型值的表示法只有一个，即struct&#123;&#125;&#123;&#125;，占用0字节内存空间，全局只有一份\n\tsign :&#x3D; make(chan struct&#123;&#125;, num)\n\n\tfor i :&#x3D; 0; i &lt; num; i++ &#123;\n\t\tgo func() &#123;\n\t\t\tfmt.Println(i)\n\t\t\tsign &lt;- struct&#123;&#125;&#123;&#125;\n\t\t&#125;()\n\t&#125;\n\tfor j :&#x3D; 0; j &lt; num; j++ &#123;\n\t\t&lt;-sign &#x2F;&#x2F;go语句全执行完，主goroutine才会全部不阻塞，顺利执行完\n\t&#125;\n&#125;\n让多个goroutine按既定顺序执行\nfunc main() &#123;\n  var count uint32\n  &#x2F;&#x2F;trigger函数会不断地获取一个名叫count的变量的值，并判断该值是否与参数i的值相同。如果相同，那么就\n  &#x2F;&#x2F;立即调用fn代表的函数，然后把count变量的值加1，最后显式地退出当前的循环。否则，我们就先让当前的\n  &#x2F;&#x2F;goroutine“睡眠”一个纳秒再进入下一个迭代。\n  trigger :&#x3D; func(i uint32, fn func()) &#123;\n    for &#123;\n      &#x2F;&#x2F;原子操作，count是一个信号，值总是下一个可以调用打印函数的go函数的序号\n      if n :&#x3D; atomic.LoadUint32(&amp;count); n &#x3D;&#x3D; i &#123;\n        fn()\n        atomic.AddUint32(&amp;count, 1)\n        break\n      &#125;\n      time.Sleep(time.Nanosecond)\n    &#125;\n  &#125;\n  for i :&#x3D; uint32(0); i &lt; 10; i++ &#123;\n    go func(i uint32) &#123;\n      fn :&#x3D; func() &#123;\n        fmt.Println(i)\n      &#125;\n      trigger(i, fn)\n    &#125;(i)\n  &#125;\n  &#x2F;&#x2F;让主 goroutine 最后一个运行完毕。当手动启用的 goroutine 都运行完毕之后，count的值一定会是10\n  &#x2F;&#x2F;所以把10作为了第一个参数值。又由于并不想打印这个10，所以把一个什么都不做的函数作为了第二个参数值\n  trigger(10, func() &#123;&#125;)\n&#125;\n\n\n\n\n错误处理（defer）\n\nerror类型是一个接口类型，也是一个Go语言的内建类型。在这个接口类型的声明中只包含一个方法Error，不接受任何参数，但会返回一个string类型的结果，作用是返回错误信息的字符串表示形式，相当于其他类型值的String方法。示例如下\npackage main\n\nimport (\n  &quot;errors&quot;\n  &quot;fmt&quot;\n)\n&#x2F;&#x2F;用在结果列表的最后，声明一个error类型的结果\nfunc echo(request string) (response string, err error) &#123;\n  if request &#x3D;&#x3D; &quot;&quot; &#123;\n    &#x2F;&#x2F;为err赋值，返回错误信息。err的静态类型是error，动态类型是errors包中的*errorString\n    err &#x3D; errors.New(&quot;empty request&quot;)\n    return\n  &#125;\n  response &#x3D; fmt.Sprintf(&quot;echo: %s&quot;, request)\n  return\n&#125;\n\nfunc main() &#123;\n  for _, req :&#x3D; range []string&#123;&quot;&quot;, &quot;hello!&quot;&#125; &#123;\n    fmt.Printf(&quot;request: %s\\\\n&quot;, req)\n    resp, err :&#x3D; echo(req)\n    if err !&#x3D; nil &#123;\n      &#x2F;&#x2F;fmt.Printf函数如果发现被打印的值是一个error类型的值，那么就会去调用它的Error方法\n      fmt.Printf(&quot;error: %s\\\\n&quot;, err)\n      continue\n    &#125;\n    fmt.Printf(&quot;response: %s\\\\n&quot;, resp)\n  &#125;\n&#125;\n\n\n对于类型在已知范围内的一系列错误值，一般使用类型断言表达式或类型switch语句来判断；\n&#x2F;&#x2F;os包中的几个代表错误的类型os.PathError、os.LinkError、os.SyscallError和os&#x2F;exec.Error\nfunc underlyingError(err error) error &#123;\n  switch err :&#x3D; err.(type) &#123;\n  case *os.PathError:\n    return err.Err\n  case *os.LinkError:\n    return err.Err\n  case *os.SyscallError:\n    return err.Err\n  case *exec.Error:\n    return err.Err\n  &#125;\n  return err\n&#125;\n对于已有相应变量且类型相同的一系列错误值，一般直接使用判等操作来判断；\nprintError :&#x3D; func(i int, err error) &#123;\n  if err &#x3D;&#x3D; nil &#123;\n    fmt.Println(&quot;nil error&quot;)\n    return\n  &#125;\n  err &#x3D; underlyingError(err)&#x2F;&#x2F;得到潜在错误值\n  switch err &#123;\n  case os.ErrClosed:\n    fmt.Printf(&quot;error(closed)[%d]: %s\\\\n&quot;, i, err)\n  case os.ErrInvalid:\n    fmt.Printf(&quot;error(invalid)[%d]: %s\\\\n&quot;, i, err)\n  case os.ErrPermission:\n    fmt.Printf(&quot;error(permission)[%d]: %s\\\\n&quot;, i, err)\n  &#125;\n&#125;\n对于没有相应变量且类型未知的一系列错误值，只能使用其错误信息的字符串表示形式来做判断。\n\n\n\npanic（运行时恐慌）：是一种程序异常，抛出panic时如果程序没有保护措施，就会打印出panic的详细信息，然后终止运行\npanic: runtime error: index out of range#运行时异常，panic包含一个runtime.Error接口类型的值\n\ngoroutine 1 [running]:#表示有一个ID为1的goroutine在此panic被引发的时候正在运行\nmain.main()#表明了这个goroutine包装的go函数就是命令源码文件中的那个main函数，即此为主goroutine\n &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q0&#x2F;demo47.go:5 +0x3d\nexit status 2#表明这个程序是以退出状态码2结束运行的\n\n\n从panic被引发到程序终止运行的大致过程：某行代码引发panic，从那行代码开始根据调用层级，反向依次终止函数，一直到最外层函数，即go函数/main函数，然后控制权被运行时系统收回，程序崩溃并终止运行，承载程序这次运行的进程也会随之消亡。在这个传播过程中，panic详情会逐步完善，最终打印\n怎么让panic包含一个值：通过内建函数panic，可以在程序运行期间报告异常，直接通过参数传入即可。如果某个值有可能会被记到日志里，那么就应该为它关联String方法。\n\n\nrecover：施加应对panic的保护措施，Go 语言的内建函数recover专用于恢复 panic，或者说平息运行时恐慌。recover函数无需任何参数，并且会返回一个空接口类型的值。defer语句就是被用来延迟执行代码的，延迟到该语句所在的函数即将执行结束的那一刻，无论结束执行的原因是什么。\npackage main\n\nimport (\n &quot;fmt&quot;\n &quot;errors&quot;\n)\n\nfunc main() &#123;\n fmt.Println(&quot;Enter function main.&quot;)\n defer func()&#123;&#x2F;&#x2F;类似于go语句的写法\n  fmt.Println(&quot;Enter defer function.&quot;)\n  if p :&#x3D; recover(); p !&#x3D; nil &#123;&#x2F;&#x2F;recover会返回空接口类型的结果值，如果没有panic，则值是nil\n   fmt.Printf(&quot;panic: %s\\\\n&quot;, p)\n  &#125;\n  fmt.Println(&quot;Exit defer function.&quot;)\n &#125;()\n &#x2F;&#x2F; 引发panic。\n panic(errors.New(&quot;something wrong&quot;))\n fmt.Println(&quot;Exit function main.&quot;)\n&#125;\ndefer：defer用于资源的释放，会在函数返回之前进行调用。一般采用如下模式\nf,err :&#x3D; os.Open(filename)\nif err !&#x3D; nil &#123;\n    panic(err)\n&#125;\ndefer f.Close()\n\n\n多条defer语句的执行顺序：在同一个函数中，defer函数调用的执行顺序与它们分别所属的defer语句的出现顺序（更严谨地说，是执行顺序）完全相反。当一个函数即将结束执行时，其中的写在最下边的defer函数调用会最先执行，其次是写在它上边、与它的距离最近的那个defer函数调用，以此类推，最上边的defer函数调用会最后一个执行。因为defer语句每次执行的时候，Go 语言会把它携带的defer函数及其参数值另行存储到一个链表中，这个链表是先进后出的，相当于栈\n\n\n\n\n库函数\n\nstrings\n\nGo使用Unicode编码规范中的UTF-8编码格式，一个string类型的值是由一系列对应的UTF-8编码值来表达。一个string类型的值可以被拆分成一个包含多个字符的序列，也可以被拆分为一个包含多个字节的序列。前者使用以rune为元素类型的切片来表示，后者则可以用一个以byte为元素类型的切片来表示。rune是Go特有的一个基本数据类型，一个值就代表一个字符（Unicode字符），它是一个int32类型的一个别名类型\nstr :&#x3D; &quot;Go爱好者&quot;\nfmt.Printf(&quot;The string: %q\\\\n&quot;, str)\n&#x2F;&#x2F;&#x3D;&gt; runes(char): [&#39;G&#39; &#39;o&#39; &#39;爱&#39; &#39;好&#39; &#39;者&#39;]\nfmt.Printf(&quot;  &#x3D;&gt; runes(char): %q\\\\n&quot;, []rune(str))\n&#x2F;&#x2F;&#x3D;&gt; runes(hex): [47 6f 7231 597d 8005]\nfmt.Printf(&quot;  &#x3D;&gt; runes(hex): %x\\\\n&quot;, []rune(str))\n&#x2F;&#x2F;&#x3D;&gt; bytes(hex): [47 6f e7 88 b1 e5 a5 bd e8 80 85]\nfmt.Printf(&quot;  &#x3D;&gt; bytes(hex): [% x]\\\\n&quot;, []byte(str))\n使用带有range子句的for语句遍历字符串：带有range子句的for语句会先把被遍历的字符串值拆成一个字节序列，然后再试图找出这个字节序列中包含的每一个 UTF-8 编码值，或者说每一个 Unicode 字符\nstr :&#x3D; &quot;Go爱好者&quot;\nfor i, c :&#x3D; range str &#123;\n fmt.Printf(&quot;%d: %q [% x]\\\\n&quot;, i, c, []byte(string(c)))\n&#125;\n&#x2F;&#x2F;0: &#39;G&#39; [47]\n&#x2F;&#x2F;1: &#39;o&#39; [6f]\n&#x2F;&#x2F;2: &#39;爱&#39; [e7 88 b1]\n&#x2F;&#x2F;5: &#39;好&#39; [e5 a5 bd]\n&#x2F;&#x2F;8: &#39;者&#39; [e8 80 85]\nstrings包中有strings.Builder类型的WriteRune方法、strings,Reader类型的ReadRune方法\n\n虽然string值能通过切片操作来裁剪或者通过操作符+来拼接，但是都需要拷贝到新的内存里，但是strings.Builder类型的值有以下优势：\n已存在的内容不可变，但可以拼接更多的内容：通过一个byte为元素的类型的切片来存储内容，通过一个unsafe.Pointer类型的字段来指向持有那个指向了底层字节数组的指针值。虽然可以进行任何操作，但是要求只能被拼接或完全覆盖\n减少了内存分配和内容拷贝的次数：容量不够或者调用Grow方法的时候才会扩容并拷贝数据\n可将内容重置，可重用值：通过Reset方法\n\n\nstrings.Builder类型在已被真正使用后就不可再被复制，否则会引发panic，但是可以复制指针值；并且由于其内容不是完全不可变的，所以需要使用方自行解决操作冲突和并发安全问题\nstrings.Reader类型的值可以高效地读取字符串，因为在读取过程中，Reader值会保存已读取的字节的计数，代表着下一次读取的起始位置，所以很容易计算出下一次读取的起始索引位置\n\n\n\n\nbytes\n\nstrings包和bytes包很多API是相似的，提供的函数的数量和功能也差别不大。主要区别是strings包主要面向Unicode字符和经过UTF-8编码的字符串，而bytes包面对的则主要是字节和字节切片，主要用途是作为字节序列的缓冲区\nbytes.Buffer的扩容策略：\n对于处在零值状态的Buffer值来说，如果第一次扩容时的另需字节数不大于64，那么该值就会基于一个预先定义好的、长度为64的字节数组来创建内容容器。在这种情况下，这个内容容器的容量就是64。这样做的目的是为了让Buffer值在刚被真正使用的时候就可以快速地做好准备\n如果可以（内容容器容量与其长度之差大于或等于需要的字节数）则会在当前的内容容器之上，进行长度扩容，即通过切片操作对原有的内容容器的长度进行扩容\n如果内容容器剩余容量不够，那么就会用新的内容容器去替代原有的内容容器，进行扩容。如果当前内容容器的容量的一半，仍然大于或等于其现有长度（即未读字节数）再加上另需的字节数的和，那么，扩容代码就会复用现有的内容容器，并把容器中的未读内容拷贝到它的头部位置。否则就创建一个新的内容容器，新容器的容量等于原有容量的二倍再加上另需字节数的和\n\n\n在bytes.Buffer中，Bytes方法和Next方法都可能会造成内容的泄露。原因在于，它们都把基于内容容器的切片直接返回给了方法的调用方，而且通过切片可以直接访问和操纵它的底层数组\n\n\nio\n\nstrings.Builder、strings.Reader和bytes.Buffer都分别实现了很多io包中的接口，io包中接口的优势是可以提供不同程序实体之间的互操作性\n\nio包中的接口及其关系\n\n核心接口：io.Reader、io.Writer、io.Closer\n\nio包中的简单接口共有 11 个。其中，读取操作相关的接口有 5 个，写入操作相关的接口有 4 个，而与关闭操作有关的接口只有 1 个，另外还有一个读写位置设定相关的接口。此外，io包还包含了 9 个基于这些简单接口的扩展接口\n\n\n\n\n\n\nbufio\n\nbufio是buffed I/O的缩写，即实现的I/O操作都内置了缓冲区，主要的数据类型有Reader、Scanner、Writer、ReadWriter\nbufio.Reader类型值中的缓冲区的作用：是一个数据存储中介，介于底层读取器（初始化时传入的io.Reader）与读取方法及其调用方之间。Reader值的读取方法一般都会先从其所属值的缓冲区中读取数据。同时，在必要的时候，它们还会预先从底层读取器那里读出一部分数据，并暂存于缓冲区之中以备后用。可以降低读取方法的执行时间。（fill函数）\nbufio.Reader类型读取方法有哪些不同\nPeek：读取并返回其缓冲区中的n个未读字节，并且它会从已读计数代表的索引位置开始读。即使它读取了缓冲区中的数据，也不会更改已读计数的值。\nRead：有时会把缓冲区中的未读字节，依次拷贝到其参数p代表的字节切片中，并立即根据实际拷贝的字节数增加已读计数的值\n在缓冲区中还有未读字节的情况下，该方法的做法就是如此。不过，在另一些时候，其所属值的已读计数会等于已写计数，这表明：此时的缓冲区中已经没有任何未读的字节了。\n当缓冲区中已无未读字节时，Read方法会先检查参数p的长度是否大于或等于缓冲区的长度。如果是，那么Read方法会索性放弃向缓冲区中填充数据，转而直接从其底层读取器中读出数据并拷贝到p中。这意味着它完全跨过了缓冲区，并直连了数据供需的双方。\n\n\nReadSlice：先在其缓冲区的未读部分中寻找分隔符。如果未能找到，并且缓冲区未满，那么该方法会先通过调用fill方法对缓冲区进行填充，然后再次寻找，如此往复\nReadBytes：会通过调用ReadSlice方法一次又一次地从缓冲区中读取数据，直至找到分隔符为止。在这个过程中，ReadSlice方法可能会因缓冲区已满而返回所有已读到的字节和相应的错误值，但ReadBytes方法总是会忽略掉这样的错误，并再次调用ReadSlice方法，这使得后者会继续填充缓冲区并在其中寻找分隔符。\n\n\n\n\nos\n\nos包中的API可以帮助我们使用操作系统中的文件系统、权限系统、环境变量、系统进程、系统信号\n\n\n\n\nnet\n\n网络编程底层以来socket系统调用，是一种IPC （Inter-Process Communication）方法。在syscall代码包中有一个与这个socket系统调用对应的函数，两者的函数签名基本一致，都会接受三个int类型的参数（通信域、类型、使用的协议），并返回一个可以代表文件描述符的结果\n\n\n在调用net.Dial函数的时候，会为它的两个参数设定值。其中的第一个参数名为network，它决定 Go 程序在底层会创建什么样的 socket 实例，并使用什么样的协议与其他程序通信，第二个参数是address。参数network有以下可选值：tcp, tcp4, tcp6, udp, udp4, udp6, unix, unixgram, unixpacket\n\nnet/http代码包\n\n使用：只需要传给它一个URL就可以，http.Get函数会返回两个结果值，第一个结果值的类型是*http.Response，它是网络服务给我们传回来的响应内容的结构化表示。第二个结果值是error类型的，它代表了在创建和发送HTTP 请求，以及接收和解析 HTTP 响应的过程中可能发生的错误。http.Get函数会在内部使用缺省的 HTTP 客户端，并且调用它的Get方法以完成功能。这个缺省的 HTTP 客户端是由net/http包中的公开变量DefaultClient代表的，其类型是*http.Client\nurl1 :&#x3D; &quot;&lt;http:&#x2F;&#x2F;google.cn&gt;&quot;\nfmt.Printf(&quot;Send request to %q with method GET ...\\\\n&quot;, url1)\nresp1, err :&#x3D; http.Get(url1)\nif err !&#x3D; nil &#123;\n  fmt.Printf(&quot;request sending error: %v\\\\n&quot;, err)\n&#125;\ndefer resp1.Body.Close()\nline1 :&#x3D; resp1.Proto + &quot; &quot; + resp1.Status\nfmt.Printf(&quot;The first line of response:\\\\n%s\\\\n&quot;, line1)\nhttp.Client类型中的Transport字段：\n\n向网络服务发送 HTTP 请求，并从网络服务接收 HTTP 响应的操作过程。也就是说，该字段的方法RoundTrip应该实现单次 HTTP 事务（或者说基于 HTTP 协议的单次交互）需要的所有步骤\n这个字段是http.RoundTripper接口类型的，它有一个由http.DefaultTransport变量代表的缺省值（以下简称DefaultTransport）。当我们在初始化一个http.Client类型的值（以下简称Client值）的时候，如果没有显式地为该字段赋值，那么这个Client值就会直接使用DefaultTransport\nhttp.Client类型的Timeout字段，代表的正是前面所说的单次 HTTP 事务的超时时间，它是time.Duration类型的。它的零值是可用的，用于表示没有设置超时时间。\n\n\nhttp.Server类型的ListenAndServe方法：http.Server类型与http.Client是相对应的。http.Server代表的是基于 HTTP 协议的服务端，或者说网络服务。http.Server类型的ListenAndServe方法的功能是：监听一个基于 TCP 协议的网络地址，并对接收到的 HTTP 请求进行处理。这个方法会默认开启针对网络连接的存活探测机制，以保证连接是持久的。同时，该方法会一直执行，直到有严重的错误发生或者被外界关掉。当被外界关掉时，它会返回一个由http.ErrServerClosed变量代表的错误值。\n\nnet.Listen函数都做了哪些事情：解析参数值中包含的网络地址隐含的 IP 地址和端口号；根据给定的网络协议，确定监听的方法，并开始进行监听。\nhttp.Server类型的Serve方法是怎样接受和处理 HTTP 请求的：在一个for循环中，网络监听器的Accept方法会被不断地调用，该方法会返回两个结果值；第一个结果值是net.Conn类型的，它会代表包含了新到来的 HTTP 请求的网络连接；第二个结果值是代表了可能发生的错误的error类型值。\n\n\n\n\n\n\n\n\n\n","slug":"Go","date":"2023-05-04T04:42:41.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"838ae74e3a76757d637de803a615bfd9","title":"MySQL","content":"MySQL1.SQL语法\n数据库概念：数据库（DB）、数据库管理系统（DBMS）、数据库系统（软件+数据库+DBA）、数据库管理员（DBA）、元祖（tuple 一行）、码（列）、候选码（唯一标识元祖）、主码（主键）、外码（另一表的主键）、主属性（候选码中的属性）、非主属性、注释（##，–，/* */）、SQL语句不区分大小写（MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写）\n\n表设计\n\nE-R图（Entity Relationship Diagram 实体+属性+联系「1:1, 1:N, M:N」）\n范式\n1NF：强调列的原子性，列不可再分\n2NF：1NF基础上，表必须有一个主键+非主键列不能部分依赖主键\n3NF：2NF基础上，非主键列必须不能传递依赖主键\nBCNF：关系模式中每一个决定因素都包含候选键，只要A能决定B，A内部就必须有主键列\n\n\n\n\n常见数据类型\n\n整数类型：（TINYINT(1)、SMALLINT(2)、MEDIUMINT(3)、INT(4)、BIGINT(8)）\n小数类型：浮点数（FLOAT、DOUBLE）、定点数（DECIMAL、NUMERIC）\n字符串类型：CHAR、VARCHAR、BLOB、TEXT\nVARCHAR：可变长度最大为65535、存储附加元信息、超出长度返回警告（CHAR直接截断）\nInnoDB会将长度超过768字节的定长字段存储为变长字段，可以跨页存储。例如：CHAR(255)在utf8mb4字符集（字符编码可能超过3字节）下可能会被存储成变长字段\nCHAR会截断尾空格，VARCHAR不会，插入没有尾空格的数据时，使用=查找时有没有尾空格都可以查出数据（自动补空格），但是用like查找时查不出没有空格的\n\n\n日期类型：DATE（YYYY-MM-DD）、TIME（hh:mm:ss[.fraction]）、DATETIME（YYYY-MM-DD hh:mm:ss[.fraction]）、TIMESTAMP（从1970年开始的秒数）、YEAR（YYYY）\n不要用字符串存储日期：占用空间大、查询慢（逐个字符进行比对）、无法用日期相关的函数\n数值型时间戳：这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便；缺点是可读性太差，无法直观的看到具体时间\nDatetime和 Timestamp是 MySQL 提供的两种比较相似的保存时间的数据类型，通常会首选Timestamp\nDateTime 类型没有时区信息，导致服务器更换地址的时候，数据库读出的时间有错误\nTimestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，在不同时区，查询同一条记录值会不一样\nTimestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小（1970年到2037年）\n\n\n\n\n\n\n基础SQL语句\n\n数据定义语言（DDL）：CREATE、ALTER、DROP、USE、ADD\n操作对象：视图（VIEW）、表（TABLE）、索引（INDEX）\n修饰约束：NOT NULL、UNIQUE、PRIMARY KEY、FOREIGN KEY、CHECK、DEFULT、KEY\n\n\n数据操纵语言（DML）：INSERT、UPDATE、DELETE、SELECT\n约束：DISTINCT返回不同值、LIMIT限制返回行数、ORDER BY排序（ASC升序、DESC降序）\n子查询：子查询可以嵌入 SELECT、INSERT、UPDATE和 DELETE语句中（需要放入()中），也可以和 =、&lt;、&gt;、&lt;&gt;、&gt;=、&lt;=、IN、BETWEEN、EXISTS、LIKE（%或_）、AND、OR、NOT等运算符一起使用\n分组：group by、聚合（count，max，sum，avg忽律null行）、having用于对汇总的 group by结果进行过滤\n连接：join…on…、join…using…（列名相同）、join默认是inner join（还有left join、right join、full join、self join需命名一个表、cross join笛卡尔积）\nstraight_join 让 MySQL 使用固定的连接方式执行查询\n\n\n组合：UNION运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自UNION中参与查询的提取行\n\n\n事务控制语言：COMMIT、ROLLBACK\n不能回退 SELECT语句，回退 SELECT语句也没意义；也不能回退 CREATE和 DROP语句；默认每一条语句都当成一个事务进行提交\n当出现 START TRANSACTION语句时，会关闭隐式提交；当 COMMIT或 ROLLBACK语句执行后，事务会自动关闭，重新恢复隐式提交\n通过 set autocommit=0可以取消自动提交，直到 set autocommit=1才会提交；autocommit标记是针对每个连接而不是针对服务器\n\n\n\n\n进阶SQL语句\n\nshow processlist\n\ncommand（查看连接状态）：sleep（空闲连接）、\nstate：\n\nmysql&gt; show processlist;\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\n| Id | User            | Host      | db   | Command | Time   | State                  | Info        |\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\n|  5 | event_scheduler | localhost | NULL | Daemon  | 610663 | Waiting on empty queue | NULL         |\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\ndelimiter\n\nexplain：并不会真的执行语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息\n\ntype表的访问方法\npossible_keys可能用到的索引\nkey实际用到的索引\nrows预计要读取的行数\nfiltered按表条件过滤后，留存的记录数的百分比\nExtra 字段的 Using index，表示的是使用了覆盖索引\n\nmysql&gt; explain select * from t where a between 10000 and 20000;\n+--+-----------+-----+----------+----+-------------+---+-------+---+----+--------+------------------+\n|id|select_type|table|partitions|type|possible_keys|key|key_len|ref|rows|filtered|Extra    |\n+----+---------+-----+----------+----+-------------+------+---------+------+-------+----------+-----------------------+\n| 1| SIMPLE    | t   | NULL     |range| a          | a | 5     | NULL | 10001 | 100.00 | Using index condition |\n通过查询 sys库的 schema_unused_indexes视图来查询哪些索引从未被使用\n\nkill\n\nkill query + 线程 id：终止这个线程中正在执行的语句\n把session的运行状态改成THD::KILL_QUERY（将变量 killed 赋值为THD::KILL_QUERY），给session的执行线程发信号\nsession语句中执行到预埋点后才可以终止语句逻辑，处于等待状态的必须是可唤醒的等待\n\n\nkill connection + 线程 id：断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的\nkill query + 线程 id失效的情况：show processlist的时候，看到Command列显示为 killed\n线程没有执行到判断线程状态的逻辑\n等行锁时使用pthread_cond_timedwait函数，虽然可被唤醒但是唤醒后的执行逻辑并没有判断线程状态\n由于 IO 压力过大，读写 IO 的函数一直无法返回，导致不能及时判断线程的状态\n\n\n终止逻辑耗时较长\n超大事务执行期间被 kill：这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长\n大查询回滚：如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长\nDDL 命令执行到最后阶段，如果被 kill，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久\n\n\n直接在客户端通过 Ctrl+C 命令也无法终止：由于 MySQL 是停等协议，所以这个线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，执行 Ctrl+C 的时候，是 MySQL 客户端另外启动一个连接，然后发送一个 kill query 命令\n\n\n\n\n\n\n\n2.基础知识1.基础架构\n\n\n\n\n\n\n\n\n客户端+server层+存储引擎，其中server层包括五部分，连接器（身份权限验证）、查询缓存（键值对，易失效，8.0移除）、分析器（词法分析+语法分析，返回出错位置）、优化器（选择索引，按照最优方案执行）、执行器（检验表权限，操作引擎，返回结果）\n\n查询语句执行流程；客户端验证登陆并通过TCP三次握手==连接==服务端，提交的执行语句经过分析器进行词法分析和语法分析通过后，提交给优化器来生成最优的执行方案，最后交给引擎来具体执行\n\n查询缓存不命中的情况    \n任何两个查询在任何字符上的不同都会导致缓存不命中\n如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存\n表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效\n\n\n客户端长时间（wait_timeout）没有命令时，连接器会自动断开\n数据传输（net_buffer）：服务端不保存一个完整的结果集，而是将取到的每一行写入net_buffer中，写满就发送然后清空；如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送\nMySQL 客户端发送请求后，接收服务端返回结果的方式有两种，默认使用第一种，加上-quick后使用第二种\n一种是本地缓存，也就是在本地开一片内存，先把结果存起来。如果用 API 开发，对应的就是 mysql_store_result 方法\n另一种是不缓存，读一个处理一个。如果用 API 开发，对应的就是 mysql_use_result 方法\n\n\n对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，建议使用mysql_store_result这个接口，直接把查询结果保存到本地内存，否则使用mysql_use_result接口\n\n\n\n\n更新语句执行流程：查询缓存，调用引擎API写入数据，InnoDB通过==两阶段提交==记录日志，流程为redo log（prepare）-&gt;binlog-&gt;redo log（commit）\n\n异常时：有prepare，但没有binlog，则回滚事务；由prepare、binlog，但没有commit，则提交事务恢复数据\n非两阶段提交：先写redo log然后宕机，虽然可以通过redo log恢复数据，但是通过binlog备份的时候会丢失数据；先写binlog然后宕机，本地无法通过redo log恢复数据，通过binlog备份时会多出一条事务\nchange buffer：当有更新操作，如果数据页在内存中，则直接更新数据页；如果数据页不在内存中，则会将更新操作先缓存在change buffer中。在后续数据页读入到内存中时执行merger操作，即将change buffer内的更改同步到数据页\n使用场景：适用于普通索引，但唯一索引需要每次都取数据确定唯一性；适用于写多读少的情况（merge操作少）\nchange buffer和redo log：redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗\n\n\n\n\n存储引擎对比\n\nMyISAM 不支持==事务==（MVCC+Next-Key Lock区间锁）和==行级锁==，而且最大的缺陷就是崩溃后无法安全恢复（只有binlog无==redo log==）\n\nMemory引擎：主要用于内存临时表的场景（没有并发问题、不许持久化数据、主备库之间不冲突）\n\n索引组织形式\n\nInnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，称为索引组织表（Index Organizied Table），整体结构为B+树，数据有序存放，数据位置变化时只需要改主键索引，查找需要回表\nMemory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，称为堆组织表（Heap Organizied Table），整体结构为hash表，数据按写入顺序存放，数据位置变化时需要改所有索引\n使用b树索引：alter table t1 add index a_btree_index using btree (id);\n\n\nInnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同\n\n生产环境不使用内存表的原因：内存表不支持行锁，只支持表锁；数据库重启后，所有内存表都会被清空\n\n\n\n\n\n\n2.日志\nredo log\n定义：InnoDB特有的，组织成大小为4*1GB（文件组）的一个环形缓冲区，使用两个指针记录位置，write pos（下一次写入位置）+ check point（等待擦除的位置），满了之后就阻塞等待，主要用于MySQL崩溃（实例挂了/宕机）后的恢复\nredo log buffer：查询和删除都是直接操作Buffer Pool中的数据页，更新时记录到redo log buffer中，然后刷盘到redo log\n记录条目：“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”\n刷盘时机：innodb_flush_log_at_trx_commit 参数确定何时刷新、一个后台定时线程每秒刷新、redo log buffer占用的空间即将达到 innodb_log_buffer_size一半时刷新\n\n\n写入流程：\n\n\nbinlog\n定义：server层的通用模块，与redo log记录物理日志（在哪个数据页上做了什么）不同，binlog记录逻辑日志，即语句的原始逻辑（在哪个表上做了什么），并且不会覆盖已有日志，直接写入新文件。主要用于数据备份和主从数据同步\n格式；statement（SQL语句原文）、row（SQL语句+数据）、mixed（MySQL选择用哪一个）\n\n\nbinlog cache：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。通过binlog_cache_size确定空间大小，一个事务的binlog不能被拆开，空间不够时需要暂存到磁盘上\n数据先write到文件系统的page cache，再fsync到磁盘，由参数sync_binlog控制write和fsync的时机\n\n\n为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的：MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中。redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中\n\n\nundo log\n用途：用于事务执行异常时进行回滚、提供MVCC机制需要的历史版本数据\n回滚日志先于数据持久化到磁盘上，在事务执行中宕机也可以回滚已执行的一半事务\n\n\n\n\n\n慢查询日志\n记录了执行时间超过long_query_time（默认10s，通常设置为1s）的所有查询语句，在解决SQL慢查询的时候经常用到\n命令：开启（SET GLOBAL slow slow_query_log=ON）、查看状态（show variables like “slow_query_log”; ）\n\n\n中转日志（relay log）\n\n3.锁\n全局锁：Flush tables with read lock;，主要用于做主库逻辑备份，其它全库备份方法如下\nmysqldump+–single-transaction：在一致性读隔离级别开启一个事务，来确保拿到一致性视图，通过MVCC来保证数据可正常更新，需要引擎支持一致性读级别（InnoDB支持MyISAM不支持）\nset global readonly=true：可以让全库进入只读状态，但一方面readonly会有其他用处这样改有副作用，另一方面数据库异常后不自动改此值，导致数据库一直不可写（全局锁自动释放）\n\n\n表级锁（MyISAM、InnoDB）：针对非索引字段加锁，对当前操作的整张表加锁，实现简单，资源消耗少，不会出现死锁，但是高并发下效率低\n表锁：lock tables t1 read, t2 write;\n可以使用unlock tables主动释放锁，也可以在客户端断开的时候自动释放，所以建议把可能影响并发度的锁尽量往后放\nlock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象，比如线程A执行上面的示例语句，线程B写t1、读写t2都会被阻塞；线程A解锁前也只能读t1、读写t2\n意向锁：用表锁的时候快速判断表中的记录是否有行锁，意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁（意向共享锁、意向排他锁）\n\n\n元数据锁（MDL）：MDL 不需要显式使用，在访问一个表的时候会被自动加上，语句执行开始时申请，但是在整个事务提交后才释放（可以通过加超时机制防止阻塞太多后续命令）\n在MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁\n读锁之间不互斥，因此可以有多个线程同时对一张表增删改查；读写锁之间、写锁之间是互斥的，因此两个线程同时给一个表加字段，其中一个要等另一个执行完才能开始执行\n\n\n\n\n行级锁（InnoDB）：针对索引字段进行加锁，只针对当前操作的行记录进行加锁，锁粒度小、并发度高、锁开销大，会出现死锁\n两阶段锁：行锁是在需要的时候加上去的，但是要等事务结束时才释放\n行锁是针对索引字段加的锁，如果where语句中字段没有命中唯一索引或者索引失效时，会导致扫描全表，对表中的所有行记录加锁，但有的时候即使用了索引，也会全表扫描（优化器的原因）\nInnoDB有哪几类行锁：REPEATABLE-READ隔离级别下，默认使用Next-Key Lock，操作的索引是唯一索引或主键时，优化降级为Record Lock\n记录锁（Record Lock） ：也被称为记录锁，属于单个行记录上的锁\n间隙锁（Gap Lock） ：锁定一个范围，不包括记录本身。跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作，两个间隙锁之间不存在冲突关系\n两个线程都拿到了同一间隙锁，然后在执行插入时等待对方的间隙锁，这就导致了同样的语句因为间隙锁的存在会锁住更大的范围而产生死锁\n\n\n临键锁（Next-Key Lock） ：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁\n每个 next-key lock 是前开后闭区间，对于正无穷使用了一个不存在的最大值 supremum 代替（保证闭区间）\n\n\n\n\n\n\n\n4.事务\n基础概念\n\nACID属性：Atomic、Consistency、Isolation、Durability（AID是手段，C是目的）\n并发带来的问题：脏读（读后被回滚）、丢失修改（写后被覆盖）、不可重复读（两次读结果不同）、幻读（第二次读到的行数多了）\n解决幻读的方法：提升事务隔离级别到可序列化、可重复读级别下添加表锁或添加Next-key Lock（记录锁+间隙锁）、隔离级别降到读提交并将binlog改成row格式（记录更改前后数据）\n\n\n\n\nMVCC\n\n原理：实现依赖==隐藏字段==、==Read View==、==undo log==。在内部实现中，InnoDB 通过数据行的DB_TRX_ID（事务id）和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\nMySQL 中每条记录在更新的时候都会同时记录一条回滚操作（在回滚日志中，在没有比该条日志更旧的read-view后自动删除），记录上的最新值，通过回滚操作，都可以得到前一个状态的值\nMVCC解决部分幻读：MVCC只能解决读取数据是的幻读（当前事务读取时，不受其他事务修改的影响），但是不能解决写入时的幻读（需要MVCC+锁、或者可串行化事务隔离级别）\n与间隙锁的对比：间隙锁锁定索引范围而非实际数据的锁，MVCC与间隙锁的目的都是保证数据库的并发访问安全性，但是MVCC的优势是没有用到锁，性能比间隙锁更好\n\n\n相关字段\nInnoDB为每一行添加了三个隐藏字段\nDB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务 id\nDB_ROLL_PTR（7字节）回滚指针，指向该行的 undo log\nDB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引\n\n\nRead View：用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”\nm_low_limit_id：目前出现过的最大的事务 ID+1，即下一个将被分配的事务 ID。大于等于这个 ID 的数据版本均不可见\nm_up_limit_id：活跃事务列表 m_ids 中最小的事务 ID，如果 m_ids 为空，则 m_up_limit_id 为 m_low_limit_id。小于这个 ID 的数据版本均可见\nm_ids：Read View 创建时其他未提交的活跃事务 ID 列表。创建 Read View时，将当前未提交事务 ID 记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids 不包括当前事务自己和已提交的事务（正在内存中）\nm_creator_trx_id：创建该 Read View 的事务 ID\n\n\nundo-log：事务回滚时恢复数据，分为两类\ninsert undo log：指在 insert操作中产生的 undo log。因为 insert操作的记录只对事务本身可见，对其他事务不可见，故该 undo log可以在事务提交后直接删除。不需要进行 purge操作\nupdate undo log：update或 delete操作中产生的 undo log。该 undo log可能需要提供 MVCC机制，因此不能在事务提交时就进行删除。提交时放入 undo log链表，等待 purge线程进行最后的删除\n\n\n\n\n底层实现（未完待续）：数据可达性算法\n\n\n事务隔离机制：读未提交、读已提交、可重复读（默认级别）、可序列化\nSET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE]\n\n\n避免长事务的方式：通过information_schema.innodb_trx表监控事务的持续时间、增加undo表空间、通过配置参数max_execution_time指定事务执行的最长时间、利用pt工具监控长事务\n可重复读与幻读\n标准的SQL隔离级别定义里，可重复读是不可以防止幻读的，但是InnoDB实现的可重复读隔离级别可以解决幻读问题\n快照读（一致性非锁定读）：由MVCC机制保证不出现幻读\nRR/RC级别select默认是快照读（RC级别读锁定行最新快照数据，RR级别读事务开始的数据）、读取到的行正在执行update或delete则不等待锁释放直接读取快照\n\n\n当前读（锁定读）：由Next-Key Lock加锁来防止幻读\nselect加锁（lock in share mode共享锁、for update排他锁）是当前读、update、insert、delete\nRR级别：扫描到的数据都会加行锁和间隙锁，并在commit时释放\nRC级别：扫描到的数据都会加行锁，但不满足条件的数据，不需等到commit，扫描完就释放\n\n\n\n\n\n\n\n5.索引\nB+树：索引的底层数据结构，InnoDB中每个节点使用一个页（page），页的大小为16KB，元数据占128字节，一条记录大约16字节，对于非叶节点，可以存1000条记录，对于叶节点，假设可以存100条数据，综上，对于一颗3层B+树，可以存储1亿条记录，充分利用局部性原理减少IO次数\n其它索引结构：Hash索引不支持顺序和范围查询、二叉查找树容易不平衡、平衡二叉树由于旋转耗时，删树数据时效率很低、红黑树效率高但是高度太高增加IO次数、B树节点过大增加IO次数\nB树和B+树的区别：B+树非叶子不存数据、叶子节点有一条引用链指向其它相邻叶子节点所以可直接对链表进行遍历、B+树查到叶子才返回数据可在非叶子节点中重复出现\nMyISAM和InnoDB引擎对B+树的不同实现\nMyISAM中，叶子节点的data域存放的是数据记录的地址，需要通过改地址读取对应的数据记录\nInnoDB中，索引文件和数据文件是分离的，数据文件是以主键为索引的key形成的树，叶子节点保存了完整的数据，其他的索引的叶子节点存储的是主键的值。所以通过主键查找直接能找到数据，通过其他索引只能找到对应主键，然后再根据主键去数据文件找。所以建议使用单调的字段作为主键，防止造成主索引频繁分裂（B+树的插入机制）\n索引结构和数据一起存放的索引称为聚簇索引，如InnoDB的主键索引；索引结构和数据分开存放的索引称为非聚簇索引，如InnoDB的辅助索引\n\n\n\n\n主键索引（聚簇索引，clustered index）和非主键索引（二级索引，secondary index）\n主键索引：加速查询 + 列值唯一（不可以有 NULL）+ 表中只有一个，查询速度快但更新代价大，所以一般都是不可修改的\n当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引且不允许存在 null 值的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键\n建表语句里一定要有自增主键，只有在类似于哈希表的数据表中才会使用业务字段直接锁主键\n重建主键索引的方法：直接删除重建会使得所有非主键索引都失效，推荐方法为用空的alter操作，比如ALTER TABLE t1 ENGINE = InnoDB;这样子就会原地重建表结构\n\n\n非主键索引：叶子节点存储的数据是主键，需要根据主键去主键索引在搜索一次（回表），更新代价小但需要回表\n唯一索引：加速查询 + 列值唯一（可以有 NULL），主要为了保证属性列的数据的唯一性\n普通索引：仅加速查询，允许重复、允许为NULL、允许创建多个\n前缀索引(Prefix)：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符（alter table SUser add index index2(email(6));）\n倒序索引：select field_list from t where id_card = reverse(&#39;input_id_card_string&#39;);\n前缀索引对覆盖索引的影响：使用前缀索引就用不上覆盖索引对查询性能的优化了，因为无法确定前缀索引是否截断了完整信息\n\n\n全文索引：对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替\n\n\n为什么不推荐使用外键和级联（主键改外键需要跟着改）：不适用高并发、分库分表不友好、增加复杂性（外键约束、业务变化）\n外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度\n增加了复杂性：\n每次做 DELETE 或者 UPDATE 都必须考虑外键约束，会导致开发的时候很痛苦, 测试数据极为不方便;\n外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦\n\n\n对分库分表不友好：因为分库分表下外键是无法生效的\n\n\n\n\n联合索引及相关优化：覆盖索引、最左前缀匹配原则、索引下推\n联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并，实现为ALTER TABLE cus_order ADD INDEX id_score_name(score, name);\n覆盖索引：一个索引叶子节点数据包含（或者说覆盖）所有需要查询的字段的值，可以不用二次查询，比如在非主键索引查记录的主键可以不用回表\n最左前缀匹配原则：在使用联合索引时，MySQL会根据联合索引中的字段顺序，从左到右依次到查询条件中去匹配，如果查询条件中存在与联合索引中最左侧字段相匹配的字段，则就会使用该字段过滤一批数据，直至联合索引中全部字段匹配完成，或者在执行过程中遇到范围查询（如**&gt;、&lt;**）才会停止匹配。所以在使用联合索引时，可以将区分度高的字段放在最左边，这样可以过滤掉更多数据\n索引下推：MySQL 5.6 版本中提供的一项索引优化功能，可以在索引遍历过程中，对索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数\n\n\n非主键索引默认与主键建立联合索引，可以减少需要的联合索引个数\n\n\n\n3.进阶知识1.索引选择\n\n\n\n\n\n\n\n\n选择合适的字段创建索引（不为NULL、被频繁查询、被作为条件查询、频繁需要排序的、频繁用于连接的）；频繁用于更新的字段不适合建立索引，维护索引的成本很高；索引数量不能过多，避免冗余索引\n\n索引选择：\n\n指标\n\n预估扫描行数：show index的cardinality列反应的是索引的基数（索引上不同值个数），通过使用采样统计选择M个数据页，统计每个页面上不同值个数，然后求求平均再乘索引的页面数得到索引的基数（变更的数据行超过1/M时重新统计）\n\nanalyze table tableName;：当索引的统计信息不对时，可以用来重新统计索引信息\n\nMySQL有两种存储索引统计的方式通过设置参数 innodb_stats_persistent 的值来选择：\n\n设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10\n设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16\n\n\n\n\n是否需要回表：选择不需要回表的作为索引\n\n是否需要再次排序：选择已排序列为索引\n\n\n\n引导优化器选择索引的方法\n\n采用 force index 强行选择一个索引：select * from t force index(a) where a between 10000 and 20000;\n修改语句，引导 MySQL 使用我们期望的索引：在保证业务正确的前提下，进行一些优化：如order by b limit 1 改为 order by b,a limit 1，可以使其使用a为索引\n新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引\n\n\n通过查询sys库的schema_unused_indexes视图来查询哪些索引从未被使用\n\n\n\n对索引字段进行函数操作，优化器会放弃走树搜索功能\n\n条件字段函数操作：如果对字段做了函数计算，就用不上索引了，这是 MySQL 的规定\n\n问题SQL：select count(*) from tradelog where month(t_modified)=7;\n\n原因：对索引字段做函数操作（包括+1操作），可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能，但是不代表放弃这个索引，而是继续根据之前指标来确定索引\n\n改进：把 SQL 语句改成基于字段本身的范围查询，这样优化器就能用上 t_modified 索引的快速定位能力了，否则需要进行全表扫描\nmysql&gt; select count(*) from tradelog where\n    -&gt; (t_modified &gt;&#x3D; &#39;2016-7-1&#39; and t_modified&lt;&#39;2016-8-1&#39;) or\n    -&gt; (t_modified &gt;&#x3D; &#39;2017-7-1&#39; and t_modified&lt;&#39;2017-8-1&#39;) or \n    -&gt; (t_modified &gt;&#x3D; &#39;2018-7-1&#39; and t_modified&lt;&#39;2018-8-1&#39;);\n\n\n隐式类型转换\n\n问题SQL：select * from tradelog where tradeid=110717;\n原因：tradeid字段是varchar(32)，输入的参数确实整型，所以需要做类型转换，这里的类型转换规则是字符串和数组做比较，将字符串转换成数字\n对于优化器来说，上面的语句相当于：select * from tradelog where CAST(tradid AS signed int) = 110717;，即对索引字段使用了函数，优化器放弃走树搜索功能\n\n\n\n\n隐式字符编码转换\n\n问题SQL：select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;，其中tradelog\n字符集为utf8mb4，trade_detail字符集为utf8\n\n底层：select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value;\n改进：select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2;\n\n\n原因：两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引，导致tradelog查处一行后去trade_detail查时使用的全表扫描\n\n字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较\n因此， 在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，在跟另一表中的字段进行比较\n\n\n不会出现问题的SQL：select operator from tradelog where traideid =$R4.tradeid.value;\n\n底层：select operator from tradelog where traideid =CONVERT($R4.tradeid.value USING utf8mb4);\n这里的 CONVERT 函数是加在输入参数上的，这样就可以用上被驱动表的 traideid 索引\n\n\n\n\n\n\n索引失效的情况\n\n使用 SELECT * 进行查询;\n\n创建了组合索引，但查询条件未遵守最左匹配原则;\n\n在索引列上进行计算、函数、类型转换等操作;\n\n以 % 开头的 LIKE 查询比如 like &#39;%abc&#39;;\n\n查询条件中使用 or，且 or 的前后条件中有一个列没有索引，涉及的索引都不会被使用到;\n\n发生隐式转换\n\n问题：下面四条语句中，第3条比1、2、4慢很多\n1: SELECT * FROM &#96;test1&#96; WHERE num1 &#x3D; 10000;\n2: SELECT * FROM &#96;test1&#96; WHERE num1 &#x3D; &#39;10000&#39;;\n3: SELECT * FROM &#96;test1&#96; WHERE num2 &#x3D; 10000;\n4: SELECT * FROM &#96;test1&#96; WHERE num2 &#x3D; &#39;10000&#39;;\n定义：当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。某些转换是隐式发生的。例如，MySQL 会根据需要自动将字符串转换为数字，反之亦然\n\n根据文档：语句2和语句3的两边被转换成了浮点数来比较\n其中语句2都转换成了浮点数进行比较，转换结果是唯一确定的（都是10000），不影响索引使用\n语句3虽然都转换成了10000，但是除了‘10000’可以转换成10000，‘01000’也可以，所以不是唯一的，不可用索引\n转换规则\n不以数字开头的字符串都将转换为0。如&#39;abc&#39;、&#39;a123bc&#39;、&#39;abc123&#39;都会转化为0；\n以数字开头的字符串转换时会进行截取，从第一个字符截取到第一个非数字内容为止。比如&#39;123abc&#39;会转换为123，&#39;012abc&#39;会转换为012也就是12，&#39;5.3a66b78c&#39;会转换为5.3，其他同理\n\n\n\n\n\n\n\n\n两列数据做比较，即使两列都创建了索引，索引也会失效\n\n查询条件是is null时正常走索引，使用is not null时，不走索引\n\n当查询条件为大于等于、in等范围查询时，根据查询结果占全表数据比例的不同，优化器有可能会放弃索引，进行全表扫描\n\n\n\n\n2.缓存\n刷脏页：InnoDB使用buffer pool管理内存，当内存数据页与磁盘不一样时就称为脏页，需要合适的时机刷新到磁盘上同步数据\n\n刷脏页的时机\n\nInnoDB 的 redo log 写满了。系统会停止所有更新操作，把checkpoint往前推进，将扫到的redo log字段对应的数据页flush到磁盘上，redo log留出空间可以继续写\n系统内存不足需要淘汰掉内存中的页时，如果该页是脏页则需要将数据同步到磁盘上，保证每个数据页不论在内存中还是磁盘上，都是正确的数据（在内存的数据页，其磁盘的就是旧值）\nMySQL 认为系统“空闲”的时候（见缝插针刷新脏页）、MySQL 正常关闭的时候（刷新所有脏页，再次启动时直接读磁盘）\n\n\nInnoDB刷脏页的控制策略\n\n影响性能的情况：一个查询要更新的脏页个数太多；日志写满更新全部堵住，写性能跌为0\n\n刷盘速度：X * max(F1(M), F2(N))\n\ninnodb_io_capacity：告诉 InnoDB 现在的磁盘能力，可以设置成磁盘的IOPS，假设当前为（X）\n\ninnodb_max_dirty_pages_pct：==脏页比例==上限，默认是75%，InnoDB会根据当前脏页比例（假设为M，计算方式 如下），算出一个0到100之间的数字（F1(M)）\nmysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME &#x3D; &#39;Innodb_buffer_pool_pages_dirty&#39;;\nselect VARIABLE_VALUE into @b from global_status where VARIABLE_NAME &#x3D; &#39;Innodb_buffer_pool_pages_total&#39;;\nselect @a&#x2F;@b; #即Innodb_buffer_pool_pages_dirty&#x2F;Innodb_buffer_pool_pages_total\n==redo log写盘速度==：根据写入日志的序号和checkpoint序号之间的差值（假设为N），计算出另一个0到100之间的数字（F2(N)）\n\n\n\n脏页选择算法：改进的LRU算法\n\n按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域，前面是young区域，LRU_old指向old区域第一块\nyoung区域：访问后放到young头部，新数据插入到LRU-old处\nold区域：在LRU中存在超过1s，移到链表头部；否则保持不变（很快失效的不会被保存很久）。所以短时间多次访问一个表不会让其它缓存失效\n\n\n\n\n邻居刷新机制：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷（innodb_flush_neighbors = 1时启用，0时关闭）\n\n\n\n标记删除：从5.6.6开始每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中，删除的记录不会直接在B+树中删除，而是标记删除并等待被复用，所以经过大量删除的表可能存在空洞，通过重建表来收缩空洞减少内存消耗\n\n原理：新建一个表B，表A的数据按顺序插入到B中，这个过程需要全程拿MDL写锁（需要移动数据，下面的Online DDL不需要移动数据，数据存放在tmp_file临时文件中）\nOnline DDL：MySQL5.6引入，可以在重建表的过程中，保证表A上的更新操作不被阻塞：使用日志文件（row log）记录所有A的操作（alter table t engine=innodb,ALGORITHM=inplace;）\nanalyze table t 不是重建表，只是通过那MDL读锁并重新统计；而 optimize table t 等于 recreate+analyze\n\n\n临时表\n\n特点\n可以使用各种引擎类型，使用InnoDB引擎/MyISAM引擎就写到磁盘上，否则使用Memory引擎写到内存上，支持自动回收\n一个临时表只能被创建它的session访问，对其他线程不可见，不同session的临时表可重名\n创建一个名为\\#sql&#123;进程 id&#125;_&#123;线程 id&#125;_ 序列号.frm的文件，所以可重名\n\n\n可以与普通表同名，同名时除了show tables外，都显示临时表，如show create、增删改查等语句\n内存中每个表都对应一个 table_def_key，普通表的值为库名 + 表名，临时表的值外加了server_id+thread_id\n\n\n\n\n用途：因为不用担心重名冲突，所以常被用在复杂查询的优化过程（sort buffer、join buffer）\n分库分表的跨库查询：把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作\n\n\n日志记录\n临时表 redolog：不记录，因为崩溃之后，临时表全没了，也不需要恢复\nundolog：需要记录，5.6之前是和普通表放一块的；5.7之后放在临时表空间的\nbinlog：row格式不用记，statement/mix需要记录，但不记录\n主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，这时候我们就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行\n线程是session级别的且binlog_fotmat=row时，drop table 临时表不会传过去，因为row模式从库没有临时表\n\n\n\n\n内部临时表\n如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果；\njoin_buffer 是无序数组，sort_buffer 是有序数组，临时表是二维表结构；\n如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union 需要用到唯一索引约束， group by 还需要用到另外一个字段来存累积计数\n\n\n\n\n\n3.加锁规则\n查询长时间不返回\n阻塞\n可以使用 show processlist命令查看当前执行的语句是否在等待锁\n通过查询 sys.schema_table_lock_waits 这张表（select blocking_pid from sys.schema_table_lock_waits;），就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可\n\n\n等flush\n通过select * from information_schema.processlist where id=1;语句查看是否在等flush\nMySQL 里面对表做 flush 操作的用法，一般有以下两个flush tables t with read lock; 和flush tables with read lock;，但是这两条语句一般都执行很快，waiting for table flush状态可能是有一个flush tables命令被别的语句堵住\n\n\n等行锁\n通过select * from t sys.innodb_lock_waits where locked_table=&#39;test.t&#39;\\\\G来查询谁占着这个写锁\n\n\n不断回滚\n使用带lock in share mode的SQL语句，是当前读，而不带这个的SQL语句会使用undolog，不断回滚找到自己的视图，这样速度会很慢\n\n\n\n\n加锁规则（5.x 系列 &lt;=5.7.24，8.0 系列 &lt;=8.0.13）\n原则 1：加锁的基本单位是 next-key lock（前开后闭区间）\n原则 2：查找过程中访问到的对象才会加锁\n优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁\n优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁\n一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止\n\n\n死锁解除：方法一是设定超时时间（innodb_lock_wait_timeout）、方法二是发起死锁检测（innodb_deadlock_detect=on），主动回滚死锁链条中的某一事务、方法三是控制并发度、方法四是确保业务一定不死锁，产生了就回滚、方法五是将一个总账户分成多个小账户来提高并发度（需要业务控制逻辑正确）\n\n4.日志配置\n双1配置\n定义：sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog\n日志逻辑序列号（log sequence number）：单调递增，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。\n组提交：一个事务提交的时候，使用组里的现有事务作为LSN，并将现有事务一起写入磁盘中。所以在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好\n\n\n如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？\n设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险\n将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志\n将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据\n\n\n主备同步（未完待续）\n\n5.SQL语句\ncount(*)\n\nMyISAM每个表缓存此值，但是InnoDB每次都需要重新计算，因为MVCC机制，每个版本的表不同，一个表记录一个值没有意义\n优化：将此值保存在数据库的一张表里，通过事务机制来保证数据更改的并发问题（使用Redis缓存不是原子操作有并发问题）\n不同的count用法：server层要什么就给什么、InnoDB只给必要的值、优化器之优化了count(*)的语义为“取行数”\n对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加\n对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加\n对于 count(字段) 来说，\n如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，字段肯定不能为 null 可以直接按行累加\n如果这个“字段”定义允许为 null，那么执行的时候，字段有可能是 null，需要把值取出来再判断一下，不是 null 才累加\n\n\n count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加（效率最高）\n\n\n\n\norder by：通过max_length_for_sort_data参数来决定排序方法，需要的一行数据小于此值时使用全字段排序，大于则使用rowid 排序\n\n全字段排序：通过索引取出满足条件的记录的所需字段，放入名为sort_buffer的内存中，然后进行快速排序或外部归并排序（取决于sort_buffer_size的大小，不够则使用磁盘里的临时文件来辅助）\nrowid排序：放入sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id，排序完后按照顺序返回原表中取出所需的其它字段\n优化：建立联合索引（另一字段有序），覆盖索引（不用回表）来使得查询不用每次都排序\n使用SELECT * FROM information_schema.OPTIMIZER_TRACE\\\\G来查看相关数据\nnumber_of_tmp_files：看到使用的临时文件数量\nsort_mode：packed_additional_fields（使用实际大小申请内存）、rowid（使用rowid排序）\n\n\n\n\n\n\n显示随机消息：select word from words order by rand() limit 3;随机拿出的值是需要放到临时表中存储的，大小超过tmp_table_size参数使用order by的InnoDB表的排序方式，小于tmp_table_size参数则使用内存临时表和rowid方法来排序（不用回表）\n\n当limit限制的行数所占用的内存小于sort_buffer_size时，会选择优先级队列排序算法（堆排序），大于sort_buffer_size时，使用外部归并排序算法\n优化方法\n方法一：取得这个表的主键 id 的最大值 M 和最小值 N；用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N；取不小于 X 的第一个 ID 的行（结果不是严格随机的，但是效率高）\n方法二：取得整个表的行数，并记为 C；取得 Y = floor(C * rand())（ floor 函数在这里的作用，就是取整数部分）；再用 limit Y,1 取得一行（结果是严格随机的，但是效果低于方法一）\n\n\n\n\ndrop（删除表）、delete（清除记录）、truncate（清空表中数据）的区别\n\n用法不同\ndrop：丢弃数据，如drop table 表名，直接将表删除掉，不但数据会删除，表的结构也会删除\ntruncate：清空数据，如truncate table 表名，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用\ndelete：删除数据，如delete from 表名 where 列名=值，删除某一行的数据，如果不加 where子句和truncate table 表名作用类似\n\n\ndrop和truncate属于DDL（数据定义）语句，操作立即生效，不能回滚，而delete是DML（数据操作语言）语句，如果放到rollback片段中，事务提交之后才会生效\n执行速度不同：一般来说：drop&gt;truncate&gt;delete\ndelete命令执行的时候会产生数据库的binlog日志，而日志记录是需要消耗时间的，但是也有个好处方便数据回滚恢复\ntruncate命令执行的时候不会产生数据库日志，因此比delete要快。除此之外，还会把表的自增值重置和索引恢复到初始大小等\ndrop命令会把表占用的空间全部释放掉\n\n\n\n\njoin\n\nIndex Nested-Loop Join：先从表1取1行数据，然后取出join字段去表2中查找，取出满足条件的行，并且表2该join字段有索引，可以走树搜索过程\n驱动表是走全表扫描，而被驱动表是走树搜索，所以让小表做驱动表更快\n小表确定：在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表\nBatched Key Access算法：set optimizer_switch=&#39;mrr=on,mrr_cost_based=off,batched_key_access=on&#39;;\nMulti-Range Read 优化原理：join得到多个返回值时，先放入read_rnd_buffer进行排序，然后批量返回进行顺序查找\n使用join_buffer来暂存数据用于排序\nBNL算法转成BKA算法：直接在被驱动表上建索引（数据存到临时表再加索引），这时就可以使用NLJ算法，然后使用 BKA 算法了\n\n\n\n\nSimple Nested-Loop Join：先从表1取1行数据，然后取出join字段去表2中查找，取出满足条件的行，但是表2该join字段没有索引，需要走全表扫描，效率低，所以被驱动表没有可用索引时使用下面的join方法\nBlock Nested-Loop Join：把表 t1 的数据读入线程内存 join_buffer 中，扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回\njoin_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，就分段放\n内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数还是应该选择小表来作为驱动表\n缺点：多次扫描一个表，虽然有优化后的LRU算法，但是如果是冷表就会有问题\n冷表的数据量小于整个 Buffer Pool 的 3/8：多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部\n冷表很大：由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰，业务正常访问的数据页，没有机会进入 young 区域\n\n\n\n\n\n\ngroup by\n\n如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null\nselect id%10 as m, count(*) as c from t1 group by m order by null;\n尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort\n\n如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过调大 tmp_table_size 参数，避免用到磁盘临时表；\nset tmp_table_size&#x3D;1024;\nselect id%100 as m, count(*) as c from t1 group by m order by null limit 10;\n如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果\nselect SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;\n\n\n\n6.紧急操作\n短期临时提升性能\n\n短连接风暴：正常执行流程是创建短连接，执行少量的SQL，然后断开。但是在连接数暴涨（超过max_connections参数）时，系统就会拒绝接下来的连接请求，返回“Too many connections”\n\n方法一：先处理掉那些占着连接但是不工作的线程，通过kill connection + id;主动断开不需要的连接，类似于实现设置连接的wait_timeout参数，空闲过久则断开连接\n安全删除：通过show processlist;查找sleep的线程，通过查 information_schema 库的 innodb_trx 表看对应事务具体的状态\n断开的连接会返回“ERROR 2013 (HY000): Lost connection to MySQL server during query”，需要业务系统发起新的连接请求，否则业务认为MySQL一直没恢复\n\n\n方法二：减少连接过程的消耗\n跳过权限验证的方法：重启数据库，并使用–skip-grant-tables 参数启动。这样，整个 MySQL 会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内\n\n\n\n\n慢查询性能问题：在上线前使用慢查询日志记录所有语句的执行过程，看看Rows_examined字段是否与预期一致\n\n索引没有设计好：通过紧急创建索引，直接执行alter table语句，可以使用下面的方法，或者使用gh-ost这样的方案\n\n在备库 B 上执行 set sql_log_bin=off，也就是不写 binlog，然后执行 alter table 语句加上索引\n执行主备切换；这时候主库是 B，备库是 A\n在 A 上执行 set sql_log_bin=off，然后执行 alter table 语句加上索引\n\n\nSQL 语句没写好：5.7开始提供query_rewrite功能，可以把输入的一种语句改写成另一种模式，如下所示\n#改写 select * from t where id + 1 &#x3D; 10000\nmysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (&quot;select * from t where id + 1 &#x3D; ?&quot;, &quot;select * from t where id &#x3D; ? - 1&quot;, &quot;db1&quot;);\nmysql&gt; call query_rewrite.flush_rewrite_rules();\nMySQL 选错了索引：使用查询重写功能，给原来的语句加上 force index\n\n\n\nQPS（每秒查询数）突增：由于业务突然出现高峰，或应用程序bug所导致，解决方案如下\n\n一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉\n如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0\n如果这个新增的功能跟主体功能是部署在一起的，那么只能通过处理语句来限制，可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成”select 1”返回\n\n\n\n\n误删数据\n\n使用 delete 语句误删数据行：用 Flashback 工具通过闪回把数据恢复回来；原理是通过修改binlog的内容，拿回原库重放；前提是确保 binlog_format=row 和 binlog_row_image=FULL\n不建议直接在主库上执行这些操作，恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库\n事前预防：把 sql_safe_updates 参数设置为 on（没有where时会报错）；代码上线前，必须经过 SQL 审计\n使用 truncate /drop table 和 drop database 命令删除的数据，就无法通过 Flashback 来恢复了，因为binlog没有每一条记录\n\n\n使用 drop database 语句误删数据库：使用全量备份，加增量日志的方式了。这个方案要求线上有定期的全量备份，并且实时备份 binlog\n跳过误操作的语句\n先用–stop-position 参数执行到误操作之前的日志，然后再用–start-position从误操作之后的日志继续执行；\n实例使用了 GTID 模式，通过set gtid_next=gtid1;begin;commit;跳过改语句；\n\n\n一种加速方法：在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，跳过增量日志的读取过程\n在 start slave 之前，先通过执行﻿﻿change replication filter replicate_do_table = (tbl_name)命令，就可以让临时库只同步误操作的表，这样做也可以用上并行复制技术，来加速整个数据恢复过程\n在接入线上备库的从库时, 需要先将误删除的gtid先设置跳过, 然后利用主从同步的并行复制技术，来加速整个数据恢复过程\n\n\n预防方法\n搭建延迟复制的备库，通过CHANGE MASTER TO MASTER_DELAY = N命令，可以指定这个备库持续保持跟主库有 N 秒的延迟\n账号分离：只给业务开发 DML 权限，而不给 truncate/drop 权限\n制定操作规范：在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响后再删除这张表\n\n\n\n\n使用 rm 命令误删整个 MySQL 实例\n对于一个有高可用机制的 MySQL 集群来说，最不怕的就是 rm 删除数据了。只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作\n\n\n\n\n快速复制一张表\n\n使用 mysqldump 命令将数据导出成一组 INSERT 语句\n\n–single-transaction 的作用是，在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法；\n–add-locks 设置为 0，表示在输出的文件结果里，不增加” LOCK TABLES t WRITE;” ；\n–no-create-info 的意思是，不需要导出表结构；\n–set-gtid-purged=off 表示的是，不输出跟 GTID 相关的信息；\n–result-file 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的\n\nmysqldump -h$host -P$port -u$user --add-locks&#x3D;0 --no-create-info --single-transaction  --set-gtid-purged&#x3D;OFF db1 t --where&#x3D;&quot;a&gt;900&quot; --result-file&#x3D;&#x2F;client_tmp&#x2F;t.sql\n\n# 将这些 INSERT 语句放到 db2 库里去执行\nmysql -h127.0.0.1 -P13000  -uroot db2 -e &quot;source &#x2F;client_tmp&#x2F;t.sql&quot;\n导出和导入 CSV 文件:\nselect * from db1.t where a&gt;900 into outfile &#39;&#x2F;server_tmp&#x2F;t.csv&#39;;\nload data infile &#39;&#x2F;server_tmp&#x2F;t.csv&#39; into table db2.t;\n物理拷贝方法\n\n\n\n\n","slug":"MySQL","date":"2023-04-27T10:54:53.000Z","categories_index":"","tags_index":"database","author_index":"Dajunnnnnn"},{"id":"221a7ad001c03569112f684b2dfdc120","title":"Git","content":"Git1.Git整体结构\n\n工作区：电脑里能看到的目录\n暂存区：.git目录下的index文件（.git/index），也叫做索引（index）\n本地版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库\n远程仓库\nGit文件状态\n版本\nHEAD：指向master分支的一个游标，所以出现Head的地方可以用master来替换\n分支\n\n2.常用操作\n查看/修改提交用户名、邮箱\n#查看\n$ git config --list\n$ git config user.name\n$ git config user.email\n#不加--global将只更改本仓库的配置\n$ git config --global user.name &quot;dajunnnnnn&quot;\n$ git config --global user.email &quot;1064049895@qq.com&quot;\n操作流程\n\n初始化：git init，在执行完成 git init命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变\n\n克隆：git clone，从现有Git仓库中拷贝项目，例如git clone https://github.com/Dajun-2019/Learning.git\n\n暂存：git add .，对工作区执行此命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库（.git/objects）中的一个新对象中，而该对象的ID被记录在暂存区的文件索引中\n\n提交：git commit -m &quot;注释信息”，暂存区的目录树写到版本库（对象库）中，master分支会做相应的更新，即master指向的目录树就是提交时暂存区的目录树\n\n远端操作\n\n首先载入远程仓库（git clone…）、然后进入此仓库，执行git remote -v，输出的origin为远程地址的别名\n显示某个远程仓库的信息：git remote show [remote]\n其他命令\n添加远程版本库：git remote add origin git@github.com:Dajun-2019/Learning\n删除远程仓库：git remote rm name\n修改仓库名：’git remote rename old-name new_name’\n\n\n\n\n拉取：git fetch origin master:temp（将远程仓库（origin为别名）的master分支的代码下载到本地分支上面）\n\n拉去代码并合并：$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;\n\n上传远程代码并合并：git push -u origin master，在git中，“push -u”的意思是将本地的分支版本上传到远程合并，并且记录push到远程分支的默认值；当添加“-u”参数时，表示下次继续push的这个远端分支的时候推送命令就可以简写成“git push”\n\n查看状态（git status）：查看仓库当前状态，显示有变更的文件。git status -s简介输出\n\n分支操作\n\n创建分支：git branch (branch name)\n\n查看分支：git branch\n\n切换分支：\ngit checkout (branch name)\n\n\n当执行 git checkout .或者 git checkout -- &lt;file&gt;命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区中的改动。\n当执行 git checkout HEAD .或者 git checkout HEAD &lt;file&gt;命令时，会用 HEAD 指向的 master 分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改动。\n\n\n删除分支：git branch -d (branch name)\n\n合并分支：git merge （branch name）（branch name会被合并到主分支）\n\n\n\n\n\n\n3.实践\nPull Request流程\n\n把别人的代码fork到自己的仓库\n\n在自己的仓库上修改后的分支，摁下New pull request按钮\n\n\n这时，会进入一个新页面，有Base 和 Head 两个选项。Base 是你希望提交变更的目标，Head 是目前包含你的变更的那个分支或仓库\n\n\n填写说明，帮助别人理解你的提交，然后按下”create pull request”按钮即可\n\ngit am用于将一个patch文件，合并进入当前代码（Github 对每个 PR 会自动生成一个 patch 文件），可以下载该文件，合并进本地代码，就可以在本地查看效果了\n\nGitHub Flow协作流程\n\n克隆 / Fork 仓库：git clone …\n创建分支: git branch …\n修改代码:\n发起 Pull Request: git\nCode Review\nMerge 分支\n删除分支\n\n\n\n\n本地仓库推送到github上的流程\n\n确保Github账户上已经添加了本机的SSH key\n在Github上创建一个新的仓库，并且复制对应链接\n切换到本地项目所在目录，执行git init初始化一个本地仓库\n关联远程库：git remote add origin https://github.com/...\n进行代码合并，将远程代码下载到本地：git pull --rebase origin master\n将本地项目添加到本地仓库\ngit add .\ngit commit -m &quot;提交信息&quot;\ngit status\n\n\n推送master分支的所有内容：git push -u origin master\n去github上检查仓库是否提交成功\n\n\n\n","slug":"Git","date":"2023-04-20T12:35:53.000Z","categories_index":"","tags_index":"tools","author_index":"Dajunnnnnn"},{"id":"a48e6dda0c21e40880cba7e763278b04","title":"Docker","content":"Docker1.概念\n出现背景：在个人开发环境中，需要大量的虚拟机配置不同的项目环境；在公司内部开发环境中，环境配置不可复用，测试环境与开发环境不匹配，依赖升级需要手动操作；\n\nDocker的口号是“Build，Ship，and Run Any App，Anywhere”\nJava的口号是“Write Once,Run Anywhere”\n\n\nDocker基本概念\n\n引擎：创建和管理容器的工具，通过读取镜像来生成容器，并负责从仓库拉取镜像或提交镜像到仓库中；\n镜像：类似于虚拟机镜像，一般由一个基本操作系统环境和多个应用程序打包而成，是创建容器的模板；\n采用分层存储方式，每个镜像可依赖其他镜像进行构建，每一层的镜像可被多个镜像引用，通过共享镜像层，减少镜像仓库占用空间，对于用户而言，看到的是通过UnionFS（联合文件系统）把相关镜像层的目录“联合”到同一个挂载点呈现出来的一个整体\n在拉取镜像的时候，只会拉去缺少的层，在删除镜像的时候，只会删除没有被共享的层\n\n\n容器：可看作一个简易版的Linxu系统环境（包括root用户权限、进程空间、用户空间和网络空间等）以及运行在其中的应用程序打包而成的盒子；\n仓库：集中存放镜像文件的场所，分为公共仓库（Docker Hub）和私有仓库，类似于Git\n宿主机：运行引擎的操作系统所在服务器\n\n\nDocker架构（Docker daemon）\n\n\n命令行工具docker实际上是一个client，他会与Docker Engine里的后台服务Docker daemon通信，镜像存储在远端的Registry里，客户端不能直接访问镜像仓库\nDocker client可以通过build、pull、run等命令向Docker daemon发送请求，而Docker daemon则负责从远端拉去镜像、在本地存储镜像、从镜像生成容器、管理容器等功能\n容器化的应用：指应用程序不再直接和操作系统打交道，而是封装成镜像，再交给容器环境去运行。镜像就是静态的应用容器，容器就是动态的应用镜像\n\n\nDocker隔离实现：是一种应用容器引擎，结合LXC和一些其他技术来实现应用级别容器引擎\n\nLinux提供了Namespace和CGroup技术实现环境隔离和资源控制，其中Namespace是Linux提供的一种内核级别环境隔离的方法，能使一个进程和该进程创建的子进程的运行空间都与Linux的超级父进程相隔离（但只是进程的隔离，物理资源还是进程共用的）；CGroup技术是用来控制一个进程组群可使用的资源（CPU、内存、磁盘IO等），来实现物理资源的隔离\nLinux Container：将Namespace和CGroup相结合，就能构造一个用户空间独立且限定资源的对象，这样的对象称为容器，即LInux Container（Linux提供的容器化技术，简称LXC）。LXC仅为一种轻量级的容器化技术，它仅能对部分资源进行限制，无法做到诸如网络限制、磁盘空间占用限制等（0.7版本取出LXC使用libcontainer，1.11开始使用runC 和 containerd）\n其他技术\nChroot：该技术能在container里构造完整的Linux文件系统；虽然不能直接访问宿主机里的文件，但一般容器都通过包括一个底层的操作系统镜像来弥补\nVeth：该技术能够在主机上虚拟出一张网卡与container里的eth0网卡进行桥接，实现容器与主机、容器之间的网络通信；\nUnionFS：联合文件系统，Docker利用该技术“Copy on Write”（写的时候复制一份副本，在副本上修改）的特点实现容器的快速启动和极少的资源占用，Docker常用的是AUFS\nIptables/netfilter：通过这两个技术实现控制container网络访问策略；\nTC：该技术主要用来做流量隔离，限制带宽；\nQuota：该技术用来限制磁盘读写空间的大小；\nSetrlimit：该技术用来限制container中打开的进程数，限制打开的文件个数等\n\n\n\n\nDocker与虚拟化：虚拟化是依靠一层Hypervisor（软件层）来将虚拟机的指令翻译成物理硬件或宿主机操作系统能识别的指令；而Docker容器里的进程是直接与内核交互的，依靠的Linux的LXC和Chroot等技术，无需中转，几乎没有性能损耗\n\n在启动一个容器时， Docker引擎实际上只是增加了一个可写层和构造了一个Linux容器，这两者都几乎不消耗系统资源，因此Docker容器能够做到秒级启动\n\n为了操作系统镜像占用空间过大的情况，针对不同的场景分别构造了不同的操作系统镜像，包括一下几种：BusyBox（测试场景）、Alpine（生产环境常用）、Debian/Ubuntu、Centos/Fedora\n\n持久化存储：由于都是在可写层里的文件副本进行操作，在容器关闭是，通过以下两种方法实现持久化存储的问题\n\n把宿主机文件系统里的目录映射到容器内的目录，如下图所示。如此一来，容器内在该目录里创建的所有文件，都存储到宿主机的对应目录中，在关闭容器后，宿主机的目录依然存在，再次启动容器时还能读取到之前创建的文件，因此实现了容器的文件持久化。当然同时要明白，如果是对镜像自带文件进行了修改，由于镜像是只读的，该修改操作无法在关闭容器时保存下来，除非在修改了文件后构建一个新的镜像\n\n\n把多台宿主机的磁盘目录通过网络联合为共享存储，然后把共享存储中的特定目录映射给特定的容器，如下图所示。这样容器在重启时，还是能读取到关闭前创建的文件。生产环境中常用NFS作为共享存储方案\n\n\n\n\n\n\n容器的内部机制\n\n文件\n\ndocker cp+源路径+目标路径 命令可以在容器和主机之间互相拷贝文件，适合简单的数据交换，其中目标路径需要用容器名/ID来指明是那个容器的路径，示例：docker cp a.txt 062:/tmp\ndocker run -v命令可以让容器和主机共享本地目录，免去了拷贝操作，提升工作效率，示例：docker run -d --rm -v /tmp:/tmp redis，格式为宿主机路径: 容器内路径，把本机的/tmp路径挂载道容器里的/tmp目录，常用于不同环境运行相同文件\n\n\n网络配置\n\n网络分类：\nhost网络模式让容器与主机共享网络栈，效率高但是容易导致端口冲突，命令为docker run -d --rm --net=host nginx:alpine\nbridge网络模式实现了一个虚拟网桥，容器和主机都在一个私有网段内互联互通，默认使用此模式\n\n\ndocker有一个连接系统允许将多个容器连接在一起，共享连接信息，docker连接会创建一个父子关系，其中父容器可以看到子容器的信息\n新建网络：docker network create -d bridge test-net\n连接容器：\n运行一个容器并连接到新建的 test-net 网络: $ docker run -itd --name test1 --network test-net ubuntu /bin/bash\n打开新的终端，再运行一个容器并加入到 test-net 网络:$ docker run -itd --name test2 --network test-net ubuntu /bin/bash\ntest1和test2可以互相ping通，二者建立了互联关系\n\n\n如果有多个容器，推荐使用Docker Compose\n\n\n\n\n网络端口映射：docker run -p命令可以把主机的端口号映射到容器的内部端口号，解决了潜在的端口冲突\n\n-P：是容器内部端口随机映射到主机的端口，命令为docker run -d -p 5000:5000 training/webapp python app.py\n-p：是容器内部端口绑定到指定的主机端口，还可以附加绑定网络地址，命令为docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py\n\n\n\n\n\n2.常用操作\n安装docker服务\n\n安装docker：sudo apt install -y docker.io\n启动docker服务：sudo service docker start\n当前用户加入docker组：sudo usermod -aG docker $&#123;USER&#125;\n因为操作 Docker 必须要有 root 权限，而直接使用 root 用户不够安全\nDocker官方推荐将当前用户加入 Docker 用户组\n执行完成之后，还需要退出系统（命令 exit ），再重新登录一次，这样才能让修改用户组的命令 usermod 生效\n\n\n验证docker是否安装成功：docker version和docker info\n\n\ndocker 使用\n\n镜像操作命令\n拉取仓库：docker pull [OPTIONS] NAME[:TAG|@DIGEST]\n列出本地已有镜像：docker images\n删除不再使用的镜像：docekr rmi\n\n\n容器操作命令\n启动镜像：docker run\n-it：开启一个交互式操作的Shell\n-d：让容器在后台运行\n--name：为容器起一个名字\n--rm：不保存容器，运行完自动清除\n\n\n列出正在运行的镜像：docker ps\n在容器内执行另一个程序：docker exec\n强制停止容器：docker stop\n再次启动已经停止的容器：docker start\n彻底删除容器：docekr rm\n\n\n\n\n镜像制作\n\n通过正在运行的容器生成新镜像：通过commit命令，把正在运行的容器，叠加上可写层的修改内容，生成一个新镜像。这种方法简单，但是无法直观的设置环境变量、监听端口等内容，适合简单使用的场景\n\n\n通过Dockerfile文件生成新镜像：Dockerfile是一个定义了镜像创建步骤的文件，Docker引擎通过build命令读取Dockerfile，按定义的步骤来一步步构造镜像。在研发和实施环境中，通过Dockerfile 创建容器是主流做法。下面是一个Dockerfile的例子\n\n\ndocker build -f Dockerfile.busybox .（-f后价Dockerfile文件名，后面跟一个文件路径（构建上下文））\n新的镜像暂时没有名字（&lt;none&gt;），可以直接使用ID来查看或运行，可以通过-t参数来指定镜像的标签（tag），名字需要符合规范，用:分割名字和标签\n构建上下文：docker客户端只是把构建上下文目录打包上传，这样服务器才能获得本地的这些文件，就是指定了要打包进镜像的一些依赖文件\n为了避免目录中某些不必要文件（例如 readme/.git/.svn 等）拷贝进镜像，可以在构建上下文目录里再建立一个 .dockerignore 文件，语法与 .gitignore 类似，排除那些不需要的文件\n\n\n更新已有镜像并提交：-m提交的描述信息、-a镜像作者、e218edb10161容器ID、runoob/ubuntu:v2镜像名。可以使用dockr images来查看新镜像，命令为docker commit -m=&quot;has update&quot; -a=&quot;runoob&quot; e218edb10161 runoob/ubuntu:v2\n\n\nDockerfile内部指令\n\nFROM：所有Dockerfile都要从它开始，表示选择构建使用的基础镜像，相当于打地基，如果关注镜像的安全和大小，一般选择Alpine；如果关注运行的稳定性，则可以选择Ubuntu、CentOS、Debian\nCMD：制定docker run启动容器时默认运行的命令\nCOPY：需要把开发测试产生的一一些源码、配置等文件打包进镜像里，拷贝的源文件必须是构建上下文路径，不能随意指定文件，也就是说，必须把这些文件放在一个专门的目录，然后再docker build里指定构建上下文到这个目录才行\nRUN：执行任意的Shell命令，实现任意的镜像构建步骤，所有RUN指令会在每行的末尾使用续行符\\\\，命令之间也会用&amp;&amp;来连接，这样保证在逻辑上是一行（可以把这些Shell集中到一个脚本文件，然后用COPY命令拷贝进去在用RUN来执行）\nARG：用于创建变量，创建的变量只在镜像构建过程中可见，容器运行时不可见\nENV：用于创建变量，创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用\nEXPOSE：用来声明容器对外服务的端口号，对现在基于Node.js、Tomcat、Nginx、Go等开发的微服务系统来说非常有用\n\n\n\n\n\n","slug":"Docker","date":"2023-04-20T12:35:42.000Z","categories_index":"","tags_index":"tools","author_index":"Dajunnnnnn"},{"id":"75ca176d6b382373bec123f05862c849","title":"Java并发","content":"Java并发1.线程\n线程\n\n线程状态：NEW、RUNNABLE（READY、RUNNING）、WAITING、BLOCKED、TERMINATED、TIME_WAITING\n\n\n线程模型：内核线程（1:1）、用户线程（1:N）、混合线程（M:N）\n\nJava使用用户线程模型，上层JVM通过协作式调度来管理这些用户线程，可以在一个线程执行过程中暂停切换到另一线程执行，底层JVM将Java线程映射到操作系统的线程，由操作系统调度和管理\n启动main函数时启动了一个JVM进程，而main函数所在的线程就是这个进程中的一个（主）线程。多个线程共享进程的堆（新建的对象）和方法区资源（已加载的类信息、静态变量、常量、JIT代码），但每个线程有自己的程序计数器、虚拟机栈和本地方法栈\n多线程：减少了上下文的开销，提高了系统的并发能力，减弱IO与CPU的速度差；但会造成死锁、内存泄漏、线程不安全等问题\n\n\n线程安全\n\n线程安全：描述的对象可以是函数也可以是类，线程安全意味者不同线程并发执行相同的函数，或者不同线程执行一个类的不同函数，因为线程切换，函数内的指令都可以任意交叉执行，最终任意执行顺序得到的结果都是相同的，符合预期的\n\n临界区：可能会引起线程不安全的局部代码块，有两个特征，一是访问了共享资源、二是包含复合操作（先检查在执行、先读取再修改后写入）\n&#x2F;&#x2F;先检查再执行\npublic class Singleton &#123;\n    private static Singleton instance;\n    private Singleton()&#123;&#125;\n    public static Singleton getInstance()&#123;\n        if (instance &#x3D;&#x3D; null) &#123;\n            instance &#x3D; new Singleton();\n        &#125;\n        return instance;\n    &#125;\n&#125;\n&#x2F;&#x2F;先读取再修改后写入\npublic class Demo &#123;\n    private int count &#x3D; 0;\n    public void increment()&#123;\n        count++;\n    &#125;\n&#125;\n同步互斥：用于保证线程安全的访问临界区资源的方法\n\n\n\n\n\n线程创建\n\n实现Runnable接口的run()和start()；继承Thread类重写run方法和start方法，==可用Thread类的已有方法==\n&#x2F;&#x2F;class ThreadDemo extends Thread &#123; 内容同下 &#125; \nclass RunnableDemo implements Runnable &#123;\n   private Thread t;\n   private String threadName;\n   \n   RunnableDemo( String name) &#123; threadName &#x3D; name; &#125;\n   \n   public void run() &#123;\n      &#x2F;&#x2F;线程内需要做的操作\n   &#125;\n   \n   public void start () &#123;\n      if (t &#x3D;&#x3D; null) &#123;\n         t &#x3D; new Thread (this, threadName);\n         t.start ();\n      &#125;\n   &#125;\n&#125;\n通过Callable接口和FutureTask类创建线程，==可创建有返回值的线程（在call函数中实现）==\npublic class CallableThreadTest implements Callable&lt;Integer&gt; &#123;\n    public static void main(String[] args)  \n    &#123;  \n        CallableThreadTest ctt &#x3D; new CallableThreadTest();  \n      \t&#x2F;&#x2F;使用FutureTask包装Callable接口的实现类\n        FutureTask&lt;Integer&gt; ft &#x3D; new FutureTask&lt;&gt;(ctt);\n        for(int i &#x3D; 0;i &lt; 100;i++)  \n        &#123;  \n            System.out.println(Thread.currentThread().getName()+&quot; 的循环变量i的值&quot;+i);  \n            if(i&#x3D;&#x3D;20)  \n            &#123;  \n                new Thread(ft,&quot;有返回值的线程&quot;).start();&#x2F;&#x2F;call相当于run，但是有返回值  \n            &#125;  \n        &#125;  \n        try  \n        &#123;  \n            System.out.println(&quot;子线程的返回值：&quot;+ft.get());&#x2F;&#x2F;得到call函数的返回值   \n        &#125; catch (InterruptedException e)  \n        &#123;  \n            e.printStackTrace();  \n        &#125; catch (ExecutionException e)  \n        &#123;  \n            e.printStackTrace();  \n        &#125;  \n  \n    &#125;\n    @Override  \n    public Integer call() throws Exception  \n    &#123;  \n        int i &#x3D; 0;  \n        for(;i&lt;100;i++)  \n        &#123;  \n            System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);  \n        &#125;  \n        return i;  \n    &#125;  \n&#125;\n==注意事项==\n\n直接使用Thread类的run方法：new一个Thread类，线程进入NEW状态，调用start方法，启动一个线程并使线程进入READY状态，当分配到时间片后就可以开始运行了，start会执行线程的相应准备工作，然后自动执行run方法的内容，这是真正的多线程工作，但是直接执行run方法，会把run方法当作一个main线程下的普通方法来执行，并不会在某个线程中执行它，所以这并不是多线程工作\nsleep与wait的区别：sleep是Thread类的静态本地方法，wait则是Object类的本地方法\nsleep方法没有释放锁，wait释放了锁\nwait是让获得对象锁的进程实现等待，会自动释放当前线程占有的对象锁，每个对象（Object）都拥有对象锁，既然要释放当前线程占有的对象锁并让其进入WAITING状态，自然是要操作对应的对象（Object）而非当前的线程（Thread）\n因为Sleep是让当前线程暂停执行，不涉及到对象类，所以也不需要对象锁\n\n\nsleep常用于暂停执行，wait方法常用于线程间交互/通信\nwait方法被调用后，线程不会自动苏醒，需要notify方法或notifyAll方法，sleep执行完线程会自动苏醒，或者也可以使用wait(long timeout)超时后自动苏醒\n\n\n\n\n\n\n线程池创建\n\n线程池出现的原因：因为线程过多会增加创建、调度线程的开销，所以通过线程池提前创建若干线程，一方面避免了处理任务时频繁的，创建销毁线程的开销，另一方面避免了线程数量膨胀导致的过分调度问题，并且可以集中管理线程资源，提高系统稳定性\n\nThreadPoolExecutor\n\n基础\n\n继承链\n\nExecutor接口：声明了execute方法，使得用户不需要关注如何创建线程， 只需要传入实现了Runnable接口的线程任务类\nExecutorService接口：声明了执行一批异步生成Future的方法；声明了管控线程池的方法（关闭等方法）\nAbstractExecutorService：将执行任务的流程串联起来，保证下层的实现只需关注一个执行任务的方法\nThreadPoolExecutor：实现复杂的运行部分（维护自身的生命周期、管理线程和任务）\n\n\n参数：corePoolSize、maximumPoolSize、keepAliveTime、unit、workQueue、threadFactory、handler\n\n运行状态\n\n\n\n\n池内线程创建过程：首先使用工厂函数针对新任务创建线程直到数量达到核心线程池数量，然后将新任务存储在工作队列中，待工作队列满了之后创建一个新线程来处理任务（没任务一段时间后会被销毁），直到总线程数量达到最大线程池数量后，后续的新任务根据拒绝策略来确定对应操作\n\nworker进程实现了Runnable接口继承自AQS，持有一个线程thread（通过TheradFactory来创建），一个初始化任务firstTask\n确定线程状态：线程池通过一张hash表来保存线程的引用，通过增删引用来控制线程的生命周期。因为使用了AQS锁来实现独占锁，根据独占锁的状态反应线程现在的执行状态\nworker线程增加（addWorker方法）：增加一个线程，有两个参数firstTask和core，根据core的值判断现有线程数在哪个区见\nworker线程的回收：线程池中的回收依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程倍JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可\nworker线程执行任务：worker类中的run方法调用了runWorker方法来执行任务，轮询获取任务，再获取锁，直到没有任务\n\n\n任务与线程的匹配：通过生产者消费者模型，缓存任务，供线程池针对任务进行线程的分配\n\n线程池使用AtomicInteger变量维护：运行状态（runState）和线程数量（workerCount）\n&#x2F;&#x2F;高三位保存runState，低29位保存workerCount\nprivate final AtomicInteger ctl &#x3D; new AtomicInteger(ctlOf(RUNNING, 0));\n\n\n\n\n示例\n\nThreadPoolExecutor\npublic class ThreadPoolExecutorDemo &#123;\n\n    public static void main(String[] args) &#123;\n        &#x2F;&#x2F; 创建一个线程池，包含5个线程\n        ThreadPoolExecutor executor &#x3D; (ThreadPoolExecutor) Executors.newFixedThreadPool(5);\n        &#x2F;&#x2F; 提交10个任务给线程池执行\n        for (int i &#x3D; 0; i &lt; 10; i++) &#123;\n            Runnable worker &#x3D; new WorkerThread(&quot;Task &quot; + i);\n            executor.execute(worker);\n        &#125;\n        &#x2F;&#x2F; 关闭线程池\n        executor.shutdown();\n        while (!executor.isTerminated()) &#123;\n            &#x2F;&#x2F; 等待线程池中的任务执行完毕\n        &#125;\n        System.out.println(&quot;All tasks have been completed.&quot;);\n    &#125;\n&#125;\n\nclass WorkerThread implements Runnable &#123;\n    private String taskName;\n\n    public WorkerThread(String taskName) &#123;\n        this.taskName &#x3D; taskName;\n    &#125;\n\n    @Override\n    public void run() &#123;\n        System.out.println(Thread.currentThread().getName() + &quot; executing &quot; + taskName);\n        try &#123;\n            &#x2F;&#x2F; 模拟执行任务需要的时间\n            Thread.sleep(1000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\nExecutor框架的Executors\n&#x2F;&#x2F;1.创建\nExecutorService service &#x3D; Executors.newFixedThreadPool(10);\n&#x2F;&#x2F;2.执行\nservice.execute(new MyThread());\nservice.execute(new MyThread());\nservice.execute(new MyThread());\nservice.execute(new MyThread());\n&#x2F;&#x2F;3.关闭连接\nservice.shutdown();\n\n\n\n\n\n2.互斥2.1synchronized\n粒度：对象锁（this、newObject）、局部代码锁、类锁（Demo.class）\n\n静态synchronized方法和非静态synchronized方法之间的调用不互斥（一个是类的锁一个是实例对象的锁）\n\n尽量不要使用synchronized(String a)，因为JVM中，字符串常量池具有缓存功能\n\n构造方法不能使用 synchronized 关键字修饰，因为构造方法本身就属于线程安全的，不存在同步的构造方法\npublic synchronized void add(int value) &#123;&#125; &#x2F;&#x2F;方法\nsynchronized (this)&#123;&#125; &#x2F;&#x2F;局部代码块\nsynchronized (obj1) &#123;&#125; &#x2F;&#x2F;内部的一个对象 Object obj1 &#x3D; new Object()\nsynchronized (Wallet.class) &#x2F;&#x2F;类锁\n\n\n锁类别：偏向锁（一个）、轻量级锁（不竞争）、重量级锁（竞争）\n\n通过MarkWork字段辨别锁的类别，新创建的对象处于无锁状态，随后自动变为偏向锁状态，线程可以通过CAS操作竞争偏向锁（单进程使用），竞争成功则执行完任务，执行完后锁会继续保持偏向锁状态，竞争失败则请求线程将锁升级为轻量级锁\n\n升级过程先暂停（JVM的STW）持有锁进程，如其在运行synchronized代码，则升级为轻量级锁（线程交叉使用不存在竞争），否则将MarkWork设置为无锁状态（偏向锁升级代价大，不如直接升级为轻量级锁）\n\n在轻量级锁状态，如果通过（自适应）自旋方式循环执行CAS操作请求锁达到一定数量仍未获得时，就申请升级为重量级锁，唤醒等待重量级锁的进程\n\n锁升级：通过CAS操作，持有锁的线程继续执行，请求锁的线程负责升级任务，包括创建Monitor锁，将自己放到Monitor锁的_cxq中，调用OS系统调用来阻塞自己\n\n解锁：先检查锁标志位，如果没有升级，只需要使用CAS操作解锁即可；如果已升级为重量级锁，那么持有轻量级锁的线程去唤醒等待重量级锁的进程\n\nMonitor锁（hotspot）：\nclass ObjectlMonitor &#123;\n    void * volatile _object;&#x2F;&#x2F;该Monitor锁所属的对象\n    void * volatile _owner;&#x2F;&#x2F;获取到该Monitor锁的线程\n    ObjectWaiter * volatile _cxq;&#x2F;&#x2F;没有获取到锁的线程暂时加入_cxq\n    ObjectWaiter * volatile _EntryList;&#x2F;&#x2F;存储等待被唤醒的线程\n    &#x2F;&#x2F;存储调用了wait()的线程，用来实现wait()、notify()线程同步功能\n\t\t&#x2F;&#x2F;wait、notify等方法也依赖于monitor对象\n    ObjectWaiter * volatile _waitSet;\n    &#x2F;&#x2F;...\n&#125;\n\n\n多个对象通过CAS操作（底层为cmpxchg指令）竞争_owner字段，没有获取到锁的线程加入_cxq队列中等待，待锁释放先通知_EntryList队列中的线程通过CAS操作竞争_owner字段，如果_EntryList队列为空，则将_cxq队列中移到_EntryList队列（一个负责存，一个负责取，减少并发冲突）\n内核线程执行上述步骤没得到锁时，会调用Linux的park函数自行阻塞；阻塞线程获取到锁之后，调用unpark函数来取消对应内核线程的阻塞状态\n\n\n\n\n\n\n锁优化\n\n锁消除：虚拟机在执行JIT编译时，有时会根据对代码的分析(逃逸分析)，去掉某些没有必要的锁（局部变量的锁）\n锁粗化：虚拟机在执行JIT编译时，有时会扩大加锁范围，将对多个小范围代码的加锁，合并一个对大范围代码的加锁（如for循环内的锁）\n\n\n\n2.2锁\n锁类别\n\n可重入锁：可以被同一个线程多次加锁的锁，即在锁没有解锁前，再次加锁，通过变量记录重入次数，JUC提供的锁都是可重入锁\n公平锁：线程会按照请求的先后顺序获得锁。synchronized是非公平锁（新请求可插队），ReentrantLock既支持公平锁也支持非公平锁，默认为非公平锁，通过在构造函数中添加true可声明为公平锁。非公平锁的性能比公平锁更好。ReentrantLock通过AQS（抽象队列同步器）来排队等待锁的线程\n可中断锁：对于synchronized来说，一个线程在阻塞等待锁时，是无法响应中断的，即不可被打断。JUC Lock接口提供了lockInterruptibly()函数，支持可响应中断的方式来请求锁（用于线程池，关闭正在执行的线程）\n非阻塞锁：JUC提供了tryLock()函数，支持非阻塞的方式获取锁，如果锁已经被其他线程获取，则不阻塞直接返回\n可超时锁：JUC提供了带参数的tryLock()函数，支持非阻塞获取锁的同时设置超时时间，tryLock()也可被中断，主要用于对响应时间敏感的系统，如Tomcat\n读写锁：为了提到并发度，可多次获得读锁，JUC提供了ReadWrite接口和其实现类ReetrantReadWriteLock。读锁是一种共享锁，可以被多个线程同时获取，写锁是排他锁，同时只能被一个线程获取，读写锁之间也是排他的（写优先）\n乐观读锁：StampedLock是对ReadWriteLock的进一步优化，提供了读锁、写锁和乐观读锁，其中的读锁和写锁与ReadWriteLock中的类似，乐观读锁是对读锁的进一步优化，在读多写少的时候，大部分读操作都不会被写操作干扰，因此连读锁都不需要加，只有验证真正有被写操作干扰的情况下，再加读锁即可\n\n\nAQS\n\n抽象队列同步器，与synchronized底层的ObjectMonitor类相似，都实现了排队线程、阻塞线程和唤醒线程等功能，但只有一个队列，且基于Java语言实现，是锁实现的原理，在ReentrantLock类有体现（Sync、NofairSync、FairSync都继承自AbstractQueuedSynchronizer）\n\nCLH(Craig,Landin,and Hagersten) 队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。在 CLH 同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）\n\n方法\n\nAQS定义了8个模板方法，可以分为两组：独占模式（Lock）和共享模式（Semaphore）\n&#x2F;&#x2F;独占模式\npublic final void acquire(int arg) &#123; ...&#125;\npublic final void acquirelnterruptibly(int arg)throws InterruptedException &#123; ...&#125;\npublic final boolean tryAcquireNanos(int arg, long nanosTimeout)throws InterruptedException &#123; ...&#125;\npublic final boolean release(int arg) &#123; ...&#125;\n&#x2F;&#x2F;共享模式\npublic final void acquireShared(int arg) &#123; ...&#125;\npublic final void acquireSharedInterruptibly(int arg)throws InterruptedException &#123; ...&#125;\npublic final boolean tryAcquireSharedNanos(int arg, long nanosTimeout)throws InterruptedException &#123; ...&#125;\npublic final boolean releaseShared(int arg) &#123; ...&#125;\nAQS提供了4个抽象方法：没有声明为abstract是为了减少代码量，更灵活编写代码\n&#x2F;&#x2F;独占模式\nprotected boolean tryAcquire(int arg)&#123;throw new UnsupportedOperationException();&#125;\nprotected boolean tryRelease(int arg)&#123;throw new UnsupportedOperationException();&#125;\n&#x2F;&#x2F;共享模式\nprotected int tryAcquireShared(int arg) &#123;throw new UnsupportedOperationException();&#125;\nprotected boolean tryReleaseShared(int arg) &#123;throw new UnsupportedOperationException();&#125;\n\n\n\n\nReetrantLock：定义了两个继承自AQS的子类：NofairSync和FairSync，分别用来实现非公平锁和公平锁，并且因为底层释放锁的逻辑相同，故又抽象出公共父类Sync（未完待续）\n\nReadWriteLock：读锁不可以转成写锁，但在写锁释放前加读锁，在写锁释放后线程持有的锁自动从写锁降级为读锁（未完待续）\n\nStampedLock：在读写锁的基础上提供了乐观读锁。在读多写少的情况下，大部分操作都不会被写操作干扰，只有在真正被干扰的情况下再加读锁重复执行读操作（未完待续）\n\n\n2.3补充（改成JMM？）\n关键字：volatile、synchronized、final\nvolatile：每次都去主内存读取，修改立即写入内存（c语言中的volatile的意思是禁用cpu缓存）\n解决可见性问题：用volatile修饰的变量，在编译成机器指令时，会加入特殊指令，使得CPU对此变量的修改立即写入内存，并通过其它CPU更新缓存数据\n解决有序性问题：volatile通过禁止指令重排序来解决有序性问题，并且是部分指令重排\n内存屏障：JMM定义了4个细粒度的内存屏障，其底层依赖CPU提供的内存屏障指令（StoreStore、StoreLoad、LoadLoad、LoadStore）分别禁止屏障前后的写写、写读、读读、读写操作重排\nJMM内存模型定义部分禁止重排序的方法：volatile写操作后或者volatile读操作前会添加[StoreLoad]来防止volatile写和读的重排序，一般选择添加在写后面，因为读多写少。\n\n\n解决原子性问题\n在32位计算机上，读写64位的long或double类型数据，会执行两次内存读写操作，如果用volatile修饰，那么编译器会在两次读或写之间锁定总线指令，保证变量读写的原子性，但在64位机上就不需要了\n自增语句（count++）因为是对寄存器的值进行操作，但是volatile对变量只能保证立刻写入内存让所有CPU的缓存失败，所以不能影响寄存器内的值，需要synchronized关键字\n\n\n\n\nsynchronized：通过让原本并发执行的代码串行执行，并且每次加锁和释放锁，都会同步CPU缓存和内存中的数据，可以解决可见性、有序性、原子性的问题\nfinal：JMM对final的语义做了增强，禁止编译器将构造函数中对final变量的写操作，重排序到对象引用之后，也就是禁止初始化对象（构造函数中的语句）和将内存空间赋值给引用的重排序，否则在多线程环境下，一个线程可能看到final变量的两个不同的值\n\n\nsynchronized和volatile有什么区别（互补）\nvolatile关键字是线程同步的轻量级实现，所以性能比synchronized好，但是volatile只能用于变量而synchronized可以修饰方法以及代码块\nvolatile关键字能保证数据的可见性，但不能保证数据的原子性，synchronized关键字两者都能保证\nvolatile关键字主要用于解决变量在多个线程之间的可见性，而synchronized关键字解决的是多个线程之间访问资源的同步性\n\n\nsynchronized和ReentrantLock有什么区别\n相同点：两者都是可重入锁，即线程可以再次获取自己的内部锁，不可重入的此时会产生死锁\nReentrantLock属于可中断锁，获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理；synchronized锁属于不可中断锁，一旦线程申请了锁，就只能等到拿到锁之后才能进行其他的逻辑处理\nsynchronized依赖于JVM（用户不能直接看到代码）而ReentrantLock依赖于API（lock、unlock等方法）\nReentrantLock 比 synchronized 增加了一些高级功能，如可中断锁、公平锁、可超时锁、非阻塞锁、选择性通知（锁可以绑定多个条件）\nsynchronized需要和wait、notify结合才能实现等待/通知机制，ReentrantLock类通过Condition接口和newCondition方法实现\nCondition接口可以实现多路通知功能，也就是在一个Lock对象中可以创建多个Condition实例（对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活\n在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知”\nsynchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll方法的话就会通知所有处于等待状态的线程，这样会造成很大的效率问题\n而Condition实例的signalAll()方法，只会唤醒注册在该Condition实例中的所有等待线程\n\n\n\n\n\n3.同步2.1条件变量\nObject类：执行wait()或notify()前先加锁、使用while循环避免假唤醒，底层依赖ObjectMonitor\npublic class QueueCond&#123;\n  private List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();\n  private int count &#x3D; 0;\n  \n  public void put(String elem)&#123;\n    synchronized(this)&#123;&#x2F;&#x2F;加锁\n      list.add(count,elem);\n      count++;&#x2F;&#x2F;更新状态变量\n      this.notify();&#x2F;&#x2F;通知\n    &#125;\n  &#125;\n  \n  public String get()&#123;\n    synchronized(this)&#123;&#x2F;&#x2F;加锁\n      while(count &lt;&#x3D; 0)&#123;&#x2F;&#x2F;检查状态变量是否满足条件\n        try&#123;\n          this.wait();&#x2F;&#x2F;等待并释放锁，被唤醒之后重新竞争获取锁\n        &#125;catch(InterruptedException e)&#123;\n          return null;\n        &#125;\n      &#125;&#x2F;&#x2F;以下为业务逻辑\n      count--;\n      return list.get(count);\n    &#125;\n  &#125;\n&#125;\nCondition接口：使用前后需要lock和unlock，使用中要while，底层依赖ConditionObject（AQS的内部类）\npublic class QueueCondJUC&#123;\n  private List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();\n  private int count &#x3D; 0;\n  private Lock lock &#x3D; new ReentrantLock();\n  private Condition condition &#x3D; lock.newCondition();\n  \n  private void put(String elem)&#123;\n    lock.lock();&#x2F;&#x2F;加锁\n    try&#123;\n      list.add(count,elem);\n      count++;&#x2F;&#x2F;更新状态变量\n      condition.signal();&#x2F;&#x2F;通知\n    &#125;finally&#123;\n      lock.unlock();&#x2F;&#x2F;解锁\n    &#125;\n  &#125;\n  public String get()&#123;\n    lock.lock();&#x2F;&#x2F;加锁\n    try&#123;\n      while(count &lt;&#x3D; 0)&#123;&#x2F;&#x2F;检查状态变量是否满足条件\n        try&#123;\n          condition.await();&#x2F;&#x2F;等待并释放锁，被唤醒之后重新竞争获取锁\n        &#125;catch(InterruptedException e)&#123;\n          return null;\n        &#125;\n      &#125;&#x2F;&#x2F;以下为业务逻辑\n      count--;\n      return list.get(count);\n    &#125;finally&#123;\n      lock.unlock();&#x2F;&#x2F;解锁\n    &#125;\n  &#125;\n&#125;\n\n2.2信号量（Semaphore）\nSemaphore类\n\n信号量与锁的区别是：释放锁的线程必须持有锁，而信号量则不用。即没有调用acquire()函数的线程也可以直接调用release()函数，用来增加可用许可个数。此时，信号量不再是用来限制对临界区的并发访问，而是用来对共享资源的并发访问\n如果信号量中的许可个数为1，那么信号量就退化成了互斥锁；如果互斥量的许可个数大于1，信号量就可以看作是一种共享锁\n\npublic class Semaphore implements java.io.Serializable &#123;\n  &#x2F;&#x2F;第一组，默认一次获取或释放的许可（permit）个数为1\n  public void acquire() throws InterruptedException &#123;&#125;&#x2F;&#x2F;可中断获取\n  public void acquireUninterruptibly() &#123;&#125;&#x2F;&#x2F;不可中断获取\n  public boolean tryAcquire()&#123;&#125;;&#x2F;&#x2F;非阻塞获取\n  public boolean tryAcquire(long timeout, TimeUnit unit)&#x2F;&#x2F;可超时获取\n        throws InterruptedException &#123;&#125;\n  public void release()&#123;&#125;\n\n  &#x2F;&#x2F;第二组，默认制定一次获取或释放的许可个数\n  public void acquire(int permits) throws InterruptedException &#123;&#125;&#x2F;&#x2F;可中断获取\n  public void acquireUninterruptibly(int permits) &#123;&#125;&#x2F;&#x2F;不可中断获取\n  public boolean tryAcquire(int permits)&#123;&#125;;&#x2F;&#x2F;非阻塞获取\n  public boolean tryAcquire(int permits, long timeout, TimeUnit unit)&#x2F;&#x2F;可超时获取\n        throws InterruptedException &#123;&#125;\n  public void release(int permits)&#123;&#125;\n&#125;\n应用：共享资源并发访问控制\npublic class QueueSemaphore&#123;\n  private static final int Q_SIZE &#x3D; 20;\n  &#x2F;&#x2F;表示队列中的空闲位置\n  private Semaphore semaphore &#x3D; new Semaphore(Q_SIZE);\n  private list&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(Q_SIZE);\n  private int count &#x3D; 0;\n  \n  public void put(String elem)&#123;\n    &#x2F;&#x2F;当可用许可个数为0时，线程执行put函数时会阻塞在acquireUniterruptibly()函数中\n    semaphore.acquireUniterruptibly();\n    synchronized(this)&#123;\n      list.add(count, elem);\n      count++;\n    &#125;\n  &#125;\n  public String get()&#123;\n    if(count &#x3D;&#x3D; 0) return null;\n    synchronized(this)&#123;\n      if(count &#x3D;&#x3D; 0) return null;&#x2F;&#x2F;双重检测\n      String ret &#x3D; list.get(--count);\n      semaphore.release();\n      return ret;\n    &#125;\n  &#125;\n&#125;\n原理\n\n调用semaphore.acquire()，线程尝试获取许可证，如果 state &gt;= 0的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 state的值 state=state-1。如果 state&lt;0的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程\n调用semaphore.release();，线程尝试释放许可证，并使用 CAS 操作去修改 state的值 state=state+1。释放许可证成功之后，同时会唤醒同步队列中的一个线程。被唤醒的线程会重新尝试去修改 state的值 state=state-1，如果 state&gt;=0则获取令牌成功，否则重新进入阻塞队列，挂起线程。\n\npublic class Semaphore implements java.io.Serializable &#123;\n  &#x2F;&#x2F;实现AQS，模版模式\n  private final Sync sync;\n  abstract static class Sync extends AbstractQueuedSynchronizer &#123;\n    Sync(int permits) &#123;setState(permits);&#125;\n    protected final boolean tryReleaseShared(int releases) &#123;&#125;\n  &#125;\n\n  static final class NonfairSync extends Sync &#123;\n    NonfairSync(int permits) &#123;super(permits);&#125;\n    protected int tryAcquireShared(int acquires) &#123;\n      return nonfairTryAcquireShared(acquires);\n    &#125;\n  &#125;\n  \n  &#x2F;*\n  final int nonfairTryAcquireShared(int acquires) &#123;\n    for (;;) &#123;\n      int available &#x3D; getState();&#x2F;&#x2F;许可个数存放在state变量中\n      int remaining &#x3D; available - acquires;\n      if (remaining &lt; 0 ||\n          compareAndSetState(available, remaining))\n        return remaining;\n    &#125;\n  &#125;\n  *&#x2F;\n\n  static final class fairSync extends Sync &#123;\n    fairSync(int permits) &#123;super(permits);&#125;\n    protected int tryAcquireShared(int acquires) &#123;\n      for (;;) &#123;\n        if (hasQueuedPredecessors()) return -1;&#x2F;&#x2F;比NonfairSync多了这一行\n        int available &#x3D; getState();\n        int remaining &#x3D; available - acquires;\n        if (remaining &lt; 0 ||\n            compareAndSetState(available, remaining))\n          return remaining;\n      &#125;\n    &#125;\n  &#125;\n\n  public Semaphore(int permits) &#123;&#x2F;&#x2F;默认非公平模式\n    sync &#x3D; new NonfairSync(permits);\n  &#125;\n\n  public Semaphore(int permits, boolean fair) &#123;&#x2F;&#x2F;指定工作模式（公平&#x2F;非公平）\n    sync &#x3D; fair ? new FairSync(permits) : new NonfairSync(permits);\n  &#125;\n  &#x2F;&#x2F;暂时省略核心方法的实现\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;acquireUninterruptibly()函数&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F;位于Semaphore.java中\npublic void acquireUninterruptibly() &#123;\n  sync.acquireShared(1);\n&#125;\n&#x2F;&#x2F;位于AbstractQueuedSynchronizer.java中\npublic final void acquireShared(int arg) &#123;\n  if (tryAcquireShared(arg) &lt; 0)&#x2F;&#x2F;竞争获取许可，返回值&lt;0表示失败，需要排队等待许可\n    doAcquireShared(arg);&#x2F;&#x2F;排队等待许可\n&#125;\n&#x2F;&#x2F;其中tryAcquireShared()函数的代码实现位于NonfairSync和FairSync中，实现见上\n&#x2F;&#x2F;两种实现均通过自旋+CAS的方式获取许可，唯一区别是从等待队列中取还是可以插队\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;release()函数&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n\n2.3Latch&amp;Barrier\nCountDownLatch：等其他线程结束，允许count个线程阻塞在一个地方，直至所有线程的任务都执行完毕（是一次性的，不能重复使用）\npublic class DemoJoin&#123;\n  public static void main(String[] args) throws InterruptedException&#123;\n    Thread t1 &#x3D; new Thread(new RunnableForJoin());\n    THread t2 &#x3D; new THread(new RunnableForJoin());\n    t1.start();\n    t2.start();\n    t1.join();&#x2F;&#x2F;join只用来等待线程执行结束，并且必须知道被等待线程是谁\n    t2.join();\n  &#125;\n  public static class RunnableForJoin implements Runnable&#123;\n    @Override\n    public void run()&#123;\n      &#x2F;&#x2F;业务逻辑\n    &#125;\n  &#125;\n&#125;\npublic class DemoLatch&#123;\n  private static final CountDownLatch latch &#x3D; new CountDownLatch(2);\n  public static void main(String[] args) throws InterruptedException&#123;\n    new Thread(new RunnableForLatch()).start();\n    new Thread(new RunnbaleForLatch()).start();\n    latch.await();&#x2F;&#x2F;等待something执行完成而非等待线程结束，并且不需要知道在等谁\n    &#x2F;&#x2F;执行后续逻辑\n  &#125;\n  public static class RunnableForLatch implements Runnable&#123;\n    @Override\n    public void run()&#123;\n      &#x2F;&#x2F;do something\n      latch.countDown();\n      &#x2F;&#x2F;do otheer thing\n    &#125;\n  &#125;\n&#125;\nCyclicBarrier：CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活\npublic class Demo&#123;\n  &#x2F;&#x2F;创建parties为10的CyclicBarrier对象，用于10个线程之间相互等待，尽管10个线程的启动（执行\n  &#x2F;&#x2F;start函数）的时间不同，但每个线程结束都会调用await函数，将paeties减一，然后检查parties\n  &#x2F;&#x2F;如果不为0，则当前线程阻塞等待，如果parties为0，则当前线程唤醒所有调用了await函数的线程。\n  private static final CyclicBarrier barrier &#x3D; new CyclicBarrier(10);\n  public static void main(String[] args)&#123;\n    for(int i&#x3D;0; i&lt;10; ++i)&#123;\n      new Thread(new Runnbale()&#123;\n        @Override\n        public void run()&#123;\n          try&#123;\n            barrier.await();\n          &#125;catch(InterruptedException e)&#123;&#x2F;&#x2F;当前线程被中断\n            e.printStackTrace();\n          &#125;catch(BrokenBarrierException e)&#123;&#x2F;&#x2F;其他线程调用await()期间被中断\n            e.printStachTrace();\n          &#125;\n          &#x2F;&#x2F;执行业务逻辑\n        &#125;\n      &#125;).start();\n    &#125;\n    &#x2F;&#x2F;主线程需要等待以上10个线程执行结束，方法有以下3种：\n    &#x2F;&#x2F;1.sleep() 2.join() 3.CountDownLatch()\n  &#125;\n&#125;\n\n4.JUC1.并发阻塞（xxxBlockingQueue）\n\n\n\n\n\n\n\n\n线程安全和支持读写阻塞，阻塞并发队列一般用于实现生产者-消费者模型\n\nxxxBlockingQueue：ArrayBlockingQueue、LinkedBlockingQueue、LinkedBlockingDeque、PriorityBlockingQueue的实现原理类似，都是基于ReentrantLock锁来实现线程安全，基于Condition条件变量来实现阻塞等待\nArrayBlockingQueue：有界队列实现类，底层采用数组来实现，一旦创建容量不能改变\n使用方法和普通队列类似，只不过增加了读写可阻塞，支持公平和非公平两种工作模式，默认为非公平\n支持读写阻塞的put和take函数（ReentrantLock+Condition）\n非阻塞的offer和poll函数，只通过ReentrantLock锁来保证线程安全，没有通过条件变量来实现阻塞读写\n\n\nLinkedBlockingQueue：基于链表实现的有界阻塞并发队列，默认大小为Integer.MAX_VALUE，可以指定队列大小\nLinkedBlockingDeque：与LinkedBlockingQueue的区别在于，它是一个双端队列，支持两端读写操作\nPriorityBlockingQueue：是一个无界阻塞并发优先级队列，底层基于支持扩容的堆来实现，写操作永远不需要阻塞，只有读操作会阻塞，不可插入null值且插入对象必须可比较大小（comparable）\n\n\nDelayQueue\n延迟阻塞并发队列，底层基于PriorityQueue来实现，因为PriorityQueue支持动态扩容，所以DelayQueue为无界队列，写永远都不会阻塞，只有读会阻塞\nDelayQueue中存储的每个元素都必须实现Delayed接口，提供延迟被读取时间delayTime，PriorityQueue按照delayTime的大小将元素组织成最小顶堆，也就是说，堆顶的元素是delayTime最小的元素，应该最先被读取到\ntake函数，包含两个逻辑，针对leader线程的逻辑和针对非leader线程的逻辑。当多个线程先后调用take函数，第一个线程就是leader线程，剩下的就是非leader线程。第一个线程执行读取操作完成之后，第二个线程便称为leader线程。\n非leader线程直接调用await函数阻塞，等待leader线程执行完成之后调用signal来唤醒\nleader线程读取的是队首的元素，如果队首的元素delayTime大于0，那么leader线程会调用awaitNanos阻塞delayTime时间，当delayTime时间过去之后，leader线程自动唤醒，为了避免假唤醒（插队情况见下），leader线程会检查队首元素的delayTime是否真正变为小于等于0，如果是，则队首元素出队，调用signal唤醒第二个线程，第二个线程就成了leader线程\n插队情况：如果一个线程执行take函数时，如果检查发现队列不为空，并且队首元素的delayTime小于等于0，于是，不管是不是有其他线程在调用await或awaitNanos阻塞等待，这个线程都会直接读取队首元素并返回\n\n\n\n\n较少使用\nSynchronousQueue：用于两个线程之间传递数据，每个put操作必须阻塞等待take操作，队列中不存储任何元素\nLinkedTransferQueue：基于链表实现的无界阻塞并发队列，是LinkedBlockingQueue和SynchronousQueue的综合体，提供了transfer函数，跟SynchronousQueue的put函数的功能相同，调用transfer的线程会一直阻塞，直到数据被其他线程消费才会返回\n\n\n\n2.分段加锁（ConcurrentHashMap）\n原理\n底层数据结构：ConcurrentHashMap底层采用数组+链表/红黑树（1.7使用分段数组+链表）\n实现线程安全的方式\nJDK1.7的ConcurrentHashMap：对整个桶数组进行分割分段，每一把锁只锁其中的一部分数据，多线程访问不同段的数据就不会产生锁竞争\nJDK1.8的ConcurrentHashMap：直接用Node数组+链表/红黑树来实现，并发控制使用synchronized和CAS来操作\nTreeNode是存储红黑树节点，被TreeBin包装，TreeBin通过root属性维护红黑树的根节点，因为红黑树在旋转的时候，根节点可能会被它原来的子节点替换掉，在这个时间点如果有其他线程要写这颗红黑树就会产生线程不安全问题，所以在ConcurrentHashMap中TreeBin通过waiter属性维护当前使用这颗红黑树的线程，来防止其他线程的进入\n\n\nConcurrentHashMap比HashTable效率高的原因：ConcurrentHashMap中，table数组被分段加锁，如果table数组的大小为n，那么就对应存在n把锁，每一个链表独享一把锁，不同链表之间的操作可以多线程并行执行，互不影响，以此来提高并发性能。而HashTable使用synchronized（同一把锁）来保证线程安全，效率低，当一个线程使用put时，另一个线程既不能使用put，也不能使用get\n\n\nConcurrentHashMap类（未完待续）\n\n3.写时复制（CopyOnWriteArrayList、CopyOnWriteArraySet）\n主要应用于并发容器中，为了避免读操作和写操作（增、删、改）同时发生而产生的线程安全问题，写时复制将原始容器中的数据复制一份放入新创建的容器，然后对新创建的容器进行写操作，而对读操作继续在原始容器上进行，这样读写之间不会存在数据访问冲突，当写操作执行完成后，新创建的容器替代原始容器\n这样读操作完全不需要加锁，写入也不会阻塞读取操作，只有写入和写入之间需要进行同步等待\n\n\n弱一致性：CopyOnWriteArrayList源码显示，写操作的结果并非对读操作立即可见，这就导致了短暂的数据不一致，称为弱一致性，在某些业务场景下，会引发bug\n解决办法：CopyOnWriteArrayList提供了用于遍历容器的迭代器\n\n\n连续存储：JUC提供了CopyOnWriteArrayList、CopyOnWriteArraySet，却没有提供CopyOnWriteLinkedList、CopyOnWriteHashMap等其他类型的写时复制容器的原因：因为执行写操作需要复制整个数据，对于链表和哈希表来说，因为数据在内存中不是连续存储的，所以耗时非常大，写操作的性能无法满足工业级通用类对性能的要求。CopyOnWriteArrayList、CopyOnWriteArraySet底层都是基于数组来实现的，而且使用了JVM底层提供的native方法，通过C++代码中的指针实现了内存块的快速拷贝\n\n5.无锁编程\nCAS：CAS指的是先检查后更新这类复合操作，全称为Compare And Set或Compare And Swap。在CAS操作失败后，可以选择自旋直到CAS成功 或 执行失败处理相关的业务逻辑\n原子类：原子类的每个操作都可以看成是原子操作，在多线程环境下，执行原子类的操作不会出现线程安全问题\nLongAdder：（未完待续）\nThreadLocal：使用ThreadLocal线程局部变量替代共享变量，以实现在不需要加锁的情况下达到线程安全。其作用域范围介于类的成员变量和函数内局部变量之间，既是线程私有的，又可以在函数之间共享，不但避免了线程安全问题，还能避免参数传递带来的代码耦合问题\nUnsafe类：（未完待续）\nFuture类：（未完待续）\n\n","slug":"Java并发","date":"2023-04-13T23:56:43.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"b4296f0600f693552b5b6c6b665f6025","title":"Java特性","content":"Java1.关键字\ntrue, false, 和 null 虽然不是关键字，但它们是不能用作标识符的文字和保留字\nstrictfp（精确浮点数，跨平台产生相同结果）、native（原生方法）\n\n\n\n\n\nclass\nreturn\nbyte\ntry\nif\n\n\n\nimport\npublic\nboolean\ncache\nelse\n\n\nextends\nprotected\nshort\nfinally\nfor\n\n\nimplements\nprivate\nint\nthrow\nwhile\n\n\nenum\n==final==\nchar\nthrows\ndo\n\n\ninterface\n==static==\nlong\nresource\nswitch\n\n\npackage\nabstract\nfloat\n==volatile==\ncase\n\n\nnew\nnative\ndouble\n==synchronized==\ndefault\n\n\nsuper\nconst\nvoid\n==transient==\nbreak\n\n\nthis\ngoto\ninstanceof\nstrictfp\ncontinue\n\n\n2.概念辨析\n值传递与引用传递\n\n引用类型（数组、接口、类）的数据存储在堆上，栈上存储的是堆的地址，直接更改对象对所有引用都可见，但不能像C++那样让引用指向新的对象\n引用数据判等：==判断两个引用是否指向同一对象，equals方法+重写的hashcode方法判断属性是否相等\n\n\n深拷贝、浅拷贝、引用拷贝\n\n深拷贝与浅拷贝：深拷贝会复制整个对象，包括对象包含的内部对象；浅拷贝会在堆上创建一个新对象，但是对象内部引用类型变量只会复制引用地址，不会直接复制内部数据\n\n引用拷贝：两个不同引用指向同一对象\n\n示例\npublic class Address implements Cloneable&#123;\n    private String name;\n    &#x2F;&#x2F; 省略构造函数、Getter&amp;Setter方法\n    @Override\n    public Address clone() &#123;\n        try &#123;\n            return (Address) super.clone();\n        &#125; catch (CloneNotSupportedException e) &#123;\n            throw new AssertionError();\n        &#125;\n    &#125;\n&#125;\n\npublic class Person implements Cloneable &#123;\n    private Address address;\n    &#x2F;&#x2F; 省略构造函数、Getter&amp;Setter方法\n    @Override\n    public Person clone() &#123;\n        try &#123;\n\t\t\t\t\t\t&#x2F;&#x2F;浅拷贝\n            Person person &#x3D; (Person) super.clone();\n            return person;\n        &#125; catch (CloneNotSupportedException e) &#123;\n            throw new AssertionError();\n        &#125;\n    &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;浅拷贝&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nPerson person1 &#x3D; new Person(new Address(&quot;武汉&quot;));\nPerson person1Copy &#x3D; person1.clone();\n&#x2F;&#x2F; true\nSystem.out.println(person1.getAddress() &#x3D;&#x3D; person1Copy.getAddress());\n\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;深拷贝&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n@Override\npublic Person clone() &#123;\n    try &#123;\n        Person person &#x3D; (Person) super.clone();\n\t\t\t\t&#x2F;&#x2F;深拷贝\n        person.setAddress(person.getAddress().clone());\n        return person;\n    &#125; catch (CloneNotSupportedException e) &#123;\n        throw new AssertionError();\n    &#125;\n&#125;\nPerson person1 &#x3D; new Person(new Address(&quot;武汉&quot;));\nPerson person1Copy &#x3D; person1.clone();\n&#x2F;&#x2F; false\nSystem.out.println(person1.getAddress() &#x3D;&#x3D; person1Copy.getAddress());\n\n\n重载和重写的区别\n\n重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理（如构造函数）；重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，就要覆盖父类方法（如Override）\n如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的\n\n\n接口和抽象类\n\n共同点：都不能被实例化、都可以包含抽象方法，都可以有默认实现方法（default声明，子类可不实d现）\n不同点：\n接口主要是对API声明（参数类型、返回值类型、函数名），抽象类主要是为了代码复用\n一个类可以实现多个接口，但只能继承自一个抽象类\n接口中的成员变量只能是public static final类型的，不能被修改且必须有初始值，而抽象类的成员变量默认default，可在子类中被重新定义，也可被重新赋值\n\n\n\n\nfinal和static\n\n只有成员变量能被static、public、protected、private修饰，局部变量不行，但是两者都能被final修饰\n\n\n引用类型转换：仅限于有继承关系的类之间，分为向上转换和向下转换两种\n\n向上转换，自动类型转换，总是可以的\n向下转换需要保证转换的对象本身就是子类类型的，只不过暂时转换为了父类型，现在只是再转回去而已\n\n\n\n3.语法糖\nswitch支持String与枚举：int比数、char比ascii码、字符串用hashCode()和equals()，其它如short、byte、int都需要转换为整数\n\n泛型和类型擦除：编译时会使用泛型做类型检查，但是当代码编译为字节码之后，泛型中的类型参数和通配符都替换为上界限（==类型擦除==）\n\n泛型遇到重载：因为都会转成父类型，所以List&lt;String&gt;和List&lt;Integer&gt;这种重载会编译失败\n\n当泛型遇到catch：泛型的类型参数不能用在catch语句中，因为异常处理是由JVM在运行时刻来进行的，类型信息被擦除了，所以JVM是无法区分两个异常类型MyException&lt;String&gt;和MyException&lt;Integer&gt;的\n\n创建对象时：不能使用new T()来创建类型参数对象，在代码编译成字节之后类型信息已经擦除，所以，在运行时，JVM无法确定具体类型，也就无法知道T是否存在无参构造函数\n\n当泛型内包含静态变量：由于经过类型擦除，所有的泛型类实例都关联到同一份字节码上，泛型类的所有静态变量是共享的\npublic class StaticTest&#123;\n    public static void main(String[] args)&#123;\n        GT&lt;Integer&gt; gti &#x3D; new GT&lt;Integer&gt;();\n        gti.var&#x3D;1;\n        GT&lt;String&gt; gts &#x3D; new GT&lt;String&gt;();\n        gts.var&#x3D;2;\n        System.out.println(gti.var); &#x2F;&#x2F;输出为2\n    &#125;\n&#125;\nclass GT&lt;T&gt;&#123;\n    public static int var&#x3D;0;\n    public void nothing(T x)&#123;&#125;\n&#125;\n因为需要继承自Object，所以基本类型不可以传入类型参数，只有引用类型可以。但是有语法糖可以让List&lt;int&gt;中的int替换为Integer，但是开发上依旧需要为每个基本类型分别定义多个不同的函数接口\n\n\n\n自动装箱与拆箱：原始类型byte, short, char, int, long, float, double, boolean 对应的封装类为Byte, Short, Character, Integer, Long, Float, Double, Boolean\n\n基本类型和包装类型的区别：包装类型不赋值时是null，可用于范型，占用空间大\n\n基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中（JIT优化，逃逸分析，分配到栈上）\n基本数据类型存放在栈中是一个常见的误区！基本数据类型的成员变量如果没有被 static修饰的话（不建议这么使用，应该要使用基本数据类型对应的包装类型），就存放在堆中\n类静态成员变量存放在方法区中！（方法区又叫静态区，跟堆一样，被所有线程共享，方法区包含所有的class和static变量）\n\n\n常量池\n\nInteger等包装类使用了常量池技术，IntegerCache类（享元模式）中会缓存值为-128到127之间的Integer对象，当通过自动装箱，也就是调用valueOf()来创建Integer对象时，如果要创建的Integer对象的值在-128到127之间，会从IntegerCache中直接返回，否则才会真正调用new方法创建，详见Integer类的valueOf()（JVM也提供了方法，可以自定义缓存的最大值）\n\nByte、Short、Integer、Long这四种包装类默认创建了数值[-128,128]的相应类型的缓存数据（存放在一个Cache数组中，由static代码块直接初始化），Character创建了数值在[0，127]范围的缓存数据，Boolean直接返回True或False（return (b ? TRUE : FALSE);）\n\n所有整型包装类对象之间值的比较，全部使用 equals 方法比较\nInteger i1 &#x3D; 40; &#x2F;&#x2F;触发自动装箱，使用缓存中的对象\nInteger i2 &#x3D; new Integer(40); &#x2F;&#x2F;新创建的对象\nSystem.out.println(i1&#x3D;&#x3D;i2); &#x2F;&#x2F;返回false\n\n\n示例代码：项目首选基本类型，业务相关可选包装类用null表示空而不是0\n&#x2F;&#x2F;自动装箱，语法糖，底层实现为：Integer iobj &#x3D; Integer。valueOf(12);\nInteger iobj &#x3D; 12;\n&#x2F;&#x2F;自动拆箱，语法糖，底层实现为：int i &#x3D; iobj.intValue();\nint i &#x3D; iobj;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;触发自动装箱和拆箱的几种情况&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F;将基本类型数据赋值给包装类变量（包括参数传递）时，触发自动装箱\nint i1 &#x3D; 5\nInteger iobj1 &#x3D; 5;&#x2F;&#x2F;1\niobj &#x3D; i1;&#x2F;&#x2F;1\nList&lt;Integer&gt; list &#x3D; new ArrayList&lt;&gt;();\nlist.add(i1);&#x2F;&#x2F;1\n&#x2F;&#x2F;将包装类对象赋值给基本类型变量（包括参数传递）时，触发自动拆箱\nInteger iobj2&#x3D; new Integer(6);\nint i2 &#x3D; iobj2;&#x2F;&#x2F;2\n&#x2F;&#x2F;当包装类对象参与算术运算、关系运算（&lt;,&gt;）时，触发自动拆箱操作\nInteger iobj3 &#x3D; iobj1 + iobj2;\nboolean bl &#x3D; (iobj1 &lt; iobj2);\nbl &#x3D; (iobj1 &lt; 2);\n&#x2F;&#x2F;当包装类对象参与关系运算（&#x3D;&#x3D;），且另一方是基本类型数据时，触发自动拆箱操作。\nInteger iobj4 &#x3D; new Integer(123);\nbl &#x3D; (iob4 &#x3D;&#x3D; 123);\n\n\n方法变长参数：String… args用一个数组实现，用foreach遍历，编译后会被转变成数组\n\n枚举：当我们使用enum来定义一个枚举类型的时候，编译器会自动创建一个final类型的类继承Enum类，所以枚举类型不能被继承（public enum t&#123;&#125; =&gt; public final class T extends Enum&#123;&#125;）\n\n内部类：\n\n会独立于外部类，生成一个新的class文件，名字为外部类名$内部类名.class或外部类名$[序号].class，静态匿名内部类可访问静态成员变量+静态函数；普通匿名内部类不可访问外部函数中非final修饰的局部变量\n外部函数通过类似参数传递的方式，将局部变量通过值传递的方式传入到匿名内部类，这是外部函数局部变量的副本，所以如果能访问非final修饰的局部变量的话，内部类对其的更改不起作用，违反直觉，类似于形参的改变不影响实参\n\npublic interface I&#123;&#125;\npublic class A&#123;\n    private class B&#123;&#125; &#x2F;&#x2F;类似于ArrayList的内部类Itr\n    private class C implements I&#123;&#125;&#x2F;&#x2F;实现外部接口的内部类\n    public class D&#123;&#125;&#x2F;&#x2F;public修饰的内部类\n\t\tpublic static class E&#123;&#125;&#x2F;&#x2F;静态内部类\n    \n    public B getB()&#123; return new B(); &#125;\n    public I getC()&#123; return new C(); &#125;\n    public D getD()&#123; return new D(); &#125;\n&#125;\npublic class Demo&#123;\n    public static void main(String[] args)&#123;\n        A a &#x3D; new A();\n        A.B b &#x3D; a.getB();&#x2F;&#x2F;编译报错，满足封装原则\n        I c &#x3D; a.getC();&#x2F;&#x2F;可访问\n        A.D d1 &#x3D; a.getD();\n        A.D d2 &#x3D; a.new D();\n\t\t\t\tA.E e &#x3D; new A.E();&#x2F;&#x2F;静态内部类的对象可以独立于外部类单独创建\n    &#125;\n&#125;\n条件编译：if的条件是final且为false时，对应代码块不被编译，主要出于对代码优化的考虑\n\n断言：其实断言的底层实现就是if语言，如果断言结果为true，则什么都不做，程序继续执行，如果断言结果为false，则程序抛出AssertError来打断程序的执行\n\n数值字面量：不管是整数还是浮点数，都允许在数字之间插入任意多个下划线，为了方便阅读\n\n增强for循环：for-each用了普通的for循环和Iterator迭代器的hasNext()方法，在遍历过程中不能增删内部元素，会抛出异常（可以使用Iterator.remove()方法在删除当前迭代对象的同时维护索引的一致性）\n\ntry-with-resource：在try()中写资源申请，就不用在finally中判断是否为null在关闭了，编译期帮助我们关闭了（资源类需要实现Java.lang.AutoClosale接口）\n\nlambda表达式：只有一个函数的接口叫做函数式接口，可以用Lambda表达式简化\n\nLambda表达式\n(类型 a,类型 b)-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a，b为输入参数\n(a,b)-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a，b为输入参数\na-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a为输入参数\n&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;没有入参\n方法引用：当Lambda中的逻辑已经有现成的方法实现时，可以直接使用方法引用。方法引用要求所引用的方法的参数列表的返回值，跟函数接口中未实现方法的参数列表和返回值完全一致，格式如下\n&#x2F;&#x2F;对象::实例方法\n&#x2F;&#x2F;类::静态方法\n&#x2F;&#x2F;类::实例方法\npublic class FPDemo &#123;\n    public static void main(String] args) &#123;\n        List&lt;String&gt; strList &#x3D; Arrays.asList(&quot;wz-a.java&quot;, &quot;wz-b.txt&quot;, &quot;c.java&quot;);\n        strList.stream()\n\t\t\t\t\t\t&#x2F;&#x2F;直接引用String的方法\n            .filter(((Predicate&lt;String&gt;) String::isEmpty).negate())\n            &#x2F;&#x2F; .filter(s-&gt;s.isEmpty())\n            .filter(s-&gt;s.startsWith(&quot;wz-&quot;))\n            .map(String::length)\n            &#x2F;&#x2F;.map(s-&gt;s.length())\n            .forEach(l-&gt;System.out.printIn(I));&#x2F;&#x2F;输出9、8\n    &#125;\n&#125;\n\n\n\n4.特殊语法\n反射：在运行的过程中动态告知JVM去创建对象、创建方法、获取类信息（构造函数、方法、成员变量、注解），重要应用见Spring框架的依赖注入\n\nClass类：是一个存储类的信息的特殊的类，提供了大量的方法，可以获取类的信息，比如获取类中的方法，获取构造函数，获取成员变量等\n\nConstructor类：用来存储构造函数的信息，如通过newInstance()方法来进行有参/无参构造\nMethod类：存储方法的信息，如通过invoke()方法可以执行类中的对应方法\nField类：用来存储成员变量的信息\n\n\n获取反射的三种方法\n\n通过对象获取反射\nObject obj &#x3D; new Object(); &#x2F;&#x2F; 创建一个对象\nClass&lt;?&gt; clazz &#x3D; obj.getClass(); &#x2F;&#x2F; 获取 Class 对象\n通过类名获取反射\nClass&lt;?&gt; clazz &#x3D; Class.forName(&quot;com.example.MyClass&quot;); &#x2F;&#x2F; 获取 Class 对象\n通过类字面常量获取反射\nClass&lt;?&gt; clazz &#x3D; MyClass.class; &#x2F;&#x2F; 获取 Class 对象\n\n\n反射攻击：在Constructor、Method、Field类，包含一个公共的方法，能够改变构造函数、方法、成员变量的访问权限public void setAccessible(boolean flag)，利用这个方法，可以将私有的构造函数、方法、成员变量设置为可以访问的，这样就可以超越权限限制，在代码中访问私有的构造函数、方法和成员变量（打破单例类只能实例化一个对象的限制的情况）\n\n\n\n注解：注解相当于给元素打了一个tag，任何编译器或者应用程序通过反射可以访问的代码元素，都可以用注解去标识\n\n自定义注解：通过反射来读取注解，重要应用为Spring用注解代替XML配置文件\n&#x2F;&#x2F;Java内建注解\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Override &#123;\n&#125;\n\n&#x2F;&#x2F;自定义注解\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\npublic @interface RateLimit &#123;\n\tpublic enum TimeUnit &#123; SECOND,MINUTE, HOUR, DAY,MONTH&#125;\n    string apiName();\n\tint limitCount();\n\tTimeUnit timeUnit() default TimeUnit.SECOND;\n&#125;\n元注解\n\n@Target：用来描述注解的使用范围（如类、接口、方法、成员变量等）\n@Retention：用来描述注解的可见范围、或叫生命周期（如源码可见、字节码可见、运行时可见）\n@Documented：表示注解信息会输出到Javadoc文档中\n@interface：class、interface、enum、@interface这四者是平级关系，@interface用来定义注解，在注解中，还可以定义一些变量，特殊的是注解使用方法来定义变量，对于只有一个变量的注解，可以将其定义为value，这样，在使用时，可以不指定变量的名称\n\n\n实践应用\n\n替代注释：Guava提供@VisibleForTesting注解在方法上进行标记，这个注解只起到注释的作用，并没有实际的作用\n作为标记：Java中有一种特殊的接口，叫做标记接口（Marker Interface）。标记接口中不包含任何方法，跟注解类似，起到标记作用，比如RandomAccess、Cloneable、Serializable，可以根据标记接口判断对象是否可以执行某些操作\n替代XML文件\n@Configuration注解修饰的类中的@Bean创建首字母小写的对象\n@Component注解创建同名对象，使用@Autowired注入对象\n\n\n\n\n\n\n动态代理（==未完待续==）\n\n静态代理：通过实现接口或继承的方式，通过注入原始类并添加新功能的方式实现。实现简单，但会导致项目中的类成倍增加，所有相关的类都需要增加代理类，重复代码多\n动态代理\n一般静态指的编译阶段，动态指的运行阶段。在代理模式上，静态代理指的是在编译阶段时生成代理类的字节码，动态代理指的是运行时生成代理类的字节码，且字节码只存在与内存中，并不会生成对应的class文件\n之所以可以实现动态代理，是因为JVM设计得非常灵活，只要是符合类的格式的字节码，都可以在运行时被JVM解析并加载，不管这个字节码是来自预先编译好的(class文件)，还是在内存中临时生成的(典型应用:动态代理)，又或者从网络加载而来的(典型应用: Applet)。这部分内容涉及到JVM的类加载机制，见JVM\n实现方法一：利用JDK提供的类来实现（InvocationHandler接口+Proxy类）\n实现方法二：使用第三方的字节码类库来实现，比如CGLIB、BECL、ASM、Javassit等直接编辑字节码\n\n\n\n\n\n\n\n5.工具类5.1String\nString不可变的原因\nfinal修饰的数组，数组内容是可变的private final char value[];\n但是String没有暴露更改该数组的公共方法\n因为String类是final修饰的，所以子类无法继承，避免了子类破坏String的不可变性\n\n\n常量池技术\nString类型跟Integer等包装类类似，使用常量池技术，并且==只有使用字符串常量赋值时，才触发==，如果字符串常量在常量池中已经创建过，则直接使用已经创建的对象。用new创建的对象不在常量池中\n除了使用字符串常量赋值外，还可以使用intern()方法，将分配在堆上的String对象，原模原样在常量池中复制一份。当无法用字符串常量赋值，但又有大量重复字符串时，就可以使用intern()方法复制到常量池中，代码中使用常量池中的String对象，原String对象就被JVM回收掉\n\n\n其它\nsubstring()\nsubstring(int beginIndex, int endIndex)方法截取并返回下标在[beginIndex, endIndex)范围内的子串\n在JDK7及其以上版本中，substring()方法会生成新的String对象来存储子串，但如果传入参数正好等于字符串的长度，那么会返回字符串本身，不会创建新对象\n在JDK6及以前的版本，通过substring()方法获取到的子串会共享char数组，并有count和offset属性标志子串的长度和起点\n\n\n运算符重载：C++能直接重载运算符，但Java并不支持（重载运算符是函数式编程、并且语法太复杂），但是String类却实现了加法操作String sc = sa + sb;，主要是因为String比较常用，所以延续了基本类型及其包装类的设计，这样使用起来就方便和统一\nStringBuilder与StringBuffer\n因为String不可变，用+拼接效率低，每次都需要创建新的String对象，所以Java设计了StringBuilder\nStringBuilder支持修改和动态扩容，可以用append()函数拼接，可以把StringBuilder看作是char类型的ArrayList（ArrayList）\n在平时开发中，经常用+号连接多个字符串，实际上底层就采用StringBuilder来实现\nStringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。\n相同情况下使用 StringBuilder相比使用 StringBuffer仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。\n\n\n\n\n\n5.2JCF框架\nArrayList动态扩容：在增加元素的时候要检测是否需要扩容，首先确定最小扩容量（最小是10），然后判断是否需要扩容（最小扩容量大于当前数组长度），执行grow函数进行扩容，扩容为原来的1.5倍，如果不够的话就直接使用最小扩容量来作为长度，避免多次扩容，若是1.5倍长度大于数组最大长度，则需要看最小扩容量是否大于最大容量，如果是则为MAX_VALUE否则为MAX_VALUE-8\npublic boolean add(E e) &#123;\n    ensureCapacityInternal(size + 1);  &#x2F;&#x2F; Increments modCount!!\n    elementData[size++] &#x3D; e;\n    return true;\n&#125;\n\nprivate void ensureCapacityInternal(int minCapacity) &#123;\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;确定是否需要扩容，主要用在添加大量元素之前，减少增量分配的次数，通过提前扩容，可以提升性能&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic void ensureCapacity(int minCapacity) &#123;\n    int minExpand &#x3D; (elementData !&#x3D; DEFAULTCAPACITY_EMPTY_ELEMENTDATA)\n        &#x2F;&#x2F; any size if not default element table\n        ? 0\n        &#x2F;&#x2F; larger than default for default empty table. It&#39;s already\n        &#x2F;&#x2F; supposed to be at default size.\n        : DEFAULT_CAPACITY;\n\t\t&#x2F;&#x2F;如果期待最小容量大于已有的最大容量\n    if (minCapacity &gt; minExpand) &#123;\n        ensureExplicitCapacity(minCapacity);\n    &#125;\n&#125;\n&#x2F;&#x2F;得到最小扩容量\nprivate static int calculateCapacity(Object[] elementData, int minCapacity) &#123;\n    if (elementData &#x3D;&#x3D; DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;\n        return Math.max(DEFAULT_CAPACITY, minCapacity);\n    &#125;\n    return minCapacity;\n&#125;\n&#x2F;&#x2F;得到最小扩容量，通过最小扩容量扩容\nprivate void ensureCapacityInternal(int minCapacity) &#123;\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n&#125;\n&#x2F;&#x2F;判断是否需要扩容\nprivate void ensureExplicitCapacity(int minCapacity) &#123;\n    modCount++;\n\n    &#x2F;&#x2F; overflow-conscious code\n    if (minCapacity - elementData.length &gt; 0)\n\t\t\t\t&#x2F;&#x2F;调用grow方法进行扩容，调用此方法代表已经开始扩容了\n        grow(minCapacity);\n&#125;\nprivate void grow(int minCapacity) &#123;\n    &#x2F;&#x2F;oldCapacity为旧容量，newCapacity为新容量\n    int oldCapacity &#x3D; elementData.length;\n    &#x2F;&#x2F;将oldCapacity 右移一位，其效果相当于oldCapacity &#x2F;2，\n    &#x2F;&#x2F;我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，\n    int newCapacity &#x3D; oldCapacity + (oldCapacity &gt;&gt; 1);\n    &#x2F;&#x2F;然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，\n    if (newCapacity - minCapacity &lt; 0)\n        newCapacity &#x3D; minCapacity;\n    &#x2F;&#x2F;再检查新容量是否超出了ArrayList所定义的最大容量，\n    &#x2F;&#x2F;若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE，\n    &#x2F;&#x2F;如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为MAX_ARRAY_SIZE。\n    if (newCapacity - MAX_ARRAY_SIZE &gt; 0)\n        newCapacity &#x3D; hugeCapacity(minCapacity);\n    &#x2F;&#x2F; minCapacity is usually close to size, so this is a win:\n    elementData &#x3D; Arrays.copyOf(elementData, newCapacity);\n&#125;\n&#x2F;&#x2F;比较minCapacity和MAX_ARRAY_SIZE\nprivate static int hugeCapacity(int minCapacity) &#123;\n    if (minCapacity &lt; 0) &#x2F;&#x2F; overflow\n        throw new OutOfMemoryError();\n    return (minCapacity &gt; MAX_ARRAY_SIZE) ?\n        Integer.MAX_VALUE :\n        MAX_ARRAY_SIZE;\n&#125;\nHashMap\n\nSet容器包括HashSet、LinkedHashSet、TreeSet，从代码实现上来说，这三个类底层分别是依赖HashMap、LinkedHashMap、TreeMap。例如：往HashSet中存储对象obj，底层将obj作为key，一个空的Object对象作为value，一并存储到HashMap中\n\n底层为哈希表，对key求哈希作为hash值，包裹hash值、key和value为Node对象，作为哈希表（数组+链表）的组成节点。key不能重复，存储重复的key，新value会覆盖旧value（可以存一个key为null的键值对，但是不同key的value都可以是null）\n&#x2F;&#x2F; 包含另一个“Map”的构造函数\n public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123;\n     this.loadFactor &#x3D; DEFAULT_LOAD_FACTOR;\n     putMapEntries(m, false);&#x2F;&#x2F;下面会分析到这个方法\n &#125;\nfinal void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123;\n    int s &#x3D; m.size();\n    if (s &gt; 0) &#123;\n        &#x2F;&#x2F; 判断table是否已经初始化\n        if (table &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; pre-size\n            &#x2F;&#x2F; 未初始化，s为m的实际元素个数\n            float ft &#x3D; ((float)s &#x2F; loadFactor) + 1.0F;\n            int t &#x3D; ((ft &lt; (float)MAXIMUM_CAPACITY) ?\n                    (int)ft : MAXIMUM_CAPACITY);\n            &#x2F;&#x2F; 计算得到的t大于阈值，则初始化阈值\n            if (t &gt; threshold)\n                threshold &#x3D; tableSizeFor(t);\n        &#125;\n        &#x2F;&#x2F; 已初始化，并且m元素个数大于阈值，进行扩容处理\n        else if (s &gt; threshold)\n            resize();\n        &#x2F;&#x2F; 将m中的所有元素添加至HashMap中\n        for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123;\n            K key &#x3D; e.getKey();\n            V value &#x3D; e.getValue();\n            putVal(hash(key), key, value, false, evict);\n        &#125;\n    &#125;\n&#125;\n哈希函数\nstatic final int hash(Object key) &#123;\n    int h;\n\t\t&#x2F;&#x2F;key为null的值存储在下标为0的位置，但一个HashMap只能存储一个值为null的key\n\t\t&#x2F;&#x2F;hashCode底层为JNI，定义在Object类中，根据对象在内存中的地址来计算哈希值，子类中可以重写\n\t\t&#x2F;&#x2F;h^(h&gt;&gt;&gt;16)：数组长度一般不超过2^16，所以通过将h的高16位和低16位异或，来增加参与运算的信息\n    return (key &#x3D;&#x3D; null) ? 0 : (h &#x3D; key.hashCode()) ^ (h &gt;&gt;&gt; 16);\n&#125;\n&#x2F;&#x2F;确定插入数组时的位置，使用位操作与数组长度n进行取模计算（前提是n为2的幂次方），防止索引越界\nint index &#x3D; hash(key)&amp;(n-1); &#x2F;&#x2F; n-1为 11111，与其进行&amp;运算，相当于对n取余数\n\npublic V get(Object key) &#123;\n    Node&lt;K,V&gt; e;\n    return (e &#x3D; getNode(hash(key), key)) &#x3D;&#x3D; null ? null : e.value;\n&#125;\nfinal Node&lt;K,V&gt; getNode(int hash, Object key) &#123;\n    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;\n    if ((tab &#x3D; table) !&#x3D; null &amp;&amp; (n &#x3D; tab.length) &gt; 0 &amp;&amp;\n        (first &#x3D; tab[(n - 1) &amp; hash]) !&#x3D; null) &#123;&#x2F;&#x2F; hash表不为空，待查找链表有值\n        if (first.hash &#x3D;&#x3D; hash &amp;&amp; &#x2F;&#x2F; always check first node，先查hash(key)，再查key.equals()\n            ((k &#x3D; first.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))&#x2F;&#x2F;检测是否哈希冲突\n            return first;\n        if ((e &#x3D; first.next) !&#x3D; null) &#123;\n            if (first instanceof TreeNode) &#x2F;&#x2F;已经树化，进行树上的查找\n                return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);\n            do &#123;\n                if (e.hash &#x3D;&#x3D; hash &amp;&amp;\n                    ((k &#x3D; e.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))\n                    return e;\n            &#125; while ((e &#x3D; e.next) !&#x3D; null);&#x2F;&#x2F;未树化，进行链表上的遍历查找\n        &#125;\n    &#125;\n    return null;\n&#125;\n装载因子：table大小（n）和装载因子（loadFactor）可以用默认的也可以通过构造函数传入，一般为0.75：\n\n权衡时间效率和空间效率之后的结果\n大概是[0.5,1]之间，因为小于0.5会有一半空间从来未用，当大于1时，哈希冲突的概率会大大增加，即使有链表和树化，也会影响性能\n因为table数组的大小n都是2的倍数，而且触发扩容的阈值threshold = n * loadfactor，所以，在[0.5,1]之间，只有0.75能使得得到的阈值一直是整数\n\npublic HashMap(int initialCapacity, float loadFactor) &#123;\n\t\t&#x2F;&#x2F;...initialCapacity和loadFactor的可行性检验代码...\n    this.loadFactor &#x3D; loadFactor;\n\t\t&#x2F;&#x2F;直接赋值的原因：此时table数组只声明未创建，其值为null，在第一次调用put()函数后，\n\t\t&#x2F;&#x2F;HashMap会先用threshold作为数组大小创建table数组，再将其重新赋值为真正的扩容阈值\n\t\t&#x2F;&#x2F;this.table &#x3D; new T[this.threshold];\n\t\t&#x2F;&#x2F;this.threshold *&#x3D; this.factor;\n    this.threshold &#x3D; tableSizeFor(initialCapacity);\n&#125;\n&#x2F;&#x2F;initialCapacity需要是2的幂次方，如果不是，需要寻找比initialCapacity大的第一个2的幂次方数\nstatic final int tableSizeFor(int cap) &#123; &#x2F;&#x2F; 1100  12 应该返回 10000\n    int n &#x3D; cap - 1; &#x2F;&#x2F; 1011\n    n |&#x3D; n &gt;&gt;&gt; 1; &#x2F;&#x2F; 0101 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 2; &#x2F;&#x2F; 0011 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 4; &#x2F;&#x2F; 0000 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 8; &#x2F;&#x2F; 0000 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 16; &#x2F;&#x2F; 0000 - 1111\n    return (n &lt; 0) ? 1 : (n &gt;&#x3D; MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#x2F;&#x2F; 10000 16\n&#125;\n动态扩容：put后，若元素个数超过threshold=n*loadFactor时触发（n为table大小，loadFactor为装载因子）\n\nHashMap的默认初始化大小为16，之后每次扩充容量为原来的2倍，如果指定了大小，也会选择2的幂次来作为初始值\n因为Hashmap的容量大小是2的幂次方，所以可以通过&amp;运算来优化%运算。例如：（16 % 5 ）等价于 （16 &amp; （5 - 1））\n为了能把数据分配均匀，Hash值的范围是-2147483648 到 2147483647，很难碰撞，但是需要对数组取模，操作如上\n\n\n因为容量变大，位置会发生变化，将每个节点的hash值与新的容量取模，取模操作仍可以用位运算来替代，但JDK8中优化为：如果node.hash&amp;oldCap == 0，则节点在新table数组中的下标不变；如果node.hash &amp; oldCap != 0，则节点在新table数组中的下标变为i+oldCap（i为在原数组的下标）\n扫描table数组中的每一条链表，根据节点的下标是否更改，将链表中的节点分配到lo链表和hi链表，lo链表中存储的是下标值未变的节点，hi链表存储的是下标值有所改变的节点。处理完一条链表后，将lo链表和hi链表分别存储到新的table数组中的对应位置\n\npublic V put(K key, V value) &#123;\n    return putVal(hash(key), key, value, false, true);\n&#125;\n\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent,\n               boolean evict) &#123;\n    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;\n    if ((tab &#x3D; table) &#x3D;&#x3D; null || (n &#x3D; tab.length) &#x3D;&#x3D; 0)\n        n &#x3D; (tab &#x3D; resize()).length; &#x2F;&#x2F;使用resize创建新table\n    if ((p &#x3D; tab[i &#x3D; (n - 1) &amp; hash]) &#x3D;&#x3D; null)&#x2F;&#x2F;数组中链表头不存在，初始化\n        tab[i] &#x3D; newNode(hash, key, value, null);\n    else &#123;&#x2F;&#x2F;数组中插入位置有链表头，遍历\n        Node&lt;K,V&gt; e; K k;\n        if (p.hash &#x3D;&#x3D; hash &amp;&amp;\n            ((k &#x3D; p.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))&#x2F;&#x2F;先检查第一个节点\n            e &#x3D; p;&#x2F;&#x2F;找到\n        else if (p instanceof TreeNode)\n            e &#x3D; ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);\n        else &#123;\n\t\t\t\t\t\t&#x2F;&#x2F;遍历链表\n            for (int binCount &#x3D; 0; ; ++binCount) &#123;\n                if ((e &#x3D; p.next) &#x3D;&#x3D; null) &#123;&#x2F;&#x2F;没找到，新建节点\n                    p.next &#x3D; newNode(hash, key, value, null);\n\t\t\t\t\t\t\t\t\t\t&#x2F;&#x2F; 如果链表元素个数大于等于TREEIFY_THRESHOLD（8）\n                    if (binCount &gt;&#x3D; TREEIFY_THRESHOLD - 1) &#x2F;&#x2F; -1 for 1st\n                        treeifyBin(tab, hash); &#x2F;&#x2F;树化？红黑树转换，并不会直接转换成红黑树\n                    break;\n                &#125;\n                if (e.hash &#x3D;&#x3D; hash &amp;&amp;\n                    ((k &#x3D; e.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k)))) &#x2F;&#x2F;找到\n                    break;\n                p &#x3D; e;&#x2F;&#x2F;继续遍历\n            &#125;\n        &#125;\n        if (e !&#x3D; null) &#123; &#x2F;&#x2F; existing mapping for key\n            V oldValue &#x3D; e.value;\n            if (!onlyIfAbsent || oldValue &#x3D;&#x3D; null)\n                e.value &#x3D; value;&#x2F;&#x2F;更新值\n            afterNodeAccess(e);&#x2F;&#x2F;见LinkedHashMap\n            return oldValue;\n        &#125;\n    &#125;\n    ++modCount;\n    if (++size &gt; threshold)\n        resize();\n    afterNodeInsertion(evict);&#x2F;&#x2F;见LinkedHashMap\n    return null;\n&#125;\n\nfinal Node&lt;K,V&gt;[] resize() &#123;\n    Node&lt;K,V&gt;[] oldTab &#x3D; table;\n    int oldCap &#x3D; (oldTab &#x3D;&#x3D; null) ? 0 : oldTab.length;\n    int oldThr &#x3D; threshold;\n    int newCap, newThr &#x3D; 0;\n    if (oldCap &gt; 0) &#123;\n        &#x2F;&#x2F; 超过最大值就不再扩充了，就只好随你碰撞去吧\n        if (oldCap &gt;&#x3D; MAXIMUM_CAPACITY) &#123;\n            threshold &#x3D; Integer.MAX_VALUE;\n            return oldTab;\n        &#125;\n        &#x2F;&#x2F; 没超过最大值，就扩充为原来的2倍\n        else if ((newCap &#x3D; oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;&#x3D; DEFAULT_INITIAL_CAPACITY)\n            newThr &#x3D; oldThr &lt;&lt; 1; &#x2F;&#x2F; double threshold\n    &#125;\n    else if (oldThr &gt; 0) &#x2F;&#x2F; initial capacity was placed in threshold\n        newCap &#x3D; oldThr;\n    else &#123;\n        &#x2F;&#x2F; signifies using defaults\n        newCap &#x3D; DEFAULT_INITIAL_CAPACITY;\n        newThr &#x3D; (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);\n    &#125;\n    &#x2F;&#x2F; 计算新的resize上限\n    if (newThr &#x3D;&#x3D; 0) &#123;\n        float ft &#x3D; (float)newCap * loadFactor;\n        newThr &#x3D; (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);\n    &#125;\n    threshold &#x3D; newThr;\n    @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)\n        Node&lt;K,V&gt;[] newTab &#x3D; (Node&lt;K,V&gt;[])new Node[newCap];\n    table &#x3D; newTab;\n    if (oldTab !&#x3D; null) &#123;\n        &#x2F;&#x2F; 把每个bucket都移动到新的buckets中\n        for (int j &#x3D; 0; j &lt; oldCap; ++j) &#123;\n            Node&lt;K,V&gt; e;\n            if ((e &#x3D; oldTab[j]) !&#x3D; null) &#123;\n                oldTab[j] &#x3D; null;\n                if (e.next &#x3D;&#x3D; null)\n                    newTab[e.hash &amp; (newCap - 1)] &#x3D; e;\n                else if (e instanceof TreeNode)\n                    ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);\n                else &#123;\n                    Node&lt;K,V&gt; loHead &#x3D; null, loTail &#x3D; null;\n                    Node&lt;K,V&gt; hiHead &#x3D; null, hiTail &#x3D; null;\n                    Node&lt;K,V&gt; next;\n                    do &#123;\n                        next &#x3D; e.next;\n                        &#x2F;&#x2F; 原索引\n                        if ((e.hash &amp; oldCap) &#x3D;&#x3D; 0) &#123;\n                            if (loTail &#x3D;&#x3D; null)\n                                loHead &#x3D; e;\n                            else\n                                loTail.next &#x3D; e;\n                            loTail &#x3D; e;\n                        &#125;\n                        &#x2F;&#x2F; 原索引+oldCap\n                        else &#123;\n                            if (hiTail &#x3D;&#x3D; null)\n                                hiHead &#x3D; e;\n                            else\n                                hiTail.next &#x3D; e;\n                            hiTail &#x3D; e;\n                        &#125;\n                    &#125; while ((e &#x3D; next) !&#x3D; null);\n                    &#x2F;&#x2F; 原索引放到bucket里\n                    if (loTail !&#x3D; null) &#123;\n                        loTail.next &#x3D; null;\n                        newTab[j] &#x3D; loHead;\n                    &#125;\n                    &#x2F;&#x2F; 原索引+oldCap放到bucket里\n                    if (hiTail !&#x3D; null) &#123;\n                        hiTail.next &#x3D; null;\n                        newTab[j + oldCap] &#x3D; hiHead;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n    return newTab;\n&#125;\n链表树化：降低单个链表长度（jdk1.8新增的特性，1.7仅有链表）\n\n当某个链表中的节点个数大于等于8（TREEIFY_THRESHOLD静态常量），并且table数组的大小大于等于64时，将会把链表转化为红黑树，这个过程就叫treeify（树化）\n\n如果table数组长度小于64，即便链表中的节点个数大于等于8，也不会触发treeify，而是触发扩容操作，将长链表拆分为短链表\n\n当红黑树中节点个数比较少时，HashMap会再将其转换回链表，因为维护红黑树的成本比较高，对于少许节点，使用链表存储更高效，红黑树转换为链表的过程，叫做untreeify，促发untreeify的场景有以下两个：\n\n删除键值对：如果红黑树满足以下结构，则会触发untreeify，这个结构的红黑树的节点个数应该处于[2,6]之间，尽管treeify的阈值是8，但untreeify的阈值是[2,6]之间的某个数，之所以不相等是为了避免频繁的插入删除操作，导致节点个数在7，8之间频繁波动\n&#x2F;&#x2F;removeTreeNode函数中\nif (root &#x3D;&#x3D; null || root.right &#x3D;&#x3D; null ||\n    (rl &#x3D; root.left) &#x3D;&#x3D; null || rl.left &#x3D;&#x3D; null) &#123;\n    tab[index] &#x3D; first.untreeify(map);  &#x2F;&#x2F; too small\n    return;\n&#125;\n扩容：每一条链表都会分割为lo和hi两条，同理红黑树也会分割为lt和ht两个红黑树，lt中存储的是下标位置不变的节点，ht中存储的是下标位置变化的节点。不过，在构建lt和ht之前，会先统计属于lt和ht的节点个数lc和hc，如果lc小于等于6（UNTREEIFY_THRESHOLD静态常量），在新的table数组中，HashMap会使用链表来存储下标不变的节点，同理，如果hc小于等于6，在新的table数组中，HashMap会使用链表来存储下标改变的节点。\n\n\n\n\n\n\n\nCollections\n\nsort()：用来对List进行排序，默认为从小到大，支持传入Comparator接口的匿名类改为降序，底层依赖Arrays\n基本类型数组排序算法：JDK8及以后使用DualPivotQuickSort()，JDK7及其以前使用快排，使用不稳定排序\nDualPivotQuickSort根据长度和元素类型，使用双轴快速排序算法、插入排序、计数排序、归并排序等算法来组合进行排序操作\n\n\n对象数组排序算法：JDK8及其以后使用TimSort()，JDK7及其以前使用归并排序，使用的是稳定的排序方式\nTimSort用非递归版本归并排序，归并到阈值后开始进行二分插入排序算法，即在插入时选择用二分查找来确定插入位置\n\n\n\n\nbinarySearch()：用来对已排序的List容器进行二分查找，因为涉及元素比较，所以需要传入实现Comparable接口的对象或者主动传入Comparator接口的匿名类对象\nindexedBinarySearch：查找mid使用的是链表的get函数，需要从头遍历链表来得到对应值\niteratorBinarySearch：查找mid使用的是新定义的get函数，从上一次迭代器的位置（mid）开始向前或向后查找，需要遍历的范围变小了，执行效率就变高了\n\n\nsynchronizedXXX()：JCF中的容器都是非线程安全的，当要使用线程安全的容器时，首选使用JUC并发容器，但当没有合适的JUC并发容器可以使用时，可以使用Collectinos类中的synchronizedXXX()函数来创建线程安全的容器\n\n\n\n5.3Exception体系\n\n\n\n\n\n\n\n\n相比较C语言返回错误码的方式，可以携带更多的错误信息（message、stack trace等），并且可以将业务代码和异常处理代码分离，这样代码的可读性会更好\n\n异常体系\n\n\n继承自Error的异常：表示程序无法处理的严重错误，这些错误有可能导致线程或JVM终止\n继承自Exception的异常：也叫做受检异常（Checked Exception）或编译时异常（Compile Exception），在编写代码的时候，需要主动取捕获或者在函数定义中声明此类异常，否则编译就会报错\n继承自RuntimeException的异常：也叫做非受检异常（Unchecked Exception）或者运行时异常（Runtime Exception），在编写代码的时候，可以不主动取捕获和在函数定义中声明此类异常，不处理也可以通过编译\n\n\n自定义异常：要么继承自Exception，要么继承自RuntimeException，但是现在一般都依赖框架来编程，受检和非受检异常大部分情况下都会被框架兜底捕获并处理，并不会直接导致程序的终止，所以从这个角度来看，继承自哪个异常均可\n&#x2F;&#x2F;受检异常的使用违反开闭原则，整条调用链都需要修改代码；非受检异常需要主动处理，但容易被遗忘\npublic class UserNotExistingException extends Exception&#123;\n    public UserNotExistingException()&#123;\n        super();\n    &#125;\n    public UserNotExistingException(String msg,Throwable cause)&#123;\n        super(msg,cause)；\n    &#125;\n    public UserNotExistingException(String msg)&#123;\n        super(msg);\n    &#125;\n    public UserNotExistingException(Throwable cause)&#123;\n        super(cause);\n    &#125;\n&#125;\n异常处理\n\n打印调用链：在函数内部，如果某代码的异常行为，并不会导致调用此函数的上层代码出现异常行为，也就是说，上层代码并不关心被调用函数内部的这个异常，我们就可以在函数内部将这个异常捕获并打印日志记录\npublic void f() throws LowLevelException&#123;...&#125;\n&#x2F;&#x2F;捕获后记录日志\npublic void g()&#123;\n    try&#123;\n        f();\n    &#125;catch(LowLevelException e)&#123;\n        log.warn(&quot;...&quot;,e);&#x2F;&#x2F;使用日志框架记录日志\n    &#125;\n&#125;\n使用throws抛出异常：如果函数内部的异常行为会导致调用此函数的上层代码出现异常行为，那么，就必须让上层代码感知此异常的存在\n&#x2F;&#x2F;原封不动再抛出\n&#x2F;&#x2F;如果LowLevelException是非受检异常，则不需要再函数g()定义中声明\npublic void g() throws LowLevelException&#123;\n    f();\n&#125;\n使用new创建新的异常：如果此异常跟函数的业务相关，上层代码在调用此函数时，知道如何处理异常，那么直接将其抛出即可；如果此异常跟业务无关，上层代码无法理解这个异常的含义，那么就需要包装成新的跟函数业务相关的异常重新抛出\n&#x2F;&#x2F;包装成新异常抛出\npublic void g()&#123;\n    try&#123;\n        f();\n    &#125;catch(LowLevelExceptioin e)&#123;\n\t\t\t\t&#x2F;&#x2F;异常调用链可以完整的描述异常发生的整个过程，但需要特别注意的是，捕获异常并包裹成新的异常抛出时，\n\t\t\t\t&#x2F;&#x2F;一定要将先前的异常通过cause参数（下面代码中的e）传递进新的异常，否则，异常调用链会断开\n        throw new HighLevelException(&quot;...&quot;,e);\n    &#125;\n&#125;\n\n\n异常实现原理：异常代码块执行顺序：不管try监听的代码块有没有异常抛出，finally代码块总是被执行，并且在finally代码执行完成之后，try代码块和catch代码块中的return语句才会被执行\n\n\n异常表：对应于上图最后一部分的Exception table，其中from、to、target都表示字节码的行号，当行号在[from，to）之间的代码抛出type类型的异常时，JVM会跳转至target行字节码继续执行\n异常兜底：第50行代码开始，主要是捕获try代码块和catch代码块中未被捕获的异常，然后再执行完finally代码块之后，在原封不动的将异常抛出\nfinally内联：JVM在生成字节码时，会将finally代码块内联（插入）到try代码块和catch代码块中的return语句之前，这样就可以实现不管程序是否抛出异常，finally代码块总是会被执行，并且再函数返回之前执行。如果finally有return语句，会提前返回\n\n\n异常性能分析\n\n使用new创建异常：在堆上创建异常对象，初始化成员变量，调用异常父类Throwable中的fillInStackTrace()函数生成栈追踪信息，通过getStackTrace()函数打印stackTrace栈追踪信息（当调用层次过深时，会导致fillInStackTrace耗时高，所以在递归中不要轻易抛出异常）\n&#x2F;&#x2F;当创建异常时函数调用栈中的所有函数的信息，栈追踪信息记录了异常产生的整个函数调用链路，方便定位此异常是如何产生的\nprivate StackTraceElement[] stackTrace;\npublic final class StackTraceElement implements java.io.Serializable &#123;\n    &#x2F;&#x2F; Normally initialized by VM (public constructor added in 1.5)\n    private String declaringClass;&#x2F;&#x2F;函数所属类名\n    private String methodName;&#x2F;&#x2F;函数名\n    private String fileName;&#x2F;&#x2F;函数所属类文件名\n    private int    lineNumber;&#x2F;&#x2F;异常抛出时，函数执行到了哪一行\n    &#x2F;&#x2F;...\n&#125;\n&#x2F;&#x2F;通过getStackTrace()函数，将异常的stackTrace栈追踪信息打印出来\nRuntimeException e &#x3D; new RuntimeException(&quot;oops&quot;);\nStackTraceElement[] stackTrace &#x3D; e.getStackTrace();\nfor(StackTraceElement element : stackTrace)&#123;\n    System.out.println(element);\n&#125;\n使用throw抛出异常：当有函数抛出异常时，JVM会在底层执行栈展开（stack unwinding），依次将函数调用栈中的函数栈帧弹出，直到找到哪个函数可以捕获这个异常为止，然后JVM从这个函数继续再执行（不同于return导致的栈展开，异常导致的栈展开会有一个在函数的异常表中查找是否有可匹配的处理规则的过程，这样的查找在调用层次过深时和耗时）\n&#x2F;&#x2F;throw new RuntimeException(&quot;oops&quot;)这样一个异常抛出代码包括两个操作：创建异常和抛出异常等价于下面的两行代码\nRuntimeException e &#x3D; new RuntimeException(&quot;oops!&quot;);\nthrow e;\n打印异常调用链\n\n\n\n\n\n\n\n\n\n每个异常的stackTrace栈追踪消息都是一直到main函数的，不可以只记录生命周期内的函数，因为stackTrace栈追踪信息是在异常创建时生成的，在打印异常时，异常的声明周期未必就一定结束，所以无法只填充生命周期内所经历的函数\n\n原封不动抛出：相当于没捕获\n\n封装成新的异常抛出\ntry&#123;\n    &#x2F;&#x2F;...\n&#125;catch(IOException e)&#123;\n\t\t&#x2F;&#x2F;将捕获的异常通过cause参数传递给新的异常，调用链就不会断，主要调用了下面的Throwable的构造函数\n    throw new RuntimeException(&quot;oops&quot;,e);\n&#125;\n\npublic class Throwable&#123;\n    private String detailMessage;\n    private Throwable cause &#x3D; this;&#x2F;&#x2F;异常调用\n    private StackTraceElement[] stackTrace &#x3D; UNASSIGNED_STACK;\n    \n    public Throwable(String message, Throwable cause) &#123;\n        fillInStackTrace();&#x2F;&#x2F;生成stackTrace\n        detailMessage &#x3D; message;\n        this.cause &#x3D; cause;\n    &#125;\n    &#x2F;&#x2F;...\n&#125;\n记录日志：一般在开发中使用日志框架来记录异常，异常调用链信息会输出到日志文件中，方便开发者事后查看，一般不推荐使用e.pringStackTrace()来打印异常日志，因为会打印到标准出错输出System.err中，即命令行中，这不方便保存以便反复查看\ntry&#123;\n    &#x2F;&#x2F;...\n&#125;catch(IOException e)&#123;\n    log.error(&quot;...&quot;,e);\n    &#x2F;&#x2F;e.printStackTrace() 不推荐\n&#125;\n\n\n\n\n异常最佳实践：对于业务异常只需要将一些有用的信息，记录在异常的detailMessage成员变量中即可，通过向构造函数的参数writableStackTrace传入false，即可禁止在创建异常的同时调用fillStackTrace()函数\nprotected Throwable(String message, Throwable cause,\n                    boolean enableSuppression,\n                    boolean writableStackTrace) &#123;\n    if (writableStackTrace) &#123;\n        fillInStackTrace();\n    &#125; else &#123;\n        stackTrace &#x3D; null;\n    &#125;\n    detailMessage &#x3D; message;\n    this.cause &#x3D; cause;\n    if (!enableSuppression)\n        suppressedExceptions &#x3D; null;\n&#125;\n&#x2F;&#x2F;使用，可以解决高并发下程序中大量业务异常导致的程序变慢的问题\npublic class UserNotExistingException extends Throwable&#123;\n    public UserNotExistingException() &#123;\n        super(null,null,true,false);\n    &#125;\n\n    public UserNotExistingException(String message) &#123;\n        super(message,null,true,false);    &#125;\n\n    public UserNotExistingException(String message, Throwable cause) &#123;\n        super(message,cause,true,false);\n    &#125;\n\n    public UserNotExistingException(Throwable cause) &#123;\n        super(null,cause,true,false);\n    &#125;\n&#125;\n\n","slug":"Java特性","date":"2023-04-13T11:25:47.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"a7db680819d5984afbc70473b0c79529","title":"SaToken","content":"Sa-Token\n\n\n\n\n\n\n\n\n一款开源的、轻量级的Java权限认证框架，主要解决：登陆认证、权限认证、单点登录、OAuth2.0、分布式Session会话、微服务网关鉴权等问题\n1.登陆认证\n登陆流程\n\n用户提交name+password参数，调用登录接口\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;示例&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n@RequestMapping(&quot;doLogin&quot;)\npublic CommonResult doLogin(String name, String pwd)&#123;\n\t&#x2F;&#x2F;将用户传过来的name和pwd与数据库中的进行比对\n\tif(&quot;admin&quot;.equals(name) &amp;&amp; &quot;123456&quot;.equals(pwd))&#123;\n\t\t&#x2F;&#x2F;根据账号id，进行登陆\n\t\t&#x2F;&#x2F;id为要登陆的账号，类型建议使用long、int、String类型\n\t\tStpUtil.login(10001);\n\t\treturn CommonResult.success(&quot;登陆成功&quot;)；\n\t&#125;\n\treturn CommonResult.failed(&quot;登录失败&quot;)；\n&#125;\n登陆成功，通过Cookie上下文返回给前端这个用户的token，该用户后续请求都带上这个token，服务器可以根据token判断此会话是否登陆成功\n\n如果校验未通过，则抛出异常，告知其需要先进行登陆\n\n\n\n示例\n&#x2F;**\n * 登录测试 \n *&#x2F;\n@RestController\n@RequestMapping(&quot;&#x2F;acc&#x2F;&quot;)\npublic class LoginController &#123;\n\n    &#x2F;&#x2F; 测试登录  ---- &lt;http:&#x2F;&#x2F;localhost:8081&#x2F;acc&#x2F;doLogin?name&#x3D;zhang&amp;pwd&#x3D;123456&gt;\n    @RequestMapping(&quot;doLogin&quot;)\n    public SaResult doLogin(String name, String pwd) &#123;\n        &#x2F;&#x2F; 此处仅作模拟示例，真实项目需要从数据库中查询数据进行比对 \n        if(&quot;zhang&quot;.equals(name) &amp;&amp; &quot;123456&quot;.equals(pwd)) &#123;\n            StpUtil.login(10001);\n            return SaResult.ok(&quot;登录成功&quot;);\n        &#125;\n        return SaResult.error(&quot;登录失败&quot;);\n    &#125;\n\n    &#x2F;&#x2F; 查询登录状态  ---- &lt;http:&#x2F;&#x2F;localhost:8081&#x2F;acc&#x2F;isLogin&gt;\n    @RequestMapping(&quot;isLogin&quot;)\n    public SaResult isLogin() &#123;\n        return SaResult.ok(&quot;是否登录：&quot; + StpUtil.isLogin());\n    &#125;\n    \n    &#x2F;&#x2F; 查询 Token 信息  ---- &lt;http:&#x2F;&#x2F;localhost:8081&#x2F;acc&#x2F;tokenInfo&gt;\n    @RequestMapping(&quot;tokenInfo&quot;)\n    public SaResult tokenInfo() &#123;\n        return SaResult.data(StpUtil.getTokenInfo());\n    &#125;\n    \n    &#x2F;&#x2F; 测试注销  ---- &lt;http:&#x2F;&#x2F;localhost:8081&#x2F;acc&#x2F;logout&gt;\n    @RequestMapping(&quot;logout&quot;)\n    public SaResult logout() &#123;\n        StpUtil.logout();\n        return SaResult.ok();\n    &#125;\n    \n&#125;\n\n2.权限认证\n@RestControllerAdvice注解\n\n与切面有关的注解，作用范围为项目中使用了@RequestMapping的类\n\n与@ExceptionHandler的组合使用：两个注解组合使用是一个全局异常处理方法，发生了对应异常后，进入@ExceptionHandler修饰的方法，在这里处理全局异常（打印到日志里）\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;拦截全局异常&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n@RestControllerAdvice\npublic class GlobalExceptionHandler &#123;\n    &#x2F;&#x2F; 全局异常拦截 \n    @ExceptionHandler\n    public SaResult handlerException(Exception e) &#123;\n        e.printStackTrace(); \n        return SaResult.error(e.getMessage());\n    &#125;\n&#125;\n底层原理：https://juejin.cn/post/7025484367539470344\n\nExceptionHandlerExceptionResolver实现了InitializingBean接口的afterPropertiesSet()方法，方法内调用initExceptionHandlerAdviceCache()扫描所有带@ControllerAdvice注解的类放入到adviceBeans链表里，然后将所有adviceBeans转换为ExceptionHandlerMethodResolver，转换过程中扫描每个ControllerAdvice中的带@ExceptionHandler注解的方法，再取出带@ExceptionHandler所处理的Exception类型，以类型为key，方法为value插入到mappedMethods这个map中进行缓存，最后将adviceBean和resolver插入到exceptionHandlerAdviceCache中进行缓存\n\n\n\n\nAPI\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;权限认证&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F; 获取：当前账号所拥有的权限集合\nStpUtil.getPermissionList();\n\n&#x2F;&#x2F; 判断：当前账号是否含有指定权限, 返回 true 或 false\nStpUtil.hasPermission(&quot;user.add&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定权限, 如果验证未通过，则抛出异常: NotPermissionException \nStpUtil.checkPermission(&quot;user.add&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定权限 [指定多个，必须全部验证通过]\nStpUtil.checkPermissionAnd(&quot;user.add&quot;, &quot;user.delete&quot;, &quot;user.get&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定权限 [指定多个，只要其一验证通过即可]\nStpUtil.checkPermissionOr(&quot;user.add&quot;, &quot;user.delete&quot;, &quot;user.get&quot;);\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;角色校验&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F; 获取：当前账号所拥有的角色集合\nStpUtil.getRoleList();\n\n&#x2F;&#x2F; 判断：当前账号是否拥有指定角色, 返回 true 或 false\nStpUtil.hasRole(&quot;super-admin&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定角色标识, 如果验证未通过，则抛出异常: NotRoleException\nStpUtil.checkRole(&quot;super-admin&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定角色标识 [指定多个，必须全部验证通过]\nStpUtil.checkRoleAnd(&quot;super-admin&quot;, &quot;shop-admin&quot;);        \n\n&#x2F;&#x2F; 校验：当前账号是否含有指定角色标识 [指定多个，只要其一验证通过即可] \nStpUtil.checkRoleOr(&quot;super-admin&quot;, &quot;shop-admin&quot;);\n注解鉴权\n\n注解示例\n\n@SaCheckLogin: 登录校验 —— 只有登录之后才能进入该方法。\n@SaCheckRole(&quot;admin&quot;): 角色校验 —— 必须具有指定角色标识才能进入该方法。\n@SaCheckPermission(&quot;user:add&quot;): 权限校验 —— 必须具有指定权限才能进入该方法。\n@SaCheckSafe: 二级认证校验 —— 必须二级认证之后才能进入该方法。\n@SaCheckBasic: HttpBasic校验 —— 只有通过 Basic 认证后才能进入该方法。\n@SaIgnore：忽略校验 —— 表示被修饰的方法或类无需进行注解鉴权和路由拦截器鉴权。\n@SaCheckDisable(&quot;comment&quot;)：账号服务封禁校验 —— 校验当前账号指定服务是否被封禁。\n\n\n注册拦截器：开启Sa-Token的全局蓝机器到项目中\n@Configuration\npublic class SaTokenConfigure implements WebMvcConfigurer &#123;\n    &#x2F;&#x2F; 注册 Sa-Token 拦截器，打开注解式鉴权功能 \n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        &#x2F;&#x2F; 注册 Sa-Token 拦截器，打开注解式鉴权功能 \n        registry.addInterceptor(new SaInterceptor()).addPathPatterns(&quot;&#x2F;**&quot;);    \n    &#125;\n&#125;\n使用注解鉴权\n&#x2F;&#x2F; 登录校验：只有登录之后才能进入该方法 \n@SaCheckLogin                        \n@RequestMapping(&quot;info&quot;)\npublic String info() &#123;\n    return &quot;查询用户信息&quot;;\n&#125;\n\n&#x2F;&#x2F; 角色校验：必须具有指定角色才能进入该方法 \n@SaCheckRole(&quot;super-admin&quot;)        \n@RequestMapping(&quot;add&quot;)\npublic String add() &#123;\n    return &quot;用户增加&quot;;\n&#125;\n\n&#x2F;&#x2F; 权限校验：必须具有指定权限才能进入该方法 \n@SaCheckPermission(&quot;user-add&quot;)        \n@RequestMapping(&quot;add&quot;)\npublic String add() &#123;\n    return &quot;用户增加&quot;;\n&#125;\n\n&#x2F;&#x2F; 二级认证校验：必须二级认证之后才能进入该方法 \n@SaCheckSafe()        \n@RequestMapping(&quot;add&quot;)\npublic String add() &#123;\n    return &quot;用户增加&quot;;\n&#125;\n\n&#x2F;&#x2F; Http Basic 校验：只有通过 Basic 认证后才能进入该方法 \n@SaCheckBasic(account &#x3D; &quot;sa:123456&quot;)\n@RequestMapping(&quot;add&quot;)\npublic String add() &#123;\n    return &quot;用户增加&quot;;\n&#125;\n\n&#x2F;&#x2F; 校验当前账号是否被封禁 comment 服务，如果已被封禁会抛出异常，无法进入方法 \n@SaCheckDisable(&quot;comment&quot;)                \n@RequestMapping(&quot;send&quot;)\npublic String send() &#123;\n    return &quot;查询用户信息&quot;;\n&#125;\n&#x2F;&#x2F; 此接口加上了 @SaIgnore 可以游客访问 ，表示一个接口忽略认证\n@SaIgnore\n@RequestMapping(&quot;getList&quot;)\npublic SaResult getList() &#123;\n    &#x2F;&#x2F; ... \n    return SaResult.ok(); \n&#125;\n\n\n\n3.路由拦截鉴权\n注册Sa-Token路由拦截器\n&#x2F;&#x2F;注册了一个基于StpUtil.checkLogin()的登陆校验拦截器，除了&#x2F;user&#x2F;doLogin接口都需要登陆才能访问\n@Configuration\npublic class SaTokenConfigure implements WebMvcConfigurer &#123;\n    &#x2F;&#x2F; 注册拦截器\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        &#x2F;&#x2F; 注册 Sa-Token 拦截器，校验规则为 StpUtil.checkLogin() 登录校验。\n        registry.addInterceptor(new SaInterceptor(handle -&gt; StpUtil.checkLogin()))\n                .addPathPatterns(&quot;&#x2F;**&quot;)\n                .excludePathPatterns(&quot;&#x2F;user&#x2F;doLogin&quot;); \n    &#125;\n&#125;\n完整的配置方式示例\n@Configuration\npublic class SaTokenConfigure implements WebMvcConfigurer &#123;\n    &#x2F;&#x2F; 注册 Sa-Token 的拦截器\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        &#x2F;&#x2F; 注册路由拦截器，自定义认证规则 \n        registry.addInterceptor(new SaInterceptor(handler -&gt; &#123;\n            \n            &#x2F;&#x2F; 登录校验 -- 拦截所有路由，并排除&#x2F;user&#x2F;doLogin 用于开放登录 \n            SaRouter.match(&quot;&#x2F;**&quot;, &quot;&#x2F;user&#x2F;doLogin&quot;, r -&gt; StpUtil.checkLogin());\n\n            &#x2F;&#x2F; 角色校验 -- 拦截以 admin 开头的路由，必须具备 admin 角色或者 super-admin 角色才可以通过认证 \n            SaRouter.match(&quot;&#x2F;admin&#x2F;**&quot;, r -&gt; StpUtil.checkRoleOr(&quot;admin&quot;, &quot;super-admin&quot;));\n\n            &#x2F;&#x2F; 权限校验 -- 不同模块校验不同权限 \n            SaRouter.match(&quot;&#x2F;user&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;user&quot;));\n            SaRouter.match(&quot;&#x2F;admin&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;admin&quot;));\n            SaRouter.match(&quot;&#x2F;goods&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;goods&quot;));\n            SaRouter.match(&quot;&#x2F;orders&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;orders&quot;));\n            SaRouter.match(&quot;&#x2F;notice&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;notice&quot;));\n            SaRouter.match(&quot;&#x2F;comment&#x2F;**&quot;, r -&gt; StpUtil.checkPermission(&quot;comment&quot;));\n            \n            &#x2F;&#x2F; 甚至你可以随意的写一个打印语句\n            SaRouter.match(&quot;&#x2F;**&quot;, r -&gt; System.out.println(&quot;----啦啦啦----&quot;));\n\n            &#x2F;&#x2F; 连缀写法\n            SaRouter.match(&quot;&#x2F;**&quot;).check(r -&gt; System.out.println(&quot;----啦啦啦----&quot;));\n            \n        &#125;)).addPathPatterns(&quot;&#x2F;**&quot;);\n    &#125;\n&#125;\n\n4.Session会话\nSession是会话中专业的数据缓存组件，通过Session可以缓存一些高频读写的数据，提高程序性能。在Sa-Token中，Session分为三种\n\nUser-Session: 指的是框架为每个 账号id 分配的 Session\nToken-Session: 指的是框架为每个 token 分配的 Session\nCustom-Session: 指的是以一个 特定的值 作为SessionId，来分配的 Session\n\n\n在Session上存取值\n&#x2F;&#x2F; 写值 \nsession.set(&quot;name&quot;, &quot;zhang&quot;); \n\n&#x2F;&#x2F; 写值 (只有在此key原本无值的时候才会写入)\nsession.setDefaultValue(&quot;name&quot;, &quot;zhang&quot;);\n\n&#x2F;&#x2F; 取值\nsession.get(&quot;name&quot;);\n\n&#x2F;&#x2F; 取值 (指定默认值)\nsession.get(&quot;name&quot;, &quot;&lt;defaultValue&gt;&quot;); \n\n&#x2F;&#x2F; 取值 (若无值则执行参数方法, 之后将结果保存到此键名下,并返回此结果   若有值则直接返回, 无需执行参数方法)\nsession.get(&quot;name&quot;, () -&gt; &#123;\n            return ...;\n        &#125;);\n\n&#x2F;&#x2F; ---------- 数据类型转换： ----------\nsession.getInt(&quot;age&quot;);         &#x2F;&#x2F; 取值 (转int类型)\nsession.getLong(&quot;age&quot;);        &#x2F;&#x2F; 取值 (转long类型)\nsession.getString(&quot;name&quot;);     &#x2F;&#x2F; 取值 (转String类型)\nsession.getDouble(&quot;result&quot;);   &#x2F;&#x2F; 取值 (转double类型)\nsession.getFloat(&quot;result&quot;);    &#x2F;&#x2F; 取值 (转float类型)\nsession.getModel(&quot;key&quot;, Student.class);     &#x2F;&#x2F; 取值 (指定转换类型)\nsession.getModel(&quot;key&quot;, Student.class, &lt;defaultValue&gt;);  &#x2F;&#x2F; 取值 (指定转换类型, 并指定值为Null时返回的默认值)\n\n&#x2F;&#x2F; 是否含有某个key (返回true或false)\nsession.has(&quot;key&quot;); \n\n&#x2F;&#x2F; 删值 \nsession.delete(&#39;name&#39;);          \n\n&#x2F;&#x2F; 清空所有值 \nsession.clear();                 \n\n&#x2F;&#x2F; 获取此 Session 的所有key (返回Set&lt;String&gt;)\nsession.keys();\n\n&#x2F;&#x2F; 返回此 Session 的id \nsession.getId();                          \n\n&#x2F;&#x2F; 返回此 Session 的创建时间 (时间戳) \nsession.getCreateTime();                  \n\n&#x2F;&#x2F; 返回此 Session 会话上的底层数据对象（如果更新map里的值，请调用session.update()方法避免产生脏数据）\nsession.getDataMap();                     \n\n&#x2F;&#x2F; 将这个 Session 从持久库更新一下\nsession.update();                         \n\n&#x2F;&#x2F; 注销此 Session 会话 (从持久库删除此Session)\nsession.logout();\n\n5.框架配置############## Sa-Token 配置 (文档: &lt;https:&#x2F;&#x2F;sa-token.cc&gt;) ##############\nsa-token: \n    # token名称 (同时也是cookie名称)\n    token-name: satoken\n    # token有效期，单位s 默认30天, -1代表永不过期 \n    timeout: 2592000\n    # token临时有效期 (指定时间内无操作就视为token过期) 单位: 秒\n    activity-timeout: -1\n    # 是否允许同一账号并发登录 (为true时允许一起登录, 为false时新登录挤掉旧登录) \n    is-concurrent: true\n    # 在多人登录同一账号时，是否共用一个token (为true时所有登录共用一个token, 为false时每次登录新建一个token) \n    is-share: true\n    # token风格\n    token-style: uuid\n    # 是否输出操作日志 \n    is-log: false\n\n6.Sa-Token集成Redis\nMaven\n&lt;!-- Sa-Token 整合 Redis （使用 jdk 默认序列化方式） --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;cn.dev33&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;sa-token-dao-redis&lt;&#x2F;artifactId&gt;\n    &lt;version&gt;1.34.0&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n&lt;!-- 提供Redis连接池 --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;commons-pool2&lt;&#x2F;artifactId&gt;\n&lt;&#x2F;dependency&gt;\n配置\nspring: \n    # redis配置 \n    redis:\n        # Redis数据库索引（默认为0）\n        database: 1\n        # Redis服务器地址\n        host: 127.0.0.1\n        # Redis服务器连接端口\n        port: 6379\n        # Redis服务器连接密码（默认为空）\n        # password: \n        # 连接超时时间\n        timeout: 10s\n        lettuce:\n            pool:\n                # 连接池最大连接数\n                max-active: 200\n                # 连接池最大阻塞等待时间（使用负值表示没有限制）\n                max-wait: -1ms\n                # 连接池中的最大空闲连接\n                max-idle: 10\n                # 连接池中的最小空闲连接\n                min-idle: 0\n\n7.前后端分离\n常规Web端可以使用Cookie进行鉴权（后端控制写入、请求自动提交），但是在app、小程序等前后端分离的场景，一般没有Cookie这一功能，可以通过如下方式\n\n不能后端控制写入了，就前端自己写入。（难点在后端如何将 Token 传递到前端）\n&#x2F;&#x2F; 登录接口\n@RequestMapping(&quot;doLogin&quot;)\npublic SaResult doLogin() &#123;\n    &#x2F;&#x2F; 第1步，先登录上 \n    StpUtil.login(10001);\n    &#x2F;&#x2F; 第2步，获取 Token 相关参数(tokenName和tokenValue)\n    SaTokenInfo tokenInfo &#x3D; StpUtil.getTokenInfo();\n    &#x2F;&#x2F; 第3步，返回给前端，并保存在前端\n    return SaResult.data(tokenInfo);\n&#125;\n每次请求不能自动提交了，那就手动提交。（难点在前端如何将 Token 传递到后端，同时后端将其读取出来）\n&#x2F;&#x2F; 1、首先在登录时，将tokenName和tokenValue一起存储在本地，例如：\nuni.setStorageSync(&#39;tokenName&#39;, tokenName); \nuni.setStorageSync(&#39;tokenValue&#39;, tokenValue); \n\n&#x2F;&#x2F; 2、在发起ajax的地方，获取这两个值, 并组织到head里 \nvar tokenName &#x3D; uni.getStorageSync(&#39;tokenName&#39;);    &#x2F;&#x2F; 从本地缓存读取tokenName值\nvar tokenValue &#x3D; uni.getStorageSync(&#39;tokenValue&#39;);    &#x2F;&#x2F; 从本地缓存读取tokenValue值\nvar header &#x3D; &#123;\n    &quot;content-type&quot;: &quot;application&#x2F;x-www-form-urlencoded&quot;\n&#125;;\nif (tokenName !&#x3D; undefined &amp;&amp; tokenName !&#x3D; &#39;&#39;) &#123;\n    header[tokenName] &#x3D; tokenValue;\n&#125;\n\n&#x2F;&#x2F; 3、后续在发起请求时将 header 对象塞到请求头部 \nuni.request(&#123;\n    url: &#39;&lt;https:&#x2F;&#x2F;www.example.com&#x2F;request&gt;&#39;, &#x2F;&#x2F; 仅为示例，并非真实接口地址。\n    header: header,\n    success: (res) &#x3D;&gt; &#123;\n        console.log(res.data);    \n    &#125;\n&#125;);\n\n\n\n8.密码加密\n封装的加密算法\n\n摘要加密（md5、sha1、sha256）\n&#x2F;&#x2F; md5加密 \nSaSecureUtil.md5(&quot;123456&quot;);\n\n&#x2F;&#x2F; sha1加密 \nSaSecureUtil.sha1(&quot;123456&quot;);\n\n&#x2F;&#x2F; sha256加密 \nSaSecureUtil.sha256(&quot;123456&quot;);\n对称加密（AES）\n&#x2F;&#x2F; 定义秘钥和明文\nString key &#x3D; &quot;123456&quot;;\nString text &#x3D; &quot;Sa-Token 一个轻量级java权限认证框架&quot;;\n\n&#x2F;&#x2F; 加密 \nString ciphertext &#x3D; SaSecureUtil.aesEncrypt(key, text);\nSystem.out.println(&quot;AES加密后：&quot; + ciphertext);\n\n&#x2F;&#x2F; 解密 \nString text2 &#x3D; SaSecureUtil.aesDecrypt(key, ciphertext);\nSystem.out.println(&quot;AES解密后：&quot; + text2);\n非对称加密（RSA）\n&#x2F;&#x2F; 定义私钥和公钥 \nString privateKey &#x3D; &quot;MIICdgIBADANBgkqhkiG9w0BAQEFAASCAmAwggJcAgEAAoGBAO+wmt01pwm9lHMdq7A8gkEigk0XKMfjv+4IjAFhWCSiTeP7dtlnceFJbkWxvbc7Qo3fCOpwmfcskwUc3VSgyiJkNJDs9ivPbvlt8IU2bZ+PBDxYxSCJFrgouVOpAr8ar&#x2F;b6gNuYTi1vt3FkGtSjACFb002&#x2F;68RKUTye8&#x2F;tdcVilAgMBAAECgYA1COmrSqTUJeuD8Su9ChZ0HROhxR8T45PjMmbwIz7ilDsR1+E7R4VOKPZKW4Kz2VvnklMhtJqMs4MwXWunvxAaUFzQTTg2Fu&#x2F;WU8Y9ha14OaWZABfChMZlpkmpJW9arKmI22ZuxCEsFGxghTiJQ3tK8npj5IZq5vk+6mFHQ6aJAQJBAPghz91Dpuj+0bOUfOUmzi22obWCBncAD&#x2F;0CqCLnJlpfOoa9bOcXSusGuSPuKy5KiGyblHMgKI6bq7gcM2DWrGUCQQD3SkOcmia2s&#x2F;6i7DUEzMKaB0bkkX4Ela&#x2F;xrfV+A3GzTPv9bIBamu0VIHznuiZbeNeyw7sVo4&#x2F;GTItq&#x2F;zn2QJdBAkEA8xHsVoyXTVeShaDIWJKTFyT5dJ1TR++&#x2F;udKIcuiNIap34tZdgGPI+EM1yoTduBM7YWlnGwA9urW0mj7F9e9WIQJAFjxqSfmeg40512KP&#x2F;ed&#x2F;lCQVXtYqU7U2BfBTg8pBfhLtEcOg4wTNTroGITwe2NjL5HovJ2n2sqkNXEio6Ji0QQJAFLW1Kt80qypMqot+mHhS+0KfdOpaKeMWMSR4Ij5VfE63WzETEeWAMQESxzhavN1WOTb3&#x2F;p6icgcVbgPQBaWhGg&#x3D;&#x3D;&quot;;\nString publicKey &#x3D; &quot;MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDvsJrdNacJvZRzHauwPIJBIoJNFyjH47&#x2F;uCIwBYVgkok3j+3bZZ3HhSW5Fsb23O0KN3wjqcJn3LJMFHN1UoMoiZDSQ7PYrz275bfCFNm2fjwQ8WMUgiRa4KLlTqQK&#x2F;Gq&#x2F;2+oDbmE4tb7dxZBrUowAhW9NNv+vESlE8nvP7XXFYpQIDAQAB&quot;;\n&#x2F;&#x2F; 生成一对公钥和私钥，其中Map对象 (private&#x3D;私钥, public&#x3D;公钥)\nSystem.out.println(SaSecureUtil.rsaGenerateKeyPair());\n\n&#x2F;&#x2F; 文本\nString text &#x3D; &quot;Sa-Token 一个轻量级java权限认证框架&quot;;\n\n&#x2F;&#x2F; 使用公钥加密\nString ciphertext &#x3D; SaSecureUtil.rsaEncryptByPublic(publicKey, text);\nSystem.out.println(&quot;公钥加密后：&quot; + ciphertext);\n\n&#x2F;&#x2F; 使用私钥解密\nString text2 &#x3D; SaSecureUtil.rsaDecryptByPrivate(privateKey, ciphertext);\nSystem.out.println(&quot;私钥解密后：&quot; + text2);\n\n\n\n9.多账号体系认证\n比如一个电商系统同时有user表和admin表，两套账号都适用StpUtil类的API进行登陆鉴权，势必会发生逻辑冲突\n\nstpLogic：StpUtil只是对成员变量stpLogic的各个API包装一下进行转发，这样的实现有以下好处\n\nStpLogic 类的所有函数都可以被重写，按需扩展\n在构造方法时随意传入一个不同的 loginType，就可以再造一套账号登录体系\n\n\n示例\n\n对于原生StpUtil类，只做admin账号权限认证，而对于user账号，则新建一个权限认证类（StpUserUtil.java），将StpUtil代码全部复制到StpUserUtil中，更改一下他的loginType，其它使用方式相同\npublic class StpUserUtil &#123;\n    \n    &#x2F;**\n     * 账号体系标识 \n     *&#x2F;\n    public static final String TYPE &#x3D; &quot;user&quot;;    &#x2F;&#x2F; 将 LoginType 从&#96;login&#96;改为&#96;user&#96; \n\n    &#x2F;&#x2F; 其它代码 ... \n\n&#125;\n相关注解的使用：默认只支持StpUtil类\n&#x2F;&#x2F; 通过type属性指定此注解校验的是我们自定义的&#96;StpUserUtil&#96;，而不是原生&#96;StpUtil&#96;\n@SaCheckLogin(type &#x3D; StpUserUtil.TYPE)\n@RequestMapping(&quot;info&quot;)\npublic String info() &#123;\n    return &quot;查询用户信息&quot;;\n&#125;\n使用自定义注解简化前一种方式\n\n重写Sa-Token默认的注解处理器\n@Configuration\npublic class SaTokenConfigure &#123;\n    @Autowired\n    public void rewriteSaStrategy() &#123;\n        &#x2F;&#x2F; 重写Sa-Token的注解处理器，增加注解合并功能 \n        SaStrategy.me.getAnnotation &#x3D; (element, annotationClass) -&gt; &#123;\n            return AnnotatedElementUtils.getMergedAnnotation(element, annotationClass); \n        &#125;;\n    &#125;\n&#125;\n自定义一个注解\n&#x2F;**\n * 登录认证(User版)：只有登录之后才能进入该方法 \n * &lt;p&gt; 可标注在函数、类上（效果等同于标注在此类的所有方法上） \n *&#x2F;\n@SaCheckLogin(type &#x3D; &quot;user&quot;)\n@Retention(RetentionPolicy.RUNTIME)\n@Target(&#123; ElementType.METHOD, ElementType.TYPE&#125;)\npublic @interface SaUserCheckLogin &#123;\n    \n&#125;\n使用示例\n&#x2F;&#x2F; 使用 @SaUserCheckLogin 的效果等同于使用：@SaCheckLogin(type &#x3D; &quot;user&quot;)\n@SaUserCheckLogin\n@RequestMapping(&quot;info&quot;)\npublic String info() &#123;\n    return &quot;查询用户信息&quot;;\n&#125;\n\n\n\n\n同端多登陆\n\n一个设备同时可以登陆两套账户，可能会发生token覆盖的问题，新登陆的token会覆盖掉旧的token\n\n解决办法：更改StpUserUtil（前文配置的新类）的TokenName\npublic class StpUserUtil &#123;\n    \n    &#x2F;&#x2F; 使用匿名子类 重写&#96;stpLogic对象&#96;的一些方法 \n    public static StpLogic stpLogic &#x3D; new StpLogic(&quot;user&quot;) &#123;\n        &#x2F;&#x2F; 重写 StpLogic 类下的 &#96;splicingKeyTokenName&#96; 函数，返回一个与 &#96;StpUtil&#96; 不同的token名称, 防止冲突 \n        @Override\n        public String splicingKeyTokenName() &#123;\n            return super.splicingKeyTokenName() + &quot;-user&quot;;\n        &#125;\n        &#x2F;&#x2F; 同理你可以按需重写一些其它方法 ... \n    &#125;; \n    &#x2F;&#x2F; ... \n&#125;\n再次调用 StpUserUtil.login(10001)进行登录授权时，token的名称将不再是 satoken，而是我们重写后的 satoken-user\n\n\n\n不同体系不同SaTokenConfig配置：自定义的 StpUserUtil 需要使用不同 SaTokenConfig 对象\npublic class StpUserUtil &#123;\n    \n    &#x2F;&#x2F; 使用匿名子类 重写&#96;stpLogic对象&#96;的一些方法 \n    public static StpLogic stpLogic &#x3D; new StpLogic(&quot;user&quot;) &#123;\n        \n        &#x2F;&#x2F; 首先自定义一个 Config 对象 \n        SaTokenConfig config &#x3D; new SaTokenConfig()\n            .setTokenName(&quot;satoken&quot;)\n            .setTimeout(2592000)\n            &#x2F;&#x2F; ... 其它set\n            ;\n        \n        &#x2F;&#x2F; 然后重写 stpLogic 配置获取方法 \n        @Override\n        public SaTokenConfig getConfig() &#123;\n            return config;\n        &#125;\n    &#125;;\n    \n    &#x2F;&#x2F; ... \n    \n&#125;\n多账号体系下，在拦截器中给一个接口登录鉴权的方法如下\n&#x2F;&#x2F; 注册 Sa-Token 拦截器\n@Override\npublic void addInterceptors(InterceptorRegistry registry) &#123;\n    registry.addInterceptor(new SaInterceptor(handle -&gt; &#123;\n        \n        &#x2F;&#x2F; 如果这个接口，要求客户端登录了后台 Admin 账号才能访问：\n        SaRouter.match(&quot;&#x2F;art&#x2F;getInfo&quot;).check(r -&gt; StpUtil.checkLogin());\n\n        &#x2F;&#x2F; 如果这个接口，要求客户端登录了前台 User 账号才能访问：\n        SaRouter.match(&quot;&#x2F;art&#x2F;getInfo&quot;).check(r -&gt; StpUserUtil.checkLogin());\n        \n        &#x2F;&#x2F; 如果这个接口，要求客户端同时登录 Admin 和 User 账号，才能访问：\n        SaRouter.match(&quot;&#x2F;art&#x2F;getInfo&quot;).check(r -&gt; &#123;\n            StpUtil.checkLogin();\n            StpUserUtil.checkLogin();\n        &#125;);\n\n        &#x2F;&#x2F; 如果这个接口，要求客户端登录 Admin 和 User 账号任意一个，就能访问：\n        SaRouter.match(&quot;&#x2F;art&#x2F;getInfo&quot;).check(r -&gt; &#123;\n            if(StpUtil.isLogin() &#x3D;&#x3D; false &amp;&amp; StpUserUtil.isLogin() &#x3D;&#x3D; false) &#123;\n                throw new SaTokenException(&quot;请登录后再访问接口&quot;);\n            &#125;\n        &#125;);\n        \n    &#125;)).addPathPatterns(&quot;&#x2F;**&quot;);\n&#125;\n\n","slug":"SaToken","date":"2023-04-13T10:50:40.000Z","categories_index":"","tags_index":"middleware","author_index":"Dajunnnnnn"},{"id":"9a0ea7adf62049ce83652b0a7c894a59","title":"Java数据结构","content":"Java数据结构1.数据结构1.1常用方法\n\n\n接口\nAPI\n\n\n\nCollection\nsize、isEmpty、contains、toArray、add、remove、clear\n\n\nList\nget(index)、set(index)、add(index,element)、remove(index)、indexOf()、lastIndexOf()、subList(from, to)、sort\n\n\nQueue\noffer(element)、poll()、peek()\n\n\nDuque\nofferFirst(E e)、offerLast(E e)、pollFirst()、pollLast()、peekFirst()、peekLast()、push(E e)、pop()\n\n\nSet\nsize、isEmpty、contains、toArray、add、remove\n\n\nSortedSet\nSortedSet subSet(E fromElement, E toElement)、headSet(E toElement)、tailSet(E fromElement)、first、last\n\n\nMap\nsize、isEmpty、containsKey、containsValue、get、put、remove、keySet、values、entrySet\n\n\nMap补\ngetOrDefault(Object key, V defaultValue)、putIfAbsent(K key, V value)、replace(K key, V oldValue, V newValue)\n\n\n1.2工具类\n\n\n类名\nAPI\n\n\n\nString\ncharAt、toCharArray、split、substring（新String）、indexOf、lastIndexOf、replace、length\n\n\nString补\ntrim、toLowerCase、toUpperCase、split(String regex)、format（格式化输出，同c）\n\n\nStringBuilder\nappend、toString、charAt、length、delete、replace、insert、reverse、indexOf、lastIndexOf\n\n\nCollections\nsort（list）、binarySearch、reverse、swap、fill、copy、replaceAll、emptyXXX\n\n\nArrays\nsort、binarySearch、equals、fill、asList、copyOf、copyOfRange\n\n\nMath\nmin、max、abs、sqrt(double)、pow(double, double)、ceil（上整）、floor（下整）、round（四舍五入）\n\n\nMath补\nInteger.MAX_VALUE、Integer.MIN_VALUE、\n\n\nScanner\nnext（下一String）、nextInt、nextLong、nextLine（nextInt不会洗掉换行符，需要nextLine吸掉）\n\n\nSystem.out\nprintln、print、format(“x = %d, y = %f\\n”, x, y)\n\n\n\n简化代码：输入一串数字组成的字符\n&#x2F;&#x2F; 1 2 3 4 5...\nint[] nums&#x3D;Arrays.stream(scanner.nextLine().split(&quot; &quot;)).mapToInt(Integer::parseInt).toArray();\n\n1.3补充知识\nArrayList\n\n实现特殊接口\n\nRandomAccess：Arrays的静态方法binarySearch会根据接口调用不同的实现方法\nCloneable：使用clone方法，返回一个浅拷贝\n\n\n底层为可动态扩容的数组（支持存储null数据）\n\n首先==确定最小扩容量==，默认最小为10，如果传入的所需容量比10大，则按传入的所需容量来扩容\n\n然后==判断是否需要扩容==，如果前一阶段判定的需要容量比内部数组的长度大，则进行扩容\n\n使用位移操作，将容量扩展为内部数组长度的1.5倍，如果比需要容量小，则直接使用需要容量，防止多次扩容，然后使用System.arraycopy来复制数据\npublic static native void arraycopy(Object src,  int  srcPos,Object dest, int destPos,int length);\n\n\n使用modCount：来记录容量更改的次数，每次调用ensureCapacityInternal就将modCount加1，容量不够使才改容量。用来确定迭代的过程中，是否有其他线程更改过数据，如果有人修改过，则抛出ConcurrentModificationException异常\n\n\n\nLinkedList\n\n可以根据引用的接口不同，使用不同方法，支持List、Queue、Deque，根据结构的不同可以调用不同的方法\n底层为双向链表，并且有头尾指针，支持存储null数据\n\n\nArrayDeque\n\n基于数组实现，性能比LinkedList好，也可用来实现栈\n\n\nPriorityQueue\n\n底层依赖堆来实现（使用可变长数组），默认情况下为小顶堆，最先出队列的为当前队列中的最小值，支持Comparator接口\nQueue&lt;Integer&gt; minH &#x3D; new PriorityQueue&lt;&gt;(); &#x2F;&#x2F;小顶堆，默认大小为11\nQueue&lt;Integer&gt; maxH &#x3D; new PriorityQueue&lt;&gt;((i1, i2) -&gt; i2 - i1); &#x2F;&#x2F;大顶堆，默认大小为11\n不支持存储NULL和non-comparable对象，通过堆元素的上浮和下沉，实现了在O(logn)的时间复杂度内插入和删除堆顶元素\n\n堆的构建过程，需要比较节点中数据的大小，所以，添加到优先级队列中的元素，需要能够比较大小，方法有两种：基于Comparable接口和基于Comparator接口，都有时则优先使用comparator，详见siftUp\nprivate void siftUp(int k, E x) &#123;\n    if (comparator !&#x3D; null)\n        siftUpUsingComparator(k, x);\n    else\n        siftUpComparable(k, x);\n&#125;\n\n\nSet（HashSet、LinkedHashSet、TreeSet）\n\n底层实现分别为：HashMap、LinkedHashMap、TreeMap，存储对象的时候，使用对象作为key，一个空的Object对象作为value，插入到底层的Map中，不管\n如何检查重复：无论Set中是否已经存在了某元素，都会直接在底层进行插入，通过add方法的返回值来确定插入前是否有相同的元素\n应用场景：HashSet用于==不需要保证元素插入和取出顺序==的场景；LinkedHashSet用于==保证元素的插入和取出顺序满足FIFO==的场景（LinedHashMap底层使用双向有序链表+哈希表）；TreeSet用于支持对元素==自定义排序规则==的场景\n\n\nHashMap（==数组+链表/红黑树==）\n\n底层为哈希表，对key求哈希作为hash值，包裹hash值、key和value为Node对象，作为哈希表（数组+链表）的组成节点。key不能重复，存储重复的key，新value会覆盖旧value（可以存一个key为null的键值对，但是不同key的value都可以是null）\n底层数组长度为2的倍数：hash函数可以使用与n-1取交替代与n取余、装载因子使用0.75使得阈值（n*0.75）一直为整数、初始化的时候选择比传入参数大的最小2的幂次方数\n动态扩容：默认初始化大小为16，每次超过阈值的时候就扩容为原来的2倍；扫描数组的每一条链表，根据节点下标决定是否要更改，插入到lo链表（不需改）和hi链表（需要改），处理完一条链表，将新链表插入到对应位置\n新位置确定方式：如果node.hash&amp;oldCap == 0，则节点在新table数组中的下标不变；如果node.hash &amp; oldCap != 0，则节点在新table数组中的下标变为i+oldCap（i为在原数组的下标）\n链表树化：当某个链表中的节点个数大于等于8（TREEIFY_THRESHOLD静态常量），并且table数组的长度大于等于64时，将会把链表转化为红黑树；如果table长度不满足则触发扩容操作；如果红黑树节点数在[2，6]之间，则退化为链表\n\n\n\n\nArrays的sort\n\nCollections的sort函数底层依赖的Arrays类的sort函数，如List接口中的sort的默认实现\n基本类型：使用==DualPivotQuickSort==，jdk7之前使用快排\n对快排进行改进，选取两个pivot，通过数组的长度决定什么时候选用双轴快排、插入排序、归并排序、记数排序\n\n\n对象数组：使用==TimSort==，jdk7之前使用归并\n使用非递归版本归并排序算法，在归并排序的过程中，大的排序区间不断分解为小的待排序区间，如果带排序区间的长度小于MIN_MERGE（32），就不再继续分解，转而执行二分插入排序算法\n二分插入排序：将数组分为已排序区间和未排序区间，通过二分查找，查找插入位置，当找到后，通过调用System.arraycopy()函数，将插入点之后的数据整体快速后移一位，腾出位置给要插入的数据\n\n\n\n\nString（final数组）\n\nString不可变的原因：内部是final修饰的数组（引用不可改但是数据可改）、没有提供更改数组的方法、String类也是final的子类无法继承，避免了子类破坏String的不变性\n常量池技术：使用字符串常量赋值时触发，直接复用常量池已存在的对象，也可以使用intern方法复制堆上对象到常量池并回收堆上的对象（判等的时候使用equals()）\n运算符重载：因为String比较常用，所以延续了基本类型和包装类的设计，实现了加法操作String sc = sa + sb;，底层使用了StringBuilder来实现（StringBuffer加了锁，是线程安全的）\n\n\n\n2.算法2.1复杂度分析\n分析方法\n加法原则：总复杂度等于量级最大的那段代码的复杂度\n乘法原则：嵌套代码的复杂度等于嵌套内外的代码复杂度乘积\n其他方法：某一条语句执行的总次数；数据被访问的次数；使用递归树来分析\n\n\n空间复杂度\n不关注存储数据所需要的空间，而是关注算法所需要的额外存储消耗（循环、递归调用栈、辅助存储）\n由于现有题型大多以耗时为指标，所以尽可能使用==以空间换时间==的思想\n\n\n时间复杂度\n不看低阶和常数系数、加法取大、乘法取积\n分类：最好、最坏、平均\n\n\n\n2.2技巧\n双指针\n\n前缀和数组：原始数组不会被修改的情况下，频繁查询某个区间的累加和\n\n\n\n前缀和数组中两个元素的差，及这段区间的累加和\n示例：原数组{3,5,2,-1,4,1}；前缀和数组{0,3,8,10,8,12,13}\n\n\n差分数组：频繁对原数组的某个区间的元素进行增减\n\n\n原理：对i→n的所有元素都加3，对j+1→n的所有元素都减3\n示例：原数组{8,2,6,3,1}；差分数组{8,-6,4,-3,-2}\n\n\n单调栈：满足单调性的栈结构\n\n\n插入过程：将一个元素插入单调栈时，为了维护栈的单调性，需要先弹出一些元素直到新插入的元素可以不破坏单调性\n\n伪代码\ninsert x\nwhile !sta.empty() &amp;&amp; sta.top()&lt;x\n    sta.pop()\nsta.push(x)\n\n\n并查集（Union-Find）\nclass UF &#123;\n    &#x2F;&#x2F; 连通分量个数\n    private int count;\n    &#x2F;&#x2F; 存储每个节点的父节点\n    private int[] parent;\n\n    &#x2F;&#x2F; n 为图中节点的个数\n    public UF(int n) &#123;\n        this.count &#x3D; n;\n        parent &#x3D; new int[n];\n        for (int i &#x3D; 0; i &lt; n; i++) &#123;\n            parent[i] &#x3D; i;\n        &#125;\n    &#125;\n    \n    &#x2F;&#x2F; 将节点 p 和节点 q 连通\n    public void union(int p, int q) &#123;\n        int rootP &#x3D; find(p);\n        int rootQ &#x3D; find(q);\n        \n        if (rootP &#x3D;&#x3D; rootQ)\n            return;\n        \n        parent[rootQ] &#x3D; rootP;\n        &#x2F;&#x2F; 两个连通分量合并成一个连通分量\n        count--;\n    &#125;\n\n    &#x2F;&#x2F; 判断节点 p 和节点 q 是否连通\n    public boolean connected(int p, int q) &#123;\n        int rootP &#x3D; find(p);\n        int rootQ &#x3D; find(q);\n        return rootP &#x3D;&#x3D; rootQ;\n    &#125;\n\n    public int find(int x) &#123;\n        if (parent[x] !&#x3D; x) &#123;\n            parent[x] &#x3D; find(parent[x]);\n        &#125;\n        return parent[x];\n    &#125;\n\n    &#x2F;&#x2F; 返回图中的连通分量个数\n    public int count() &#123;\n        return count;\n    &#125;\n&#125;\n快速幂：为了在O(logn)的时间内计算a^n的技巧\n\n理论依据：a^(b+c) = a^b * a^c，与二分查找思想结合可以得出a^(2b) =a^b * a^b =  (a^b) ^2\n\n代码实现\n\n递归\nlong binpow(long a,long b)&#123;\n  if(b &#x3D;&#x3D; 0)&#123;\n    return 1;\n  &#125;\n  long res &#x3D; binpow(a, b&#x2F;2);\n  if(b % 2 &#x3D;&#x3D; 1)&#123;\n    return res * res * a; &#x2F;&#x2F;奇数次幂\n  &#125;else&#123;\n    return res * res; &#x2F;&#x2F;偶数次幂\n  &#125;\n&#125;\n非递归\nlong binpow(long a, long b)&#123;\n  long res &#x3D; 1;\n  while(b &gt; 0)&#123;\n    if((b &amp; 1) &#x3D;&#x3D; 1)&#123; &#x2F;&#x2F;当前位为1，则需要乘二进制幂，否则跳过此次\n      res &#x3D; res * a;\n    &#125;\n    a &#x3D; a*a;\n    b &gt;&gt;&#x3D; 1;\n  &#125;\n  return res;\n&#125;\n\n\n应用\n\n计算 (x^n) mod m：取模运算不会干涉乘法，所以计算过程中直接取模就行\n\n另：根据费马小定理，如果m是一个质数，可以计算x^(n mod (m-1) )来加速算法过程\n\nlong binpow(long a, long b, long m)&#123;\n  a %&#x3D; m;\n  long res &#x3D; 1;\n  while(b &gt; 0)&#123;\n    if((b &amp; 1) &#x3D;&#x3D; 1)&#123;\n      res &#x3D; res * a % m;\n    &#125;\n    a &#x3D; a * a % m;\n    b &gt;&gt;&#x3D; 1;\n  &#125;\n  return res;\n&#125;\n\n\n\n\n线段树\n\n目的：用来维护区间信息的数据结构，可以在O(logN)的时间复杂度内实现单点修改、区间修改、区间查询（区间求和、求区间最大值、求区间最小值）等操作\n\n基本结构\n&#x2F;&#x2F; 对区间[s,t]递归建树\n&#x2F;&#x2F; int[] d &#x3D; new int[n*4];\nvoid build(int s, int t, int p)&#123;\n  if(s &#x3D;&#x3D; t)&#123;\n    d[p] &#x3D; a[s];\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  build(s,m,p*2);\n  build(m+1,t,p*2+1);\n  &#x2F;&#x2F;从下向上递归建树\n  d[p] &#x3D; d[p*2] + d[p*2+1];\n&#125;\n区间查询\nint getSum(int l, int r, int s, int t, int p)&#123;\n  &#x2F;&#x2F;[l,r]为查询区间，[s,t]为当前节点包含的区间，p为当前节点的编号\n  if(l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r)&#123;\n    return d[p]; &#x2F;&#x2F;当前区间为查询区间的子集时直接返回当前节点的和\n  &#125;\n  int m &#x3D; s + ((t-s) &gt;&gt; 1);\n  int sum &#x3D; 0;\n  &#x2F;&#x2F;左儿子与查询区间有交集，递归查询左儿子\n  if(l &lt;&#x3D; m)&#123;\n    sum +&#x3D; getSum(l, r, s, m, p*2);\n  &#125;\n  &#x2F;&#x2F;右儿子与查询区间有交集，递归查询右儿子\n  if(r &gt; m)&#123;\n    sum +&#x3D; getSum(l, r, m+1, t, p*2+1);\n  &#125;\n  return sum;\n&#125;\n区间修改（存在标记的情况）\nvoid update(int l, int r, int c, int s, int t, int p)&#123;\n  &#x2F;&#x2F; [l, r] 为修改区间, c 为被修改的元素的变化量, [s, t] 为当前节点包含的区间, p为当前节点的编号\n  if(l &lt;&#x3D; s &amp;&amp;  t &lt;&#x3D; r)&#123;\n    d[p] +&#x3D; (t - s + 1) * c;\n    b[p] +&#x3D; c;\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (b[p] &amp;&amp; s !&#x3D; t) &#123;\n    &#x2F;&#x2F; 如果当前节点的懒标记非空,则更新当前节点两个子节点的值和懒标记值\n    d[p * 2] +&#x3D; b[p] * (m - s + 1);\n    d[p * 2 + 1] +&#x3D; b[p] * (t - m);\n    &#x2F;&#x2F; 将标记下传给子节点\n    b[p * 2] +&#x3D; b[p];\n    b[p * 2 + 1] +&#x3D; b[p];  \n    &#x2F;&#x2F; 清空当前节点的标记\n    b[p] &#x3D; 0;                                \n  &#125;\n  if (l &lt;&#x3D; m) &#123;\n    update(l, r, c, s, m, p * 2);\n  &#125;\n  if (r &gt; m) &#123;\n    update(l, r, c, m + 1, t, p * 2 + 1);\n  &#125;\n  d[p] &#x3D; d[p * 2] + d[p * 2 + 1];\n&#125;\n区间求和（存在标记的情况）\nint getsum(int l, int r, int s, int t, int p) &#123;\n  &#x2F;&#x2F; [l, r] 为查询区间, [s, t] 为当前节点包含的区间, p 为当前节点的编号\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) return d[p];\n  &#x2F;&#x2F; 当前区间为询问区间的子集时直接返回当前区间的和\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (b[p]) &#123;\n    &#x2F;&#x2F; 如果当前节点的懒标记非空,则更新当前节点两个子节点的值和懒标记值\n    d[p * 2] +&#x3D; b[p] * (m - s + 1);\n    d[p * 2 + 1] +&#x3D; b[p] * (t - m);\n    &#x2F;&#x2F; 将标记下传给子节点\n    b[p * 2] +&#x3D; b[p];\n    b[p * 2 + 1] +&#x3D; b[p];  \n    &#x2F;&#x2F; 清空当前节点的标记\n    b[p] &#x3D; 0;                                \n  &#125;\n  int sum &#x3D; 0;\n  if (l &lt;&#x3D; m) sum &#x3D; getsum(l, r, s, m, p * 2);\n  if (r &gt; m) sum +&#x3D; getsum(l, r, m + 1, t, p * 2 + 1);\n  return sum;\n&#125;\n区间修改为某一个值而不是加上某一个值\nvoid update(int l, int r, int c, int s, int t, int p) &#123;\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) &#123;\n    d[p] &#x3D; (t - s + 1) * c, b[p] &#x3D; c;\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  &#x2F;&#x2F; 额外数组储存是否修改值\n  if (v[p]) &#123;\n    d[p * 2] &#x3D; b[p] * (m - s + 1), d[p * 2 + 1] &#x3D; b[p] * (t - m);\n    b[p * 2] &#x3D; b[p * 2 + 1] &#x3D; b[p];\n    v[p * 2] &#x3D; v[p * 2 + 1] &#x3D; 1;\n    v[p] &#x3D; 0;\n  &#125;\n  if (l &lt;&#x3D; m) update(l, r, c, s, m, p * 2);\n  if (r &gt; m) update(l, r, c, m + 1, t, p * 2 + 1);\n  d[p] &#x3D; d[p * 2] + d[p * 2 + 1];\n&#125;\n\nint getsum(int l, int r, int s, int t, int p) &#123;\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) return d[p];\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (v[p]) &#123;\n    d[p * 2] &#x3D; b[p] * (m - s + 1), d[p * 2 + 1] &#x3D; b[p] * (t - m);\n    b[p * 2] &#x3D; b[p * 2 + 1] &#x3D; b[p];\n    v[p * 2] &#x3D; v[p * 2 + 1] &#x3D; 1;\n    v[p] &#x3D; 0;\n  &#125;\n  int sum &#x3D; 0;\n  if (l &lt;&#x3D; m) sum &#x3D; getsum(l, r, s, m, p * 2);\n  if (r &gt; m) sum +&#x3D; getsum(l, r, m + 1, t, p * 2 + 1);\n  return sum;\n&#125;\n\n\n\n2.3算法思想\n排序\n\n基础排序算法\n\nO（n^2）\n\n冒泡排序：一对对比较，一对对交换\n\n插入排序：分为已排和未排区间，取未排插入到已排。例：希尔排序\n\n选择排序：分为已排和未排区间，从未排选一个最小的插入到已排的\n\n希尔排序\n\n\n\nO（nlogn）\n\n归并排序：“分治思想”，分而治之，然后再合并\n\n快速排序：选一个pivot，大的放左，小的放右\n\n堆排序：先将数组原地建成一个堆，从下往上堆化，取堆顶元素，将下标n的元素放到堆顶，堆化\n\n二叉排序树排序\n\n\n\nO（n）\n\n计数排序：例：10G数据，100个桶\n\n基数排序：高考成绩排序，760个桶\n\n桶排序：10万个手机号码排序，从个位开始一位位进行桶或基数排序\n\n\n\n\n\n常见题型\n\n特殊排序：不是单纯的增减顺序，而是有一些特殊要求\nTop K：找到前K个大的，第K个大的……\n链表上的排序：数据结构由数组转换为链表，并进行排序\n排序预处理：排序只是问题的一部分预处理，可以运用库函数\n区间问题：（252题、56题） 先排序，再处理\n\n\n\n\n二分查找：大部分都是变形二分查找或二分答案，代码不长，但容易写对。难点在于：确定搜索区间，循环条件，区间更新，返回值\n\n查找区间永远是闭区间[low,high]\n\n循环条件永远是：low &lt;= high\n\n对于low == high的情况，必要的时候特殊处理，在while内部补充退出条件\n\n返回值永远是mid，而不是low，high\n\nlow、high的更新永远是low = mid + 1和high = mid - 1\n\n对于非确定性查找，使用前后探测法，来确定搜索区间（不用while，而只更新low或high）\n\n先处理命中情况，再处理在左右半部分查找的情况\n\n非确定查找：第一个、最后一个、第一个大于等于、最后一个小于等于、循环数组寻找最小值、寻找峰值\n\n\n\nbfs\n&#x2F;&#x2F; 计算从起点 start 到终点 target 的最近距离\nint BFS(Node start, Node target) &#123;\n    Queue&lt;Node&gt; q; &#x2F;&#x2F; 核心数据结构\n    Set&lt;Node&gt; visited; &#x2F;&#x2F; 避免走回头路\n    \n    q.offer(start); &#x2F;&#x2F; 将起点加入队列\n    visited.add(start);\n    int step &#x3D; 0; &#x2F;&#x2F; 记录扩散的步数\n\n    while (q not empty) &#123;\n        int sz &#x3D; q.size();\n        &#x2F;* 将当前队列中的所有节点向四周扩散 *&#x2F;\n        for (int i &#x3D; 0; i &lt; sz; i++) &#123;\n            Node cur &#x3D; q.poll();\n            &#x2F;* 划重点：这里判断是否到达终点 *&#x2F;\n            if (cur is target)\n                return step;\n            &#x2F;* 将 cur 的相邻节点加入队列 *&#x2F;\n            for (Node x : cur.adj()) &#123;\n                if (x not in visited) &#123;\n                    q.offer(x);\n                    visited.add(x);\n                &#125;\n            &#125;\n        &#125;\n        &#x2F;* 划重点：更新步数在这里 *&#x2F;\n        step++;\n    &#125;\n&#125;\ndfs\n\n递归\n\n代码技巧：千万不要试图想清楚整个递和归的执行过程，实际上是进入了一个思维误区\n\n怎么发现这个问题可以用递归来做：\n\n规模更小的问题，跟规模大点的问题，解决思路相同，但规模不同\n\n利用子问题的解可以组合得到原问题的解\n\n存在最小子问题，可以直接返回结果，即存在递归终止条件\n\n\n\n递归的正确编写姿势：\n\n我们可以假设子问题B,C已经解决，在此基础上思考如何解决原问题A，基于此，找递推公式+终止条件，然后翻译成代码\n\n\n\n\n时间复杂度和空间复杂度分析：\n\n时间复杂度：递推公式或者递归树\n空间复杂度：跟递归的函数调用栈最大深度成正比，即递归树的高度\n\n\n解题技巧：寻找重复结构，是否能将问题结构转化成结构相同，规模更小的子问题，然后写递推公式，包括递归终止条件，然后翻译成代码\n\n原问题解决思路和子问题解决思路是否一样\n\n子问题的解能否构造出原问题的解（递推公式）\n\n找到最小子问题（终止条件）\n\n\n\n\n\n回溯：回溯是递归的副产品，只要有递归就会有回溯，本质就是穷举+剪枝\nresult &#x3D; []\ndef backtrack(路径, 选择列表):\n    if 满足结束条件:\n        result.add(路径)\n        return\n    \n    for 选择 in 选择列表:\n        做选择\n        backtrack(路径, 选择列表)\n        撤销选择\ndfs\npublic List&lt;Integer&gt; dfs(int s,int t)&#123;\n        List&lt;Integer&gt; path &#x3D; new ArrayList&lt;&gt;();\n        path.add(s);\n        visited[s] &#x3D; true;\n        dfs_backtrack(s,t,path);\n        return resultPath;\n    &#125;\n\n    public void dfs_backtrack(int s,int t,List&lt;Integer&gt; path)&#123;\n        &#x2F;&#x2F;结束条件\n        if (s &#x3D;&#x3D; t)&#123;\n            resultPath &#x3D; new ArrayList&lt;&gt;(path);\n            return;\n        &#125;\n        for (int i &#x3D; 0; i &lt; adj[s].size(); i++) &#123;\n            int q &#x3D; adj[s].get(i);\n            if (!visited[q])&#123;\n                path.add(q);\n                visited[q] &#x3D; true;\n                dfs_backtrack(q,t,path);\n                path.remove(path.size()-1);\n            &#125;\n        &#125;\n    &#125;\n\n\ndp\n\n解题步骤\n\n可用回溯解决：使用穷举结果才能得到结果的问题（最值、可行、计数等）\n构建多阶段决策模型：看是否能将问题求解的过程分为多个阶段\n查看是否存在重复子问题：是否有多个路径到达同一状态\n定义状态：也就是如何记录每一阶段的不重复状态\n定义状态转移方程：也就是找到如何通过上一阶段的状态推导下一阶段的状态\n画状态转移表：辅助理解，验证正确性，确定状态转移的初始值\n\n\n代码结构\n# 自顶向下递归的动态规划\ndef dp(状态1, 状态2, ...):\n    for 选择 in 所有可能的选择:\n        # 此时的状态已经因为做了选择而改变\n        result &#x3D; 求最值(result, dp(状态1, 状态2, ...))\n    return result\n\n# 自底向上迭代的动态规划\n# 初始化 base case\ndp[0][0][...] &#x3D; base case\n# 进行状态转移\nfor 状态1 in 状态1的所有取值：\n    for 状态2 in 状态2的所有取值：\n        for ...\n            dp[状态1][状态2][...] &#x3D; 求最值(选择1，选择2...)\n0-1背包的最值、可行、计数\n\n最值1：有n个物品，选择其中一些物品装入背包，在不超过背包最大重量限制的前提下，背包中可装物品总重量的最大值是多少\n\n最值2：有n个物品，选择其中一些物品装入背包，正好装满背包所需物品最小个数（如果装不满，返回-1）\n\n可行：有n个物品，选择其中一些物品装入背包，能不能正好装满背包\n\n计数：有n个物品，选择其中一些物品装入背包，装满背包有多少种不同的装法\n\n\n\n完全背包（同一个物品可装n次）的最值、可行、计数\n\n背包可装物品总重量的最大值是多少\n是否能装满整个背包\n正好装满背包至少需要多少物品\n装满背包有多少种装法\n\n\n空间优化\n\n\n\n\n3.经典代码1.二叉树\n构建\n\n根据数组构建节点结构\npublic class Solution &#123;\n    static class TreeNode &#123;\n        int val;\n        TreeNode left;\n        TreeNode right;\n        public TreeNode(int x) &#123;\n            this.val &#x3D; x;\n            this.left &#x3D; null;\n            this.right &#x3D; null;\n        &#125;\n    &#125;\n    \n    &#x2F;**\n     * 根据数组构建二叉树\n     * @param arr 树的数组表示\n     * @return 构建成功后树的根节点\n     *&#x2F;\n    public TreeNode constructBinaryTree(final int[] arr) &#123;\n        &#x2F;&#x2F; 构建和原数组相同的树节点列表\n        List&lt;TreeNode&gt; treeNodeList &#x3D; arr.length &gt; 0 ? new ArrayList&lt;&gt;(arr.length) : null;\n        TreeNode root &#x3D; null;\n        &#x2F;&#x2F; 把输入数值数组，先转化为二叉树节点列表\n        for (int i &#x3D; 0; i &lt; arr.length; i++) &#123;\n            TreeNode node &#x3D; null;\n            if (arr[i] !&#x3D; -1) &#123; &#x2F;&#x2F; 用 -1 表示null\n                node &#x3D; new TreeNode(arr[i]);\n            &#125;\n            treeNodeList.add(node);\n            if (i &#x3D;&#x3D; 0) &#123;\n                root &#x3D; node;\n            &#125;\n        &#125;\n        &#x2F;&#x2F; 遍历一遍，根据规则左右孩子赋值就可以了\n        &#x2F;&#x2F; 注意这里 结束规则是 i * 2 + 1 &lt; arr.length，避免空指针\n        &#x2F;&#x2F; 为什么结束规则不能是i * 2 + 2 &lt; arr.length呢?\n        &#x2F;&#x2F; 如果i * 2 + 2 &lt; arr.length 是结束条件\n        &#x2F;&#x2F; 那么i * 2 + 1这个符合条件的节点就被忽略掉了\n        &#x2F;&#x2F; 例如[2,7,9,-1,1,9,6,-1,-1,10] 这样的一个二叉树,最后的10就会被忽略掉\n        for (int i &#x3D; 0; i * 2 + 1 &lt; arr.length; i++) &#123;\n            TreeNode node &#x3D; treeNodeList.get(i);\n            if (node !&#x3D; null) &#123;\n                &#x2F;&#x2F; 线性存储转连式存储关键逻辑\n                node.left &#x3D; treeNodeList.get(2 * i + 1);\n                &#x2F;&#x2F;  再次判断下 不忽略任何一个节点\n                if(i * 2 + 2 &lt; arr.length)\n                node.right &#x3D; treeNodeList.get(2 * i + 2);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n&#125;\n直接构建邻接表\n\nArrayList&lt;Integer&gt;[] adjs &#x3D; new ArrayList[n];\nfor(int i &#x3D; 0; adjs.size(); i++)&#123;\n  adjs[i] &#x3D; new ArrayList&lt;&gt;();\n&#125;\nfor(int i &#x3D; 2; i &lt;&#x3D; n; i++)&#123;\n  adjs[father].add(son);\n&#125;\n图的构建\n&#x2F;&#x2F; 邻接表\n&#x2F;&#x2F; graph[x] 存储 x 的所有邻居节点\nList&lt;Integer&gt;[] graph;\n\n&#x2F;&#x2F; 邻接矩阵\n&#x2F;&#x2F; matrix[x][y] 记录 x 是否有一条指向 y 的边\nboolean[][] matrix;\n\n\n递归遍历\n&#x2F;&#x2F; 前序遍历·递归·LC144_二叉树的前序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;Integer&gt;();\n        preorder(root, result);\n        return result;\n    &#125;\n\n    public void preorder(TreeNode root, List&lt;Integer&gt; result) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        result.add(root.val);\n        preorder(root.left, result);\n        preorder(root.right, result);\n    &#125;\n&#125;\n&#x2F;&#x2F; 中序遍历·递归·LC94_二叉树的中序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; res &#x3D; new ArrayList&lt;&gt;();\n        inorder(root, res);\n        return res;\n    &#125;\n\n    void inorder(TreeNode root, List&lt;Integer&gt; list) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        inorder(root.left, list);\n        list.add(root.val);             &#x2F;&#x2F; 注意这一句\n        inorder(root.right, list);\n    &#125;\n&#125;\n&#x2F;&#x2F; 后序遍历·递归·LC145_二叉树的后序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; res &#x3D; new ArrayList&lt;&gt;();\n        postorder(root, res);\n        return res;\n    &#125;\n\n    void postorder(TreeNode root, List&lt;Integer&gt; list) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        postorder(root.left, list);\n        postorder(root.right, list);\n        list.add(root.val);             &#x2F;&#x2F; 注意这一句\n    &#125;\n&#125;\n非递归遍历\n&#x2F;&#x2F; 前序遍历顺序：中-左-右，入栈顺序：中-右-左\nclass Solution &#123;\n    public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        stack.push(root);\n        while (!stack.isEmpty())&#123;\n            TreeNode node &#x3D; stack.pop();\n            result.add(node.val);\n            if (node.right !&#x3D; null)&#123;\n                stack.push(node.right);\n            &#125;\n            if (node.left !&#x3D; null)&#123;\n                stack.push(node.left);\n            &#125;\n        &#125;\n        return result;\n    &#125;\n&#125;\n\n&#x2F;&#x2F; 中序遍历顺序: 左-中-右 入栈顺序： 左-右\nclass Solution &#123;\n    public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        TreeNode cur &#x3D; root;\n        while (cur !&#x3D; null || !stack.isEmpty())&#123;\n           if (cur !&#x3D; null)&#123;\n               stack.push(cur);\n               cur &#x3D; cur.left;\n           &#125;else&#123;\n               cur &#x3D; stack.pop();\n               result.add(cur.val);\n               cur &#x3D; cur.right;\n           &#125;\n        &#125;\n        return result;\n    &#125;\n&#125;\n\n&#x2F;&#x2F; 后序遍历顺序 左-右-中 入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果\nclass Solution &#123;\n    public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        stack.push(root);\n        while (!stack.isEmpty())&#123;\n            TreeNode node &#x3D; stack.pop();\n            result.add(node.val);\n            if (node.left !&#x3D; null)&#123;\n                stack.push(node.left);\n            &#125;\n            if (node.right !&#x3D; null)&#123;\n                stack.push(node.right);\n            &#125;\n        &#125;\n        Collections.reverse(result);\n        return result;\n    &#125;\n&#125;\n层序遍历\n&#x2F;&#x2F; 102.二叉树的层序遍历\nclass Solution &#123;\n    public List&lt;List&lt;Integer&gt;&gt; resList &#x3D; new ArrayList&lt;List&lt;Integer&gt;&gt;();\n\n    public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;\n        &#x2F;&#x2F;checkFun01(root,0);\n        checkFun02(root);\n\n        return resList;\n    &#125;\n\n    &#x2F;&#x2F;DFS--递归方式\n    public void checkFun01(TreeNode node, Integer deep) &#123;\n        if (node &#x3D;&#x3D; null) return;\n        deep++;\n\n        if (resList.size() &lt; deep) &#123;\n            &#x2F;&#x2F;当层级增加时，list的Item也增加，利用list的索引值进行层级界定\n            List&lt;Integer&gt; item &#x3D; new ArrayList&lt;Integer&gt;();\n            resList.add(item);\n        &#125;\n        resList.get(deep - 1).add(node.val);\n\n        checkFun01(node.left, deep);\n        checkFun01(node.right, deep);\n    &#125;\n\n    &#x2F;&#x2F;BFS--迭代方式--借助队列\n    public void checkFun02(TreeNode node) &#123;\n        if (node &#x3D;&#x3D; null) return;\n        Queue&lt;TreeNode&gt; que &#x3D; new LinkedList&lt;TreeNode&gt;();\n        que.offer(node);\n\n        while (!que.isEmpty()) &#123;\n            List&lt;Integer&gt; itemList &#x3D; new ArrayList&lt;Integer&gt;();\n            int len &#x3D; que.size();\n\n            while (len &gt; 0) &#123;\n                TreeNode tmpNode &#x3D; que.poll();\n                itemList.add(tmpNode.val);\n\n                if (tmpNode.left !&#x3D; null) que.offer(tmpNode.left);\n                if (tmpNode.right !&#x3D; null) que.offer(tmpNode.right);\n                len--;\n            &#125;\n\n            resList.add(itemList);\n        &#125;\n\n    &#125;\n&#125;\n翻转二叉树\n&#x2F;&#x2F;DFS递归\nclass Solution &#123;\n   &#x2F;**\n     * 前后序遍历都可以\n     * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换）\n     *&#x2F;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return null;\n        &#125;\n        invertTree(root.left);\n        invertTree(root.right);\n        swapChildren(root);\n        return root;\n    &#125;\n\n    private void swapChildren(TreeNode root) &#123;\n        TreeNode tmp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; tmp;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;BFS\nclass Solution &#123;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;return null;&#125;\n        ArrayDeque&lt;TreeNode&gt; deque &#x3D; new ArrayDeque&lt;&gt;();\n        deque.offer(root);\n        while (!deque.isEmpty()) &#123;\n            int size &#x3D; deque.size();\n            while (size-- &gt; 0) &#123;\n                TreeNode node &#x3D; deque.poll();\n                swap(node);\n                if (node.left !&#x3D; null) deque.offer(node.left);\n                if (node.right !&#x3D; null) deque.offer(node.right);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n\n    public void swap(TreeNode root) &#123;\n        TreeNode temp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; temp;\n    &#125;\n&#125;\n二叉树的所有路径\n&#x2F;&#x2F;DFS递归\nclass Solution &#123;\n   &#x2F;**\n     * 前后序遍历都可以\n     * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换）\n     *&#x2F;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return null;\n        &#125;\n        invertTree(root.left);\n        invertTree(root.right);\n        swapChildren(root);\n        return root;\n    &#125;\n\n    private void swapChildren(TreeNode root) &#123;\n        TreeNode tmp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; tmp;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;BFS\nclass Solution &#123;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;return null;&#125;\n        ArrayDeque&lt;TreeNode&gt; deque &#x3D; new ArrayDeque&lt;&gt;();\n        deque.offer(root);\n        while (!deque.isEmpty()) &#123;\n            int size &#x3D; deque.size();\n            while (size-- &gt; 0) &#123;\n                TreeNode node &#x3D; deque.poll();\n                swap(node);\n                if (node.left !&#x3D; null) deque.offer(node.left);\n                if (node.right !&#x3D; null) deque.offer(node.right);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n\n    public void swap(TreeNode root) &#123;\n        TreeNode temp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; temp;\n    &#125;\n&#125;\n前序和后序构造二叉树\nclass Solution &#123;\n    Map&lt;Integer, Integer&gt; map;  &#x2F;&#x2F; 方便根据数值查找位置\n    public TreeNode buildTree(int[] inorder, int[] postorder) &#123;\n        map &#x3D; new HashMap&lt;&gt;();\n        for (int i &#x3D; 0; i &lt; inorder.length; i++) &#123; &#x2F;&#x2F; 用map保存中序序列的数值对应位置\n            map.put(inorder[i], i);\n        &#125;\n\n        return findNode(inorder,  0, inorder.length, postorder,0, postorder.length);  &#x2F;&#x2F; 前闭后开\n    &#125;\n\n    public TreeNode findNode(int[] inorder, int inBegin, int inEnd, int[] postorder, int postBegin, int postEnd) &#123;\n        &#x2F;&#x2F; 参数里的范围都是前闭后开\n        if (inBegin &gt;&#x3D; inEnd || postBegin &gt;&#x3D; postEnd) &#123;  &#x2F;&#x2F; 不满足左闭右开，说明没有元素，返回空树\n            return null;\n        &#125;\n        int rootIndex &#x3D; map.get(postorder[postEnd - 1]);  &#x2F;&#x2F; 找到后序遍历的最后一个元素在中序遍历中的位置\n        TreeNode root &#x3D; new TreeNode(inorder[rootIndex]);  &#x2F;&#x2F; 构造结点\n        int lenOfLeft &#x3D; rootIndex - inBegin;  &#x2F;&#x2F; 保存中序左子树个数，用来确定后序数列的个数\n        root.left &#x3D; findNode(inorder, inBegin, rootIndex,\n                            postorder, postBegin, postBegin + lenOfLeft);\n        root.right &#x3D; findNode(inorder, rootIndex + 1, inEnd,\n                            postorder, postBegin + lenOfLeft, postEnd - 1);\n\n        return root;\n    &#125;\n&#125;\n前序和中序构造二叉树\nclass Solution &#123;\n    Map&lt;Integer, Integer&gt; map;\n    public TreeNode buildTree(int[] preorder, int[] inorder) &#123;\n        map &#x3D; new HashMap&lt;&gt;();\n        for (int i &#x3D; 0; i &lt; inorder.length; i++) &#123; &#x2F;&#x2F; 用map保存中序序列的数值对应位置\n            map.put(inorder[i], i);\n        &#125;\n\n        return findNode(preorder, 0, preorder.length, inorder,  0, inorder.length);  &#x2F;&#x2F; 前闭后开\n    &#125;\n\n    public TreeNode findNode(int[] preorder, int preBegin, int preEnd, int[] inorder, int inBegin, int inEnd) &#123;\n        &#x2F;&#x2F; 参数里的范围都是前闭后开\n        if (preBegin &gt;&#x3D; preEnd || inBegin &gt;&#x3D; inEnd) &#123;  &#x2F;&#x2F; 不满足左闭右开，说明没有元素，返回空树\n            return null;\n        &#125;\n        int rootIndex &#x3D; map.get(preorder[preBegin]);  &#x2F;&#x2F; 找到前序遍历的第一个元素在中序遍历中的位置\n        TreeNode root &#x3D; new TreeNode(inorder[rootIndex]);  &#x2F;&#x2F; 构造结点\n        int lenOfLeft &#x3D; rootIndex - inBegin;  &#x2F;&#x2F; 保存中序左子树个数，用来确定前序数列的个数\n        root.left &#x3D; findNode(preorder, preBegin + 1, preBegin + lenOfLeft + 1,\n                            inorder, inBegin, rootIndex);\n        root.right &#x3D; findNode(preorder, preBegin + lenOfLeft + 1, preEnd,\n                            inorder, rootIndex + 1, inEnd);\n\n        return root;\n    &#125;\n&#125;\n\n2.动态规划\n背包问题：0-1、完全、多重、二维费用、分组、有依赖的\n路径问题\n打家劫舍和股票买卖\n一般动态规划问题，上一个阶段做了什么决策，不影响下一个阶段的决策。但是打家劫舍&amp;股票买卖这类问题，上一个阶段的决策会影响下一个阶段的决策，所以，每个阶段需要记录不同的决策对应的最值，而不是一个全局的最值\n\n\n爬楼梯\n匹配问题\n\n3.其它\n@SuppressWarnings(&quot;unchecked&quot;)\n\nSometimes Java generics just doesn’t let you do what you want to, and you need to effectively tell the compiler that what you’re doing really will be legal at execution time.\n\n可选的值\n\n\n\nAll\nIt will suppress all warnings.\n解释\n\n\n\nCast\nSuppress the warning while casting from a generic type to a nonqualified type or the other way around.\n\n\n\nDeprecation\nIgnores when we’re using a deprecated(no longer important) method or type.\n使用了不赞成使用的类或方法时的警告\n\n\ndivzero\nSuppresses division by zero warning.\n\n\n\nempty\nIgnores warning of a statement with an empty body.\n\n\n\nunchecked\nIt doesn’t check if the data type is Object or primitive.\n例如使用集合时没有用泛型来指定集合保存的类型\n\n\nfallthrough\nIgnores fall-through on switch statements usually (if “break” is missing).\n当switch程序块直接通往下一种情况而没有break时的警告\n\n\nhiding\nIt suppresses warnings relative to locals that hide variable\n\n\n\nserial\nIt makes the compiler shut up about a missing serialVersionUID.\n在可序列化的类上缺少serialVersionUID定义时的警告\n\n\nfinally\nAvoids warnings relative to finally block that doesn’t return.\n任何 finally 子句不能正常完成时的警告\n\n\nunused\nTo suppress warnings relative to unused code.\n\n\n\n\n\n\nRuntime Error Hangup通常是因为程序在运行时被强制终止或意外终止导致的错误。这个错误通常出现在操作系统或程序遇到了无法处理的异常情况时。一些可能导致Runtime Error 0Hangup错误的原因包括：\n\n内存不足或堆栈溢出；\n访问无效的内存地址；\n文件操作失败或无效的文件指针；\n操作系统或其他软件的错误或冲突；\n程序代码错误或逻辑错误；\n程序被用户手动终止。\n\n\n笔试系统：输入输出学习链接（https://ac.nowcoder.com/acm/contest/5657#question）\n\n示例一：\n&#x2F;&#x2F; 有些输入可能是：\n&#x2F;&#x2F; 输入一个矩阵，每行以空格分隔。\n&#x2F;&#x2F; 3 2 3\n&#x2F;&#x2F; 1 6 5\n&#x2F;&#x2F; 7 8 9\nimport java.io.*;\nimport java.util.*;\n\nclass Solution &#123;\n  public void myFunc(ArrayList&lt;ArrayList&lt;Integer&gt;&gt; arr) &#123;\n    &#x2F;&#x2F; 使用自测数据按钮时调试用，正式提交时要删掉。\n    System.out.println(arr);\n  &#125;\n&#125;\npublic class Main\n&#123;\n  public static void main(String args[])\n  &#123;\n    Scanner cin &#x3D; new Scanner(System.in);\n    ArrayList&lt;ArrayList&lt;Integer&gt;&gt; arr &#x3D; new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;();\n    while(cin.hasNextLine())\n    &#123;\n      ArrayList&lt;Integer&gt; row &#x3D; new ArrayList&lt;Integer&gt;();\n      String line &#x3D; cin.nextLine();\n      if (line.length() &gt; 0) &#123;\n        String[] arrLine &#x3D; line.split(&quot; &quot;);\n        for (int i&#x3D;0; i&lt;arrLine.length; i++) &#123;\n          row.add(Integer.parseInt(arrLine[i]));\n        &#125;\n        arr.add(row);\n      &#125;\n    &#125;\n        \n    new Solution().myFunc(arr);\n  &#125;\n&#125;\n示例二：\n&#x2F;&#x2F;package main\n&#x2F;&#x2F;注意不要添加包名称，否则会报错。\n&#x2F;&#x2F; 不要自定义包名称，否则会报错，即不要添加package answer之类的语句；\n&#x2F;&#x2F; 您可以写很多个类，但是必须有一个类名为Main，并且为public属性，并且Main为唯一的public class；\n&#x2F;&#x2F; Main类的里面必须包含一个名字为&#39;main&#39;的静态方法（函数），这个方法是程序的入口。\n\nimport java.io.*;\nimport java.util.*;\nclass Solution &#123;\n  public int addab(int a, int b) &#123;\n    return a+b;\n  &#125;\n&#125;\npublic class Main\n&#123;\n  public static void main(String args[])\n  &#123;\n    Scanner cin &#x3D; new Scanner(System.in);\n    int a, b;\n    while(cin.hasNextInt())\n    &#123;\n      a &#x3D; cin.nextInt();\n      b &#x3D; cin.nextInt();\n      Solution s &#x3D; new Solution();\n      int c &#x3D; s.addab(a, b);\n      System.out.println(c);\n    &#125;\n  &#125;\n&#125;\n示例三：从在键盘上按Ctrl+Z。这样输入会读取到EOF，表示读取结束。\nwhile (sc.hasNextLine())&#123;\n\t\tScanner sc &#x3D; new Scanner(System.in);\n    String temp &#x3D; sc.nextLine();\n    String[] ss &#x3D; temp.trim().split(&quot; &quot;);\n    int num1 &#x3D; Integer.parseInt(ss[0]);\n    int num2 &#x3D; Integer.parseInt(ss[1]);\n    if (temp.isEmpty())&#123;\n        break;\n    &#125;\n    System.out.println(temp);\n&#125;\n\npublic class Main&#123;\n    public static void main(String[] args)&#123;\n        Scanner sc &#x3D; new Scanner(System.in);\n        int n &#x3D; sc.nextInt();\n        &#x2F;&#x2F;nextInt不会吸收掉换行符，后的nextLine会直接读取换行符，然后结束输入\n        sc.nextLine();\n        String temp &#x3D; sc.nextLine();\n        String[] data;\n        data &#x3D; temp.trim().split(&quot; &quot;);\n        Arrays.sort(data);\n        for(int i &#x3D; 0; i &lt; n; i++)&#123;\n            System.out.print(data[i]);\n            if(i !&#x3D; n-1)&#123;\n                System.out.print(&quot; &quot;);\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n\n","slug":"算法基础","date":"2023-04-01T04:25:03.000Z","categories_index":"","tags_index":"algorithm","author_index":"Dajunnnnnn"}]