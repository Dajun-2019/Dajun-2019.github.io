[{"id":"f00c07cb09e8c5144602456ae7f0f75a","title":"Internet","content":"Internet1.网络层\nIP\n\n查看IP地址：ifconfig、ip addr\n\nscope：如果是global，则此张网卡是可以对外开放的，可以接受各个地方的包；对于lo来说事host，说明这张网卡仅仅可以供本机相互通信\nlo全称是loopback，又称环回接口，往往会被分配到127.0.0.1这个地址，用于本机通信，经过内核处理后直接返回，不会再任何网络中出现。\nMAC地址：在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为 MAC 地址，是一个网卡的物理地址，用十六进制，6 个 byte 表示。因为MAC没有定位功能，所以需要IP地址来寻路\n网络设备的状态标识（net_device flags）：例如&lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; ，UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST 表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。MTU1500 是指最大传输单元 MTU 为 1500，这是以太网的默认值。\n排队规则（qdisc pfifo_fast）：qdisc 全称是 queueing discipline，中文叫排队规则。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。\n\n\nIP地址分类方式（ 无类型域间选路（CIDR）：10.100.122.2/24 ，网络号+主机号组成，通过与掩码进行与运算来求值）\n\nA类地址：1.0.0.0 到126.0.0.0（私有地址10.0.0.0～10.255.255.255）\nB类地址：128.0.0.0到191.255.255.255（私有地址172.16.0.0～172.31.255.255）\nC类地址：192.0.0.0到223.255.255.255（私有地址192.168.0.0～192.168.255.255）\nD类组播地址用于VXLAN协议，E类留待后用\n0.0.0.0对应于当前主机，255.255.255.255对应于当前子网的广播地址，127.0.0.1用于环回测试（loopback test）本主机\n\n\n数据报格式\n\n\n版本 : 有 4（IPv4）和 6（IPv6）两个值；\n首部长度 : 占 4 位，因此最大值为 15。值为 1 表示的是 1 个 32 位字的长度，也就是 4 字节。因为固定部分长度为 20 字节，因此该值最小为 5。如果可选字段的长度不是 4 字节的整数倍，就用尾部的填充部分来填充。\n区分服务 : 是一个框架和一组标准，用于支持RFC2474,RFC2475,RFC3260上不同类型的服务（即不只是尽力而为的服务，而是更好的服务）\n总长度 : 包括首部长度和数据部分长度，以字节为单位（最大为MTU一般为1500字节）\n生存时间 ：TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL 为 0 时就丢弃数据报。\n协议 ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP（6）、UDP（17） 等。\n首部检验和 ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。\n标识 : 避免数据报分片的混淆，在数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符。\n片偏移 : 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。\n选项字段：很多选项已经被淘汰，只有一部分被留下来放在了IPv6的扩展头部中\n\n\n\n\n路由协议：ODPF、RIP\n\n配置路由\n\n路由表：决定如何转发流量，通常称为路由表，主要包含以下三项信息，目的网络、出口设备、下一跳网关。可以通过route命令和ip route命令进行查询或者配置。例如，我们设置ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去 10.176.48.0/20 这个目标网络，要从 eth0 端口出去，经过 10.173.32.1。\n\n策略路由配置示例：\n$ ip rule add from 192.168.1.0&#x2F;24 table 10\n$ ip rule add from 192.168.2.0&#x2F;24 table 20\n#表示从 192.168.1.10&#x2F;24 这个网段来的，使用 table 10 中的路由表，\n#而从 192.168.2.0&#x2F;24 网段来的，使用 table20 的路由表\n$ ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2\n#下一跳有两个地方，分别是 100.100.100.1 和 200.200.200.1，权重分别为 1 比 2。\n\n\n动态路由算法：如何在网络拓扑中找到两个节点的最短路径，主要有Bellman-Ford和Dijkstra算法\n\n距离矢量路由算法（distance vector routing）：基于Bellman-Ford算法，算法思想是每个路由器都保存一个路由表，从哪出和距离，每个路由器都保存全局信息，每过一段时间将已知信息告知邻居。存在两个问题\n\n好消息（新加入路由器）传的块，坏消息（下线的路由器）传的慢\n\n每次发送的时候，要发送整个全局路由表\n\n\n\n链路状态路由算法（link state routing），基于Dijkstra算法，算法思想是当一个路由器启动的时候，首先是发现邻居，向邻居 say hello，邻居都回复。然后计算和邻居的距离，发送一个 echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用 Dijkstra 算法，找到两点之间的最短路径\n\n\n\n动态路由协议\n\n基于链路状态路由算法的OSPF（Open Shortest Path First，开放式最短路径优先）：主要用于数据中心内部，又称内部网关协议（Interior Gateway Protocol IGP）。内部网关协议的重点就是找到最短路径，可以在多个路径中进行负载均衡，常被称为等价路由。常配合接入层的负载均衡LVS\n\n基于距离矢量路由算法的BGP（Border Gateway Protocol，外网路由协议）：因为每个数据中心都有自己的Policy，所以有些路可以走，有些不可以走。这一个个数据中心称为自治系统AS（Autonomous System），通过边界路由器与外面世界建立联系。BGP有两类：\n\neBGP：边界路由器之间使用eBGP广播路由\n\niBGP：使得内部路由器能够找到到达外网目的地的最好的边界路由器\n\n\n\n\n\n\n\n数据链路层：ARP（将IP解析为MAC）\n\n\n\n\n\n\n\n\n\n如果说互联网中每一个资源都有IP地址唯一标识，那么一些网络设备都有MAC地址唯一标识，MAC类似于身份证号，IP地址类似于住址\n\nMAC地址：6字节，地址空间约280万亿，由IEEE统一管理与分配，FF-FF-FF-FF-FF-FF为广播地址\n\n\n类型字段用于确定协议类型：\nIPv4为0x0800\nIPv6为0x86DD\n\n\nFCS为帧校验序列：为待检查的消息追加n为0，除以一个n+1位的生成多项式，将余数取反放到FCS中\n链路层MTU为1500字节\n\n\nARP协议原理：ARP表，广播问讯，单播响应\n\n\n\n\n\n\n\n\n\n全称为地址解析协议，它解决的是网络层地址和链路层地址之间的转换问题，因为一个IP数据报在物理上传输的过程中，总需要知道下一跳（物理上的下一目的地）该去往何处，但IP地址属于逻辑地址，而MAC地址才是物理地址，ARP协议解决了IP地址转MAC地址的一些问题\n\nARP表：在一个局域网内，每个网络设备都自己维护了一个ARP表，ARP表记录了某些网络设备的IP地址和MAC地址的映射关系（&lt;IP, MAC, TTL&gt;），其中TTL为该映射关系的生存周期（通常为20min），超时即丢弃此条目\n\n同一个局域网内的MAC寻址（A「137.196.7.23」，B「137.196.7.14」）\n\n\nA检索自己的ARP表，发现ARP表中无B的IP地址对应的条目，无法知道B的MAC地址\nA构造一个ARP查询分组，将其广播到所在的局域网中\nARP分组主要有两种，ARP查询分组和ARP响应分组，他们具有相同的格式，均包含了发送和接收的IP地址，发送和接收的MAC地址\n查询分组发送的IP地址为A的IP地址，接收的IP地址为B的IP地址；发送的MAC地址为A的MAC地址，接收的MAC地址为FF-FF-FF-FF-FF-FF\n响应分组IP地址与查询分组相反，MAC地址也相反，但是目的MAC地址为查询分组的发送者的MAC，源MAC地址为B的MAC地址\n\n\n\n\n主机A构造的查询分组将在局域网内广播，每一个设备都会收到该分组，设备通过核查IP地址是否与本地相同来确定是否响应\n主机B收到了查询分组，构造一个ARP响应分组，该分组只有一个目的地（主机A），主机B提取IP和MAC信息，插入到ARP表中\n主机A收到主机B的响应分组，提取出该分组的IP地址和MAC地址，插入到ARP表中\n\n\n从一个局域网到另一个局域网中的网络设备的寻址\n\n\n\n\n\n\n\n\n\n路由器的每一个接口都各自维护一个ARP表\n\n主机A查询ARP表，期望寻找到目标路由器（根据B的IP分析出B的子网，转发报文到该子网）的本子网接口的MAC地址\n主机A未能找到目标路由器的本子网接口的MAC地址，采用ARP协议，问询到该 MAC 地址，由于目标接口与主机 A 在同一个子网内，该过程与同一局域网内的 MAC 寻址相同\n主机 A 获取到目标接口的 MAC 地址，先构造 IP 数据报，其中源 IP 是 A 的 IP 地址，目的 IP 地址是 B 的 IP 地址，再构造链路层帧，其中源 MAC 地址是 A 的 MAC 地址，目的 MAC 地址是本子网内与路由器连接的接口的 MAC 地址。主机 A 将把这个链路层帧，以单播的方式，发送给目标接口\n目标接口接收到了链路层帧，解析，根据目的IP地址，查询转发表，将该IP数据报转发到B所在的子网相连的接口上\n路由接口查询ARP表，寻找主机B的MAC地址，如果没找到，则采用ARP查询分组，广播问询，单播响应，获取到主机B的MAC地址，路由器接口对IP数据报重新封装成链路层帧，目标MAC地址为主机B的MAC地址，单播发送，直到目的地\n\n\n\n\n\n\n应用层：DNS（由域名查IP，负载均衡技术，DNS解析过程如下）\n\n首先查缓存\n浏览器缓存检查：浏览器首先搜索自身的DNS缓存（大概1000条左右），看自身缓存是否命中，或已过期\n操作系统缓存检查 + hosts 解析：查询操作系统的DNS缓存是否命中，可以通过/etc/hosts文件来设置，将任何域名解析到任何能够访问的IP地址\n\n\n然后进行dns解析\n第一步：发送域名到本地DNS服务器，查询其缓存，如果有则返回，如果没有则本地DNS服务器发送请求给根DNS服务器进行查询\n第二步：根域名服务器没有记录对应关系时，返回顶级域名服务器的IP；顶级域名服务器向根DNS服务器返回二级域名服务器的IP；二级域名服务器将自己缓存表中的域名和IP地址的对应关系返回给本地DNS服务器\n第三步：本地 DNS 服务器将获取到与域名对应的 IP 地址返回给客户端，并且将域名和 IP 地址的对应关系保存在缓存中，以备下次别的用户查询时使用\n\n\n\n\n应用层：DHCP（已有MAC，获取IP）\n\n格式\n\n\n原理\n\n手动配置IP相当于自己买房装修，DHCP协议相当于租房\n\n新到一个城市要先找中介（DHCP服务器）租个房（IP）-Discover\n\n\n中介（DHCP）会带你看房，可能会有多个中介带你看多个房-Offer\n\n\n你看上其中一个房，愿意租下来-Request\n\n\n中介说没问题，咱们签合同，租约达成-Ack\n\n\n租约达成要广播，防止中介之间跳单\n\n租约快到之前，记得续租\n\n\n\n\n\nNAT：内网复用IP，NAT网关将私有IP改为公网IP\n\n允许多个范围内中的同一地址可以复用。专用网内部的主机使用本地 IP 地址又想和互联网上的主机通信时，可以使用 NAT 来将本地 IP 转换为全球 IP，内部原理就是重写通过路由器的数据包的识别信息\n一共有三个IPv4地址范围作为私有地址范围使用（常作为DHCP地址池）：10.0.0.0/8、172.16.0.0/12、192.168.0.0/16\n在以前，NAT 将本地 IP 和全球 IP 一一对应，这种方式下拥有 n 个全球 IP 地址的专用网内最多只可以同时有 n 台主机接入互联网。为了更有效地利用全球 IP 地址，现在常用的 NAT 转换表把传输层的端口号也用上了，使得多个专用网内部的主机共用一个全球 IP 地址。使用端口号的 NAT 也叫做网络地址与端口转换 NAPT。\n\n\nICMP（ping的底层实现）\n\n帧格式\n\n查询报文类型：一种主动请求并且获得主动应答的ICMP协议，在网络抓包时，称为ICMP ECHO REQUEST和CMP ECHO REPLY，比原生ICMP多个标识符和序号字段\n\n差错报文类型：主要有终点不可达（3）、源抑制（4）、超时（11）、重定向（5）等\n\n\n\n\nping原理\n\n\n\n\n\n2.传输层\nTCP\n\n\n\n\n\n\n\n\n\nTCP的连接：用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接\n\n头格式\n\n序号：给包编号，用来解决乱序问题（将应用层的数据分块「TCP segment」，最大为MSS、MTU除去IP头和TCP头）\n确认序号：发出去的包应该有确认，来确定是否已经收到相应包\n状态位：\nSYN：发起一个连接\nACK：回复\nRST：重新连接\nFIN：结束连接\n\n\n窗口大小：TCP要做流浪控制，通信双方动态约定一个窗口，保证双方都在高效处理数据\n\n\n\n三次握手：保证双方都有发送和接收的能力\n\n握手次数：一次的话，请求方不知道响应方是否已经成功接受请求；两次的话，接收方不知道发送方是否成功接受相应；三次的话，刚好双方都知晓；四次以及更多的话，已经没有特殊的意义了，所以再多的数据包都不能保证真的可靠，后续通过数据包和探活包来解决相关问题\n\n防止旧的重复连接初始化造成混乱：旧连接的同步客户端会返回RST报文来解决\n同步双方初始序列号：同步双方的序列号\n避免资源浪费\n\n\n\nTCP包的序号问题：为了防止连接之间数据包序号的相互影响，所以序号是随时间变化的，通过数据包的序号，双方都可以知道应收的包和应发的包都是哪个\n\nSocket编程\n\n连接建立过程\n\n\n没有 accept，能建立 TCP 连接吗：accpet 系统调用并不参与 TCP 三次握手过程，它只是负责从 TCP 全连接队列取出一个已经建立连接的 socket，用户层通过 accpet 系统调用拿到了已经建立连接的 socket，就可以对该 socket 进行读写操作了\n\n\n\n\n\n\n四次挥手\n\n等待2MSL：MSL是报文最大生存时间，虽然序号是重新生成的，但是为了防止左端提前结束后收到右端之前连接的数据包，所以需要等待2MSL时间。时间截止后，对于右端旧连接发送的数据包，左端将会直接发送RST，这样右端就知道左端已退出\n\n防止历史连接中的数据，被后面相同四元组的连接错误的接收\n确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭\n\n\n\nTIME_WAIT 状态是主动关闭连接方才会出现的状态\n\n服务器出现大量 TIME_WAIT 状态的原因：服务端主动断开大量TCP连接，有可能是HTTP 没有使用长连接、HTTP 长连接超时（nginx 提供的 keepalive_timeout）、HTTP 长连接的请求数量达到上限（nginx 的 keepalive_requests）\n\n\nCLOSE_WAIT 状态是「被动关闭方」才会有的状态，当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序没有调用 close 函数关闭连接，通过如下TCP服务流程查看具体原因\n\n创建服务端 socket，bind 绑定端口、listen 监听端口\n\n将服务端 socket 注册到 epoll\n\n没有将服务端 socket 注册到 epoll，这样有新连接到来时，服务端没办法感知这个事件，也就无法获取到已连接的 socket，那服务端自然就没机会对 socket 调用 close 函数了\n\n\nepoll_wait 等待连接到来，连接到来时，调用 accpet 获取已连接的 socket\n\n没有将服务端 socket 注册到 epoll，这样有新连接到来时，服务端没办法感知这个事件，也就无法获取到已连接的 socket，那服务端自然就没机会对 socket 调用 close 函数了\n\n\n将已连接的 socket 注册到 epoll\n\n通过 accpet 获取已连接的 socket 后，没有将其注册到 epoll，导致后续收到 FIN 报文的时候，服务端没办法感知这个事件，那服务端就没机会调用 close 函数了\n\n\nepoll_wait 等待事件发生\n\n对方连接关闭时，我方调用 close\n\n当发现客户端关闭连接后，服务端没有执行 close 函数，可能是因为代码漏处理，或者是在执行 close 函数之前，代码卡在某一个逻辑，比如发生死锁等等\n\n\n\n\n\n\n\n滑动窗口\n\n发送端\n\n第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的\n第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉\n第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的\n第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。（区分三和四是为了流量控制）\n\n\n\n接收端\n\n第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的\n第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量\n第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。\n\n\n\n问题\n\n流量控制：在对包的确认中，同时会携带一个窗口的大小。当发送方窗口已经全发送过了，但是接受端还没接收，此时每接受到一个应答，窗口大小就减一，直到为0\n丢包问题\n超时重传：对每一个发送了，但是没有 ACK 的包，都有设一个定时器，超过了一定的时间，就重新尝试。超时时间依靠自适应重传算法（Adaptive Retransmission Algorithm，比RTT多一点）\n超时间隔加倍：每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送\n快速重传机制：当接收方收到一个序号大于下一个所期望的报文段时，就会检测到数据流中的一个间隔，于是它就会发送冗余的 ACK，仍然 ACK 的是期望接收的报文段。而当客户端收到三个冗余的 ACK 后，就会在定时器过期之前，重传丢失的报文段\nSACK：解决快速重传机制不确定传输是否需要传输丢失报文后续的报文问题，这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据\nDuplicate SACK：又称 D-SACK，主要使用了 SACK 来告诉发送方有哪些数据被重复接收了\n\n\n流量控制使用什么数据结构\nTCP传输协议中，流量控制是使用滑动窗口（Sliding Window）来实现的。滑动窗口是一种基于数据流的、动态调整的、可变大小的窗口，它通过协商双方的接收窗口和发送窗口大小，控制数据的传输速率。\n在TCP协议中，每个数据包都有一个序号，接收方通过序号来确认是否收到了正确的数据包。发送方将数据分成若干个数据段，每个数据段的大小不超过发送窗口的大小，然后将这些数据段发送给接收方。接收方会确认已经收到的数据，同时告诉发送方自己的接收窗口大小。发送方根据接收方的窗口大小，动态调整自己的发送窗口大小，从而控制数据的传输速率。\n滑动窗口的大小是可以动态调整的，它可以根据网络状况和双方的能力来自适应地调整，从而实现流量控制的功能。如果接收方的接收窗口变小，发送方会相应地减小自己的发送窗口，以避免过多的数据堆积在网络中导致拥塞。如果接收方的接收窗口变大，发送方会相应地增加自己的发送窗口，以提高数据传输速率。\n\n\nTCP如何保证可靠传输\ntcp的序列号可以避免乱序的问题，保证收到的tcp报文都是有序的。\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。\nTCP 针对数据包丢失的情况，会用重传机制解决。\n用快重传解决个别报文段的丢失问题。\n使用滑动窗口实现流量控制。使用接收方确认报文中的窗口字段来控制发送方发送窗口大小，进而控制发送方的发送速率，使得接收方来得及接收。\n使用基于窗口的拥塞控制，来尽量避免避免网络拥塞。\n\n\nLInux的TCP连接数量最大为多少：Linux的TCP连接数量最大不能超过65535是一个常见的误解。实际上，一个TCP连接由一个五元组（协议、本地IP、本地端口、远程IP、远程端口）唯一确定12。对于服务器来说，本地端口一般是固定的，比如HTTP (80），但是远程IP和远程端口没有限制。因此，理论上服务器可以支持的TCP连接数是2的32次方（IP数）x2的16次方（端口数）x2的16次方（服多器端口数）个2\n\n\n\n\n拥塞窗口\n\n慢启动拥塞避免\n\n\n快重传快恢复\n\n\n\n\nTCP粘包分析与处理\n\n概念\n\nTCP粘包现象：发送方发送的多个数据包，到接收方后粘连在一起，导致数据包不能完整的体现发送的数据（UDP有消息边界，所以不会出现粘包问题）\n如果一次发送的消息太小，没达到缓冲区大小，TCP会讲多个请求合并为同一个请求来发送，这就形成了粘包问题；如果一次发送的消息过大，超过了缓冲区的大小，TCP就会将其拆分为多次发送，这就是拆包问题\n\n\n\n原因：可能是发送方的原因也可能是接收方的原因\n\n发送方：由于TCP需要尽可能高效和可靠，所以TCP默认采用Nagle算法，以合并相连的小数据包，再一次性发送，以达到提升网络传输效率的目的。但是接收方并不晓得发送方合并数据包，而且数据包的合并在TCP协议中是没有分界线的，所以这就导致接收方不能还原其本来的数据包\n接收方：TCP是基于“流”的，网络传输数据的速度可能会快过接收方处理数据的速度，这时就会导致接收方在读取缓冲区时，缓冲区存在多个数据包。在TCP协议中接收方是一次读取缓冲区中的所有内容，所以不能反映原本的数据信息\nTCP协议是基于字节流的传输层协议，没有固定的分包边界。发送方将数据分成多个小的数据包进行传输，接收方再将这些数据包组合成完整的数据。在这个过程中，可能会出现拆包和沾包现象\n网络传输中的延迟和拥塞会影响数据包发送的速度和到达接收方的顺序。这可能导致数据包的拆分和组合不规律，从而出现拆包和沾包现象\n接收方的缓冲区大小限制。当接收方的缓冲区不足以容纳一个完整的数据包时，可能会将数据包拆分成多个部分，导致拆包现象\n\n\n解决办法\n\n其他办法\n禁用Negel算法：只能解决发送方的问题，但是TCP的传输效率变低了\nPUSH标志：在发送时可以设置这个标志位，通知接收方将收到的数据提交给接收进程，并不能完全解决，只能降低发生粘包的可能性\n发送端将每个包都封装成固定的长度、发送端在每个包的末尾使用固定的分隔符、将消息分为头部和消息体其中头部包含消息长度\n在应用层实现数据包的边界识别，例如通过添加包头，包头中包含数据包长度等信息，使得接收方能够准确地将数据包进行拼接\n使用固定长度的数据包或者特殊的分隔符，以便于接收方识别数据包的边界\n使用更高级的传输层协议，如WebSocket，它在TCP基础上增加了数据帧的概念，可以更好地解决拆包和沾包问题\n\n\nNetty对粘包和拆包的处理，提供了一些解码器（Decoder）来解决粘包和吃啊包的问题\nLineBasedFrameDecoder：以行为单位进行数据包的解码；\nDelimiterBasedFrameDecoder：以特殊的符号作为分隔来进行数据包的解码；\nFixedLengthFrameDecoder：以固定长度进行数据包的解码；\nLenghtFieldBasedFrameDecode：适用于消息头包含消息长度的协议（最常用）\n\n\n\n\n\n\n\n\nUDP\n\n头格式\n\n\n使用场景\n\n需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用\n不需要一对一沟通，建立连接，而是可以广播的应用\n需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候\n基于UDP的协议：DHCP、HTTP3、云网络中的VXLAN、操作系统镜像的下载使用的 TFTP\n一些新兴改进\nQUIC是Google提出的一种基于UDP改进的通信协议，在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制。主要用在网页和APP的访问\n直播协议多使用RTMP，丢包时会影响直播效果，所以很多直播应用都基于UDP实现了视频传输协议\n实时游戏领域，在异步IO机制引入之前，常采用自定义UDP来解决对海量客户端连接的策略\n物联网领域终端资源少，维护TCP协议代价太大，而且物联网对实时性要求也很高。Google 旗下的 Nest 建立 Thread Group，推出了物联网通信协议 Thread，就是基于 UDP 协议的\n在 4G 网络里，移动流量上网的数据面对的协议 GTP-U 是基于 UDP 的\n\n\n\n\n\n\n其它\n\nTCP和UDP的区别\n连接：TCP 是面向连接的传输层协议，传输数据前先要建立连接；UDP 是不需要连接，即刻传输数据\n服务对象：TCP 是一对一的两点服务，即一条连接只有两个端点；UDP 支持一对一、一对多、多对多的交互通信\n可靠性：TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达；UDP 是尽最大努力交付，不保证可靠交付数据\n拥塞控制、流量控制：TCP 有拥塞控制和流量控制机制，保证数据传输的安全性；UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。\n首部开销：TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的；UDP 首部只有 8 个字节，并且是固定不变的，开销较小。\n传输方式：TCP 是流式传输，没有边界，但保证顺序和可靠；UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序\n分片不同\nTCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。\nUDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。\n\n\n\n\nTCP如何保证可靠的\ntcp的序列号可以避免乱序的问题，保证收到的tcp报文都是有序的。\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。\nTCP 针对数据包丢失的情况，会用重传机制解决。\n用快重传解决个别报文段的丢失问题。\n使用滑动窗口实现流量控制。使用接收方确认报文中的窗口字段来控制发送方发送窗口大小，进而控制发送方的发送速率，使得接收方来得及接收。\n使用基于窗口的拥塞控制，来尽量避免避免网络拥塞。\n\n\n\n\n\n\n3.应用层\nHTTP：HyperText Transfer Protocol（超文本传输协议）\n\nHTTP/1.1：无状态（cookie）、明文传输（SSL/TLS）、\nmethod\nGET：从服务器获取指定的资源，通过URL以ASCII格式传递参数，是安全且幂等的\nRFC 规范并没有规定 GET 请求不能带 body 的。理论上，任何请求都可以带 body 的。只是因为 RFC 规范定义的 GET 请求是获取资源，所以根据这个语义不需要用到 body。\n\n\nPOST：根据请求负荷（报文body中）对指定的资源做出对应的处理，不是幂等的\nURL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的。\n\n\n\n\n\nstatus codes\n1xx：表示目前是协议处理的中间状态，还需要后续操作\n2xx：表示成功状态，报文已收到并正确处理\n204：没有响应体、206：响应体不全\n\n\n3xx：重定向状态，资源位置发生变动，需要重新请求\n301：永久重定向、302：暂时重定向\n304：内容未更改，重定向到客户端的缓存资源\n\n\n4xx：客户端错误，请求报文有误服务器无法处理\n403：禁止访问资源、404资源未找到\n\n\n5xx：服务端错误，服务器在处理请求时内部发生了错误\n501 Not Implemented：表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思\n502 Bad Gateway：通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误\n503 Service Unavailable：表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思\n\n\n\n\nheader fields\nHost：客户端发送请求时，用来指定服务器的域名，可以将请求发往同一台服务器上的不同网站\nContent-Length ：表明本次回应的数据长度，HTTP 协议通过设置回车符、换行符作为 HTTP header 的边界，通过 Content-Length 字段作为 HTTP body 的边界，这两个方式都是为了解决“粘包”的问题\nConnection：客户端要求服务器使用HTTP 长连接机制（Keep-Alive），任意一端没有明确提出断开连接，则保持 TCP 连接状态\nContent-Type：用于服务器回应时，告诉客户端，本次数据是什么格式Content-Type: text/html; Charset=utf-8\nAccept：客户端请求的时候，声明自己可以接受哪些数据格式Accept: */*\nContent-Encoding：表示服务器返回的数据使用了什么压缩格式；Accept-Encodin：说明客户端可以接受哪些压缩方法\n\n\n\n\nHTTP缓存：通过将不变的请求-响应数据缓存在本地提升性能\n强制缓存：只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边\n实现：Cache-Control（一个相对时间）、Expires（一个绝对时间），建议使用Cache-Control\n当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；\n浏览器再次请求访问服务器中的该资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期，如果没有，则使用该缓存，否则重新请求服务器；\n服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。\n\n\n\n\n协商缓存：与服务端协商之后，通过协商结果来判断是否使用本地缓存\n实现：使用请求头部中的 If-None-Match 字段与响应头部中的 ETag 字段或 Last-Modified两种方式来实现，ETag用来唯一标识资源，Last-Modified用来记录修改时间，由于时间粒度大并且会被恶意修改，所以推荐使用ETag\n服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较：\n如果值相等，则返回 304 Not Modified，不会返回资源；\n如果不相等，则返回 200 状态码和返回资源，并在 Response 头部加上新的 ETag 唯一标识；\n\n\n\n\n\n\n性能优化\n尽量避免发送 HTTP 请求：缓存\n在需要发送 HTTP 请求时，考虑如何减少请求次数\n使用代理服务器减少重定向请求次数\n合并请求：使用CSS Image Sprites技术；使用 webpack 等打包工具将 js、css 等资源合并打包\n延迟发送请求：请求网页的时候，没必要把全部资源都获取到，而是只获取当前用户所看到的页面资源，当用户向下滑动页面的时候，再向服务器获取接下来的资源，这样就达到了延迟发送请求的效果\n\n\n减少服务器的 HTTP 响应的数据大小：无损压缩（gzip）、有损压缩（WebP）\n\n\nsession&amp;cookie\ncookie：客户端保存用户信息的一种机制，将服务器发送到浏览器的数据保存在本地，下次向同一服务器再发起请求时被携带发送，解决HTTP无状态的问题，可以保持用户登录状态\nsession：Session是一种在服务器端保存数据的机制，用来跟踪用户状态的数据结构，可以保存在文件、数据库或者集群中。客户端关闭会话，或者Session超时失效时会话结束\n目前大多数的应用都是用Cookie实现Session跟踪的。第一次创建Session时，服务端会通过在HTTP协议中返回给客户端，在Cookie中记录SessionID，后续请求时传递SessionID给服务，以便后续每次请求时都可分辨你是谁\n区别\n作用范围不同，Cookie 保存在客户端(浏览器)，Session 保存在服务器端。\n存取方式的不同，Cookie只能保存 ASCII，Session可以存任意数据类型，比如UserId等。\n有效期不同，Cookie可设置为长时间保持，比如默认登录功能功能，Session一般有效时间较短，客户端关闭或者Session超时都会失效。\n隐私策略不同，Cookie存储在客户端，信息容易被窃取；Session存储在服务端，相对安全一些。\n存储大小不同， 单个Cookie 保存的数据不能超过 4K，Session可存储数据远高于Cookie。\n\n\n分布式系统中的Session：会有多个服务器处理同一业务，Session存在不同服务器上会登录失败，解决方法如下\n方案一：请求精确定位。也就是通过负载均衡器让来自同一IP的用户请求始终分配到同一服务上。比如，Nginx的ip_hash策略，就可以做到\n方案二：Session复制共享。该方案的目标就是确保所有的服务器的Session是一致的。像Tomcat等多数主流web服务器都采用了Session复制实现Session的共享\n方案三：基于共享缓存。该方案是通过将Session放在一个公共地方，各个服务器使用时去取即可。比如，存放在Redis、Memcached等缓存中间件中\n\n\n同源策略与跨域请求\n同源：协议相同、域名相同、端口相同，主要是为了保证用户信息的安全，防止恶意的网站窃取数据\n恶意网站会使用用户的Cookie来伪装成用户，窃取用户信息\n\n\n跨域请求：请求一个与自身资源不同源的资源，浏览器设置了同源策略，但是有的时候网页中需要有跨域访问，所以W3C提供了一个解决方法（跨域资源共享CORS），浏览器将CORS请求分为两部分\n简单请求：浏览器直接处理\n预检请求：不符合简单请求条件的请求，会在正式通信之前触发一个 **OPTIONS**请求进行预检。服务端收到预检请求后，会根据上述附带的信息判断是否允许跨域，浏览器会根据返回的 CORS 信息判断是否继续发送真实的请求（确认后才发实际的HTTP请求）\n\n\n\n\n\n\n\n\nSSL/TLS\n\n在HTTP和TCP之间，HTTPS在进行三次握手之后还要进行SSL/TLS 的握手过程，才能进行加密报文传输。解决HTTP协议明文传输导致的：窃听信息（混合加密）、篡改响应（摘要算法）、冒充网站（数字证书）\n\n混合加密：对称加密和非对称加密结合，在通信建立前采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密。在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。\n\n对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。\n非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。\n\n\n摘要算法：用摘要算法（哈希函数）来计算出内容的哈希值\n\n\n数字证书：将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的\n\n\n\nSSL/TLS 协议基本流程\n\n客户端向服务器索要并验证服务器的公钥，双方协商生产「会话秘钥」\n即 SSL/TLS 的建立过程，也就是 TLS 握手阶段。TLS 的「握手阶段」涉及四次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：RSA 算法和 ECDHE 算法。\nTLS 在实现上分为握手协议和记录协议两层：\nTLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；\nTLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证\n\n\n\n\n双方采用「会话秘钥」进行加密通信\nSSL与TLS：SSL 是洋文 “Secure Sockets Layer” 的缩写，中文叫做「安全套接层」。它是在上世纪 90 年代中期，由网景公司设计的。到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为 TLS（是 “Transport Layer Security” 的缩写），中文叫做 「传输层安全协议」。很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。\n\n\n网络攻击常见手段\n\nIP欺骗：伪造某台主机的IP地址的技术，通过IP地址的伪装使得某台主机能够伪装另一台主机，而这台主机往往具有某种特权或者被另外的主机所信任\n\n示例：攻击者构造TCP数据，伪装自己的IP地址，并向服务器发送一个带有RSI位的TCP数据段，服务器接收到这样的数据后，认为该IP地址发送的连接有误，会清空缓冲区中建立好的连接。这时合法用户再发送合法数据，服务器就已经没有这样的连接了，使得服务器无法对合法用户服务\n缓解1：入口过滤是一种数据包过滤形式，通常在网络边缘设备上实施，用于检查传入的IP数据包并确定其源标头。如果这些数据包的源标头与其来源不匹配或者看上去很可疑，则拒绝这些数据包\n缓解2：一些网络还实施出口过滤，检查退出网络的 IP 数据包，确保这些数据包具有合法源标头，以防止网络内部用户使用 IP 欺骗技术发起出站恶意攻击\n\n\n洪泛攻击：DDoS、SYN Flood、UDP Flood、HTTP Flood、DNS Flood\n\n\n\n\n\n\n\n\n\nDDoS（Distributed Denial of Service，分布式拒绝服务）\n\nDDOS\n\nDDos 全名 Distributed Denial of Service，翻译成中文就是分布式拒绝服务。指的是处于不同位置的多个攻击者同时向一个或数个目标发动攻击，是一种分布的、协同的大规模攻击方式。单一的 DoS 攻击一般是采用一对一方式的，它利用网络协议和操作系统的一些缺陷，采用欺骗和伪装的策略来进行网络攻击，使网站服务器充斥大量要求回复的信息，消耗网络带宽或系统资源，导致网络或系统不胜负荷以至于瘫痪而停止提供正常的网络服务\n如何应对\n高防服务器：能独立硬防御50Gbps以上的服务器，能够帮助网站拒绝服务攻击，定期扫描网络主节点，唯一缺点就是贵\n黑名单：设置黑名单，但是会封锁正常流量，影响正常业务\nDDoS清洗：会对用户请求数据进行实时监控，及时发现 DOS攻击等异常流量，在不影响正常业务开展的情况下清洗掉这些异常流量\nCDN加速：将网站访问流量分配到了各个节点中，这样一方面隐藏网站的真实 IP，另一方面即使遭遇 DDoS 攻击，也可以将流量分散到各个节点中，防止源站崩溃\n\n\n\n\nSYN Flood\n\n互联网上最原始最经典的DDoS攻击，旨在耗尽可用服务器资源，致使服务器无法传输合法流量\n利用三次握手机制，通过工具或僵尸网络主机向服务器发送海量的不同源IP/不同源端口的TCP SYN报文，服务器响应了这些报文之后就会返回SYN-ACK报文，并生成大量的半连接，当系统资源被耗尽后，服务器将无法提供正常的服务\n常见形式\n直接攻击：不伪造IP地址\n欺骗攻击：恶意用户伪造其发送的各个SYN数据包的IP地址\n分布式DDoS：通过僵尸网络，令每台分布式设备伪造其发送数据包的IP地址\n\n\n服务器在返回SYN-ACK报文后，会有一个计时器Timer，如果超过时间还没有收到A的ACK消息，则重新发送一次SYN-ACK消息，直到重试超过一定次数才会放弃\n在短时面临海量连接时，SYN Flood攻击就形成了\n\n\n解决办法主要是判断哪些连接请求来自于真实源，屏蔽非真实源的请求以保障正常的业务请求能得到服务\n扩展积压工作队列：增加操作系统允许的最大半开连接数目，但需额外预留内存资源以处理各类新请求\n回收最先创建的 TCP 半开连接：在填充积压工作后覆盖最先创建的半开连接，这项策略要求完全建立合法连接的时间低于恶意 SYN 数据包填充积压工作的时间。当攻击量增加或积压工作规模小于实际需求时，这项特定的防御措施将不奏效\nSYN Cookie：此策略要求服务器创建 Cookie。为避免在填充积压工作时断开连接，服务器使用 SYN-ACK 数据包响应每一项连接请求，而后从积压工作中删除 SYN 请求，同时从内存中删除请求，保证端口保持打开状态并做好重新建立连接的准备。如果连接是合法请求并且已将最后一个 ACK 数据包从客户端机器发回服务器，服务器将重建（存在一些限制）SYN 积压工作队列条目。虽然这项缓解措施势必会丢失一些 TCP 连接信息，但好过因此导致对合法用户发起拒绝服务攻击\n\n\n\n\nUDP Flood\n\nUDP Flood也是一种拒绝服务攻击，将大量的用户数据报协议（UDP）数据包发送到目标服务器，目的是压倒该设备的处理和响应能力。防火墙保护目标服务器也可能因UDP泛滥而耗尽，从而导致对合法流量的拒绝服务\n\n攻击原理：当服务器在特定端口接收到 \nUDP\n 数据包时，会经过两个步骤：\n\n服务器首先检查是否正在运行正在侦听指定端口的请求的程序\n如果没有程序在该端口接收数据包，则服务器使用 ICMP（ping）数据包进行响应，以通知发送方目的地不可达\n\n\n如何缓解：大多数操作系统部分限制了ICMP报文的响应速率，以中断需要 ICMP 响应的DDoS攻击。这种缓解的一个缺点是在攻击过程中，合法的数据包也可能被过滤。如果UDP Flood的容量足够高以使目标服务器的防火墙的状态表饱和，则在服务器级别发生的任何缓解都将不足以应对目标设备上游的瓶颈\n\n\n\nHTTP Flood\n\nHTTP Flood 是一种大规模的 DDoS（Distributed Denial of Service，分布式拒绝服务）攻击，旨在利用 HTTP 请求使目标服务器不堪重负。目标因请求而达到饱和，且无法响应正常流量后，将出现拒绝服务，拒绝来自实际用户的其他请求\nHTTP Flood攻击有两种：\nHTTP GET 攻击 ：在这种攻击形式下，多台计算机或其他设备相互协调，向目标服务器发送对图像、文件或其他资产的多个请求。当目标被传入的请求和响应所淹没时，来自正常流量源的其他请求将被拒绝服务\nHTTP POST 攻击 ： 一般而言，在网站上提交表单时，服务器必须处理传入的请求并将数据推送到持久层（通常是数据库）。与发送 POST 请求所需的处理能力和带宽相比，处理表单数据和运行必要数据库命令的过程相对密集。这种攻击利用相对资源消耗的差异，直接向目标服务器发送许多 POST 请求，直到目标服务器的容量饱和并拒绝服务为止\n\n\n防护办法\n对发出请求的设备实施质询，以测试它是否是机器人，这与在线创建帐户时常用的 CAPTCHA 测试非常相似。通过提出 JavaScript 计算挑战之类的要求，可以缓解许多攻击\nWeb 应用程序防火墙 (WAF)、管理 IP 信誉数据库以跟踪和有选择地阻止恶意流量，以及由工程师进行动态分析\n\n\n\n\nDNS Flood\n\n攻击者用大量流量淹没某个域的 DNS 服务器，以尝试中断该域的 DNS 解析。通过中断 DNS 解析，DNS Flood攻击将破坏网站、API 或 Web 应用程序响应合法流量的能力。很难将 DNS Flood攻击与正常的大流量区分开来，因为这些大规模流量往往来自多个唯一地址，查询该域的真实记录，模仿合法流量\n如何防护：DNS Flood 对传统上基于放大的攻击方法做出了改变。借助轻易获得的高带宽僵尸网络，攻击者现能针对大型组织发动攻击。除非被破坏的 IoT 设备得以更新或替换，否则抵御这些攻击的唯一方法是使用一个超大型、高度分布式的 DNS 系统，以便实时监测、吸收和阻止攻击流量\n\n\n\n\n其他攻击\n\nTCP重置攻击\n在 TCP重置攻击中，攻击者通过向通信的一方或双方发送伪造的消息，告诉它们立即断开连接，从而使通信双方连接中断\n一般客户端发现到达的报文对于相关连接不正确时，就会发送一个重置报文段，从而导致TCP连接的快速拆卸\n\n\n一般只对长连接有效果，对于短连接而言，你还没攻击呢，人家已经完成了信息交换\n\n\n中间人攻击\n\n\n\n\n\n\nHTTP协议的演变\n\nHTTP/1.1 相比 HTTP/1.0 性能上的改进：\n长连接：只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载\n如果要关闭 HTTP Keep-Alive，需要在 HTTP 请求或者响应的 header 里添加 Connection:close 信息，也就是说，只要客户端和服务端任意一方的 HTTP header 中有 Connection:close 信息，那么就无法使用 HTTP 长连接的机制\n\n\n管道网络传输：即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间，但不是默认开启的\n对头堵塞问题：如果服务端在处理某个请求时耗时比较长，那么后续的请求的处理都会被阻塞住\nHTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞，可以同时发多个请求但是需要响应等待服务器\n\n\n\n\n那 HTTP/2 相比 HTTP/1.1 性能上的改进：\n安全传输：基于HTTPS\n头部压缩（HPACK算法）：通过在客户端和服务器同时维护一张头信息表，只传输索引号而不是重复传相同的头部\n二进制格式：不再是纯文本形式的报文，头信息和数据体都是二进制，并且统称为帧（frame），头信息帧（Headers Frame）和数据帧（Data Frame）\n并发传输：1 个 TCP 连接包含多个 Stream，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）\n针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的\n\n\n服务器主动推送资源：服务端不再是被动地响应，可以主动向客户端发送消息。客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n\n\nHTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP\n使用基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输\nQUIC协议\n无队头阻塞：当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题\n更快的连接建立：三次握手（ QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商）\n连接迁移：TCP通过四元组标识，Wi-Fi切换到4G时需要重新建立连接，而QUIC通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能\n\n\n\n\n队头阻塞问题\nHTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞\nHTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时（丢包），后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题\nHTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP，UDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输\n\n\n分块传输：分块传输感觉是说http协议里的chunk传输。它允许将数据分成多个块（Chunk）进行传输，每个块都包含一段数据和该块数据的长度。在传输数据时，先发送一个块的长度，然后发送该块的数据，接着发送下一个块的长度和数据，以此类推，直到所有的数据都传输完毕。\n\n\nWeb页面请求过程\n\n概览 ：HTTP解析URL（服务器名+路径名）并生成HTTP请求消息（Get/Post报文），通过DNS查询服务器名对应的IP地址（DNS请求-本地-根-顶级-权威），通过Socket委托Linux协议栈进行层层处理（TCP-IP-网卡驱动-物理网卡）；TCP协议填充头信息（端口、序号、确认号、状态位）进行三次握手、流量控制、拥塞控制，IP协议通过ip地址来确定路由方向，通过ARP协议根据ip地址得到MAC地址再通过MAC地址在以太网中传输数据；通过网卡驱动程序增加数据（报头、帧分节符、校验符）并将二进制数据转换成电信号，通过二层设备交换机通过MAC地址决定走哪个端口，通过三层设备路由器路由表来查询走哪个端口\n在发送数据包时，如果目标主机不是本地局域网，填入的 MAC 地址是路由器，也就是把数据包转发给路由器，路由器一直转发下一个路由器，直到转发到目标主机的路由器，发现目标 IP 地址是自己局域网内的主机，就会 ARP 请求获取目标主机的 MAC 地址，从而转发到这个服务器主机\n\nHTTP调度如下\n\n\n\n\nDHCP 配置主机信息\n假设主机最开始没有 IP 地址以及其它信息，那么就需要先使用 DHCP 来获取。\n主机生成一个 DHCP 请求报文，并将这个报文放入具有目的端口 67 和源端口 68 的 UDP 报文段中。\n该报文段则被放入在一个具有广播 IP 目的地址(255.255.255.255) 和源 IP 地址（0.0.0.0）的 IP 数据报中。\n该数据报则被放置在 MAC 帧中，该帧具有目的地址 FF:FF:FF:FF:FF:FF，将广播到与交换机连接的所有设备。\n连接在交换机的 DHCP 服务器收到广播帧之后，不断地向上分解得到 IP 数据报、UDP 报文段、DHCP 请求报文，之后生成 DHCP ACK 报文，该报文包含以下信息：IP 地址、DNS 服务器的 IP 地址、默认网关路由器的 IP 地址和子网掩码。该报文被放入 UDP 报文段中，UDP 报文段有被放入 IP 数据报中，最后放入 MAC 帧中。\n该帧的目的地址是请求主机的 MAC 地址，因为交换机具有自学习能力，之前主机发送了广播帧之后就记录了 MAC 地址到其转发接口的交换表项，因此现在交换机就可以直接知道应该向哪个接口发送该帧。\n主机收到该帧后，不断分解得到 DHCP 报文。之后就配置它的 IP 地址、子网掩码和 DNS 服务器的 IP 地址，并在其 IP 转发表中安装默认网关。\n\n\nARP 解析 MAC 地址\n主机通过浏览器生成一个 TCP 套接字，套接字向 HTTP 服务器发送 HTTP 请求。为了生成该套接字，主机需要知道网站的域名对应的 IP 地址。\n主机生成一个 DNS 查询报文，该报文具有 53 号端口，因为 DNS 服务器的端口号是 53。\n该 DNS 查询报文被放入目的地址为 DNS 服务器 IP 地址的 IP 数据报中。\n该 IP 数据报被放入一个以太网帧中，该帧将发送到网关路由器。\nDHCP 过程只知道网关路由器的 IP 地址，为了获取网关路由器的 MAC 地址，需要使用 ARP 协议。\n主机生成一个包含目的地址为网关路由器 IP 地址的 ARP 查询报文，将该 ARP 查询报文放入一个具有广播目的地址（FF:FF:FF:FF:FF:FF）的以太网帧中，并向交换机发送该以太网帧，交换机将该帧转发给所有的连接设备，包括网关路由器。\n网关路由器接收到该帧后，不断向上分解得到 ARP 报文，发现其中的 IP 地址与其接口的 IP 地址匹配，因此就发送一个 ARP 回答报文，包含了它的 MAC 地址，发回给主机。\n\n\nDNS 解析域名\n知道了网关路由器的 MAC 地址之后，就可以继续 DNS 的解析过程了。\n网关路由器接收到包含 DNS 查询报文的以太网帧后，抽取出 IP 数据报，并根据转发表决定该 IP 数据报应该转发的路由器。\n因为路由器具有内部网关协议（RIP、OSPF）和外部网关协议（BGP）这两种路由选择协议，因此路由表中已经配置了网关路由器到达 DNS 服务器的路由表项。\n到达 DNS 服务器之后，DNS 服务器抽取出 DNS 查询报文，并在 DNS 数据库中查找待解析的域名。\n找到 DNS 记录之后，发送 DNS 回答报文，将该回答报文放入 UDP 报文段中，然后放入 IP 数据报中，通过路由器反向转发回网关路由器，并经过以太网交换机到达主机。\n\n\nHTTP 请求页面\n有了 HTTP 服务器的 IP 地址之后，主机就能够生成 TCP 套接字，该套接字将用于向 Web 服务器发送 HTTP GET 报文。\n在生成 TCP 套接字之前，必须先与 HTTP 服务器进行三次握手来建立连接。生成一个具有目的端口 80 的 TCP SYN 报文段，并向 HTTP 服务器发送该报文段。\nHTTP 服务器收到该报文段之后，生成 TCP SYN ACK 报文段，发回给主机。\n连接建立之后，浏览器生成 HTTP GET 报文，并交付给 HTTP 服务器。\nHTTP 服务器从 TCP 套接字读取 HTTP GET 报文，生成一个 HTTP 响应报文，将 Web 页面内容放入报文主体中，发回给主机。\n浏览器收到 HTTP 响应报文后，抽取出 Web 页面内容，之后进行渲染，显示 Web 页面。\n\n\n\n\n网络分层\n\nTCP/IP四层协议：网络接口层、网络层、传输层、应用层\n\n网络接口层：MAC地址\n物理层\n物理信道：单工通信（无线电广播）、半双工通信（对讲机）、全双工通信（手机打电话）\n物理设备：交换机（连接多段网线）、调制解调器（将数字信号转换成电话线的模拟信号或光信号）\n\n\n数据链路层\n在一条链路上传输数据时，需要有对应的通信协议来控制数据的传输\n广播信道：CSMA/CD协议，如同轴电缆、集线器等组成的网络\n点对点信道：PPP协议，如2个路由器之间的信道\n\n\n数据链路层的3个基本问题\n封装成帧：帧的数据部分就是网络层传递下来的数据包，帧的最大长度为MTU（1500字节）\n透明传输：完整的数据使用SOH作为开始，使用EOT作为结束，与标志相同的数据会被转义传输，到达后被还原\n差错检验：帧尾部有一个FCS字段，是根据数据部分和数据链路层首部计算得出的，可以用来检验接收到的消息对错\n\n\nCSMA/CD：载波侦听多路访问/冲突检测，用来支持单用通信和半双工通信\n\n\n\n\n网络层：IP、ARP（IP解析为MAC）、DHCP（获取IP）、ICMP（ping的底层实现）、NAT（网关内复用IP，将私有IP转换为公网IP）、OSPF和BGP（动态路由协议，确定走哪个路由器）\n传输层：TCP（流量控制、超时重传、拥塞控制）、UDP（实时性好传输效率高）、Socket编程\n应用层：HTTP协议（超文本传输协议）、FTP（文件传输协议）、DNS（将域名转换为IP地址）、SMTP（邮件传输协议）、Telnet（使用命令行与服务器通信）\n\n\nOSI七层：（物理层、数据链路层）；网络层；传输层；（会话层、表示层、应用层）\n\n\n\n\n\n附录1.Sockets\nsocket 是什么？\n\nsock（或 socket）是操作系统内核提供的一种数据结构，用于实现网络传输功能\n基于不同的网络协议以及应用场景，衍生了各种类型的 sock。每个网络层协议都有相应的 sock 结构体来管理该层协议的连接状态和数据传输。各类 sock 操作硬件网卡，就实现了网络传输的功能\n为了将这些功能让处在用户态的应用程序使用，不但引入了 socket 层，还将各类功能的实现方式抽象成了 API 接口，供应用程序调用\n同时将 sock 封装成文件，应用程序就可以在用户层通过文件句柄（socket fd）来操作内核中 sock 的网络传输功能。这个 socket fd 是一个 int 类型的数字，而 socket 中文翻译叫做套接字，结合这个 socket fd，你是不是可以将其理解成：一套用于连接的数字\n而 socket 分 Internet socket 和 UNIX Domain socket，两者都可以用于不同主机进程间的通信和本机进程间的通信。只是前者采用的是基于 IP 协议的网络通信方式，而后者采用的是基于本地文件系统的通信方式\n\n\n网络包接收流程\n\n网卡会将收到的网络数据帧通过DMA的方式放到环形缓冲区RingBuffer（环形缓冲区，满了就丢弃）中（可通过ifconfig命令查看网卡收发数据的情况），DMA操作完后网卡会向CPU发起一个硬中断，对应中断处理程序为网络数据帧创建内核数据结构sk_buffer（sk_buffer是双向链表，每一个元素是一个网络帧，网络各层之间操作相关指针而不是复制数据）并拷贝收到的网络数据帧，然后向kernel发起软中断请求（软中断和硬中断在同一个CPU核上）\n\n为了解决频繁中断带来的性能开销，Linux 2.6 版本引入了 NAPI 机制，它是混合「中断和轮询」的方式来接收网络包，它的核心概念就是不采用中断的方式读取数据，而是首先采用中断唤醒数据接收的服务程序，然后 poll 的方法来轮询数据\n硬中断：先暂时屏蔽中断，表示已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停的被中断。接着，发起软中断，然后恢复刚才屏蔽的中断。\n\n\n内核线程ksoftirqd（一个CPU一个）响应软中断请求，调用网卡驱动注册的poll函数，poll函数将sk_buffer中的网络数据包送到内核协议栈中注册的ip_rcv函数\n\nip_rcv函数处在网络层，该函数取出IP头并判断下一跳走向，如果是本机则取出传输层协议类型（TCP或UDP），并去掉数据包的IP头，将数据包交给传输层处理（TCP协议使用tcp_rcv函数，UDP协议使用udp_rcv函数）\n\ntcp_rcv函数会去掉TCP头，根据四元组（源IP、源端口、目的IP，目的端口）查找对应的Socket，如果找到则将数据包中的数据拷贝到Socket中的接收缓冲区，否则发送一个目标不可达的icmp包\n\n当程序调用read读取Socket接收缓冲区的数据时，如果接收缓冲区没有数据则会阻塞在系统调用上，知道Socket接收缓冲区有数据。然后CPU将内核空间（Socket接收缓冲区）的数据拷贝到用户空间，最后系统调用read返回，应用程序读取数据\n\n\n\n\n网络包发送流程\n\n应用程序调用send发送数据，kernel首先根据fd找到对应的socket（包含各种协议栈的函数地址），然后构造msghdr对象封装用户要发送的数据。调用内核函数inet_sendmsg发送流程进入内核协议栈来处理，找到socket上具体协议的发送函数（TCP协议发送函数tcp_sendmsg，UDP协议发送函数udp_sendmsg）\n\ntcp_sendmsg函数内部创建内核数据结构sk_buffer将msghdr中的数据拷贝到sk_buffer中，调用tcp_write_queue_tail函数获取Socket发送队列中的队尾元素，将sk_buffer添加到socket发送队列（双向链表）的尾部\n此时数据已经被拷贝到内核，但是因为TCP的流量控制和拥塞控制，所以数据并不会被马上发送，需要符合TCP协议的发送条件。如果没有达到发送条件则本次send系统调用就会直接返回，如果符合发送条件，则调用tcp_write_xmit内核函数，循环获取socket发送队列中待发送的sk_buffer，然后进行拥塞控制和滑动窗口管理\n发送网络数据的时候，涉及几次内存拷贝操作？\n第一次，调用发送数据的系统调用的时候，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。\n第二次，在使用 TCP 传输协议的情况下，从传输层进入网络层的时候，每一个 sk_buff 都会被克隆一个新的副本出来。副本 sk_buff 会被送往网络层，等它发送完的时候就会释放掉，然后原始的 sk_buff 还保留在传输层，目的是为了实现 TCP 的可靠传输，等收到这个数据包的 ACK 时，才会释放原始的 sk_buff 。\n第三次，当 IP 层发现 sk_buff 大于 MTU 时才需要进行。会再申请额外的 sk_buff，并将原来的 sk_buff 拷贝为多个小的 sk_buff。\n\n\n\n\n将从socket发送队列中获取到的sk_buffer重新拷贝一份，并设置sk_buffer副本中的TCP HEADER\n\nsk_buffer 内部其实包含了网络协议中所有的 header。在设置 TCP HEADER的时候，只是把指针指向 sk_buffer的合适位置。后面再设置 IP HEADER的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高\n\n\n为什么不直接使用Socket发送队列中的sk_buffer而是需要拷贝一份呢？因为TCP协议是支持丢包重传的，在没有收到对端的ACK之前，这个sk_buffer是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是sk_buffer的拷贝副本，当网卡把数据发送出去后，sk_buffer拷贝副本会被释放。当收到对端的ACK之后，Socket发送队列中的sk_buffer才会被真正删除\n\n为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包：sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame，通过调整 sk_buff 中 data 的指针来实现\n\n\n\n\n调用ip_queue_xmit内核函数进行网络层的处理\n\n将sk_buffer中的指针移动到IP头位置上，设置IP头\n执行netfilters过滤。过滤通过之后，如果数据大于 MTU的话，则执行分片\n检查Socket中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到Socket中。接着在把路由表设置到sk_buffer中\n\n\n邻居子系统\n\n邻居子系统位于内核协议栈中的网络层和网络接口层之间，用于发送ARP请求获取MAC地址，然后将sk_buffer中的指针移动到MAC头位置，填充MAC头，此时sk_buffer中已经封装了一个完整的数据帧\n\n\n网络设备子系统\n\n选择发送队列（RingBuffer），因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列\n\n将sk_buffer添加到发送队列中\n\n循环从发送队列（RingBuffer）中取出sk_buffer，调用内核函数sch_direct_xmit发送数据，其中会调用网卡驱动程序来发送数据\n\n\n\n无论是用户线程的内核态还是触发NET_TX_SOFTIRQ类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数dev_hard_start_xmit来发送数据。在网卡驱动程序函数dev_hard_start_xmit中会将sk_buffer映射到网卡可访问的内存 DMA 区域，最终网卡驱动程序通过DMA的方式将数据帧通过物理网卡发送出去\n\n前文a-e是用户线程的内核态在执行，占用的CPU时间是系统态时间(sy)，当分配给用户线程的CPU quota用完的时候，会触发NET_TX_SOFTIRQ类型的软中断，内核线程ksoftirqd会响应这个软中断，并执行NET_TX_SOFTIRQ类型的软中断注册的回调函数net_tx_action，在回调函数中会执行到驱动程序函数 dev_hard_start_xmit来发送数据\n从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，通过触发NET_RX_SOFTIRQ类型的软中断在内核线程ksoftirqd中执行内核网络协议栈接受数据。而在网络数据包的发送过程中是用户线程的内核态在执行内核网络协议栈，只有当线程的CPU quota用尽时，才触发NET_TX_SOFTIRQ软中断来发送数据\n在整个网络包的发送和接受过程中，NET_TX_SOFTIRQ类型的软中断只会在发送网络包时并且当用户线程的CPU quota用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为NET_RX_SOFTIRQ。所以这就是在服务器上查看 /proc/softirqs，一般 NET_RX都要比 NET_TX大很多的的原因\n\n\n当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向CPU发送一个硬中断，CPU调用网卡驱动程序注册的硬中断响应程序，在硬中断响应中触发NET_RX_SOFTIRQ类型的软中断，在软中断的回调函数igb_poll中清理释放 sk_buffer，清理网卡发送队列（RingBuffer），解除DMA映射\n\n无论硬中断是因为有数据要接收，还是说发送完成通知，从硬中断触发的软中断都是 NET_RX_SOFTIRQ\n这里释放清理的只是sk_buffer的副本，真正的sk_buffer现在还是存放在Socket的发送队列中。前面在传输层处理的时候我们提到过，因为传输层需要保证可靠性，所以 sk_buffer其实还没有删除。它得等收到对方的 ACK 之后才会真正删除\n\n\n\n\n\n2.网络设备\n路由器和交换机的区别\n工作层次不同：\n交换机主要工作在数据链路层（第二层）\n路由器工作在网络层（第三层）\n\n\n转发依据不同：\n交换机转发所依据的对象时：MAC地址。（物理地址）\n路由转发所依据的对象是：IP地址。（网络地址），因为只处理MAC地址匹配路由器的网络爆\n\n\n主要功能不同：（交换机能做的，路由都能做。）\n交换机主要用于组建局域网，不能分割广播域\n而路由主要功能是将由交换机组好的局域网相互连接起来，或者接入Internet，可以提供防火墙功能\n\n\n\n\n\n3.命令行\nnetstat -napt：查看TCP的连接状态\n\n\nroute -n：查看当前系统的路由表，将目的IP与掩码与操作得到的结果与Destination比较，相同则走对应的iface（网卡）\n\n第三条是默认网关：如果其他所有条目都无法匹配，就会自动匹配这一行。并且后续就把包发给路由器（Gateway即对应路由器的IP地址）\n\n\n\narp -a 查看 ARP 缓存的内容，记录IP与MAC的映射关系\n\n\n\n4.RPC\n为什么有HTTP协议了?还要用RPC?\nRPC 本质上不算是协议，而是一种调用方式，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，不一定非得基于 TCP 协议。\n从发展历史来说，HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。\nRPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP/1.1 性能要更好，所以大部分公司内部都还在使用 RPC。\nHTTP/2.0在 HTTP/1.1的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。\n\n\n\n5.CDN\nCDN 将内容资源分发到位于多个地理位置机房中的服务器上，这样我们在访问内容资源的时候，不用访问源服务器。而是直接访问离我们最近的 CDN 节点 ，这样一来就省去了长途跋涉的时间成本，从而实现了网络加速\n\n找到离用户最近的 CDN 节点是由 CDN 的全局负载均衡器（*Global Sever Load Balance，GSLB*）负责的。\n\n在没有 CDN 的情况下，访问域名时，通过DNS 服务器返回源服务器的地址\n\n在有CDN的情况下会在 xiaolin.com 这个 DNS 服务器上，设置一个 CNAME 别名，指向另外一个域名 www.xiaolin.cdn.com，返回给本地 DNS 服务器。\n\n接着继续解析该域名，这个时候访问的就是 xiaolin.cdn.com 这台 CDN 专用的 DNS 服务器，在这个服务器上，又会设置一个 CNAME，指向另外一个域名，这次指向的就是 CDN 的 GSLB。\n\n接着，本地 DNS 服务器去请求 CDN 的 GSLB 的域名，GSLB 就会为用户选择一台合适的 CDN 节点提供服务，选择的依据主要有以下几点：\n\n看用户的 IP 地址，查表得知地理位置，找相对最近的 CDN 节点；\n看用户所在的运营商网络，找相同网络的 CDN 节点；\n看用户请求 URL，判断哪一台服务器上有用户所请求的资源；\n查询 CDN 节点的负载情况，找负载较轻的节点；\n\n\nGSLB 会基于以上的条件进行综合分析后，找出一台最合适的 CDN 节点，并返回该 CDN 节点的 IP 地址给本地 DNS 服务器，然后本地 DNS 服务器缓存该 IP 地址，并将 IP 返回给客户端，客户端去访问这个 CDN 节点，下载资源\n\n\n\n\n\n","slug":"Internet","date":"2023-08-02T14:26:24.000Z","categories_index":"","tags_index":"","author_index":"Dajunnnnnn"},{"id":"7092a7d1e2affc5f0cb3af30af9d1e19","title":"Tools","content":"1.markdown\n框架\n\n多级标题：#与后面文本有一个空格，标题行前后建议空行\n\n\n\nMarkdown\nHTML\n\n\n\n# Heading level 1\nHeading level 1\n\n\n## Heading level 2\nHeading level 2\n\n\n### Heading level 3\nHeading level 3\n\n\n#### Heading level 4\nHeading level 4\n\n\n##### Heading level 5\nHeading level 5\n\n\n###### Heading level 6\nHeading level 6\n\n\n\n引用\n&gt; #### The quarterly results look great!\n&gt;\n&gt; - Revenue was off the chart.\n&gt; - Profits were higher than ever.\n&gt;\n&gt;  *Everything* is going according to **plan**.\n&gt;&gt; The Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood.\n段落：用一个空行分离两段文本，每一个文本段落建议不首行缩进\n\n\n\nMarkdown\nHTML\n\n\n\nI really like using Markdown.I think I’ll use it to format all of my documents from now on.\nI really like using Markdown.I think I’ll use it to format all of my documents from now on.\n\n\n\n\n\n文本格式\n\n\n\n\nMarkdown\nHTML\n\n\n\n加粗\nI just love bold text.\nI just love bold text.\n\n\n斜体\nItalicized text is the cat’s meow.\nItalicized text is the cat’s meow.\n\n\n加粗+斜体\nThis text is really important.\nThis text is really important.\n\n\n删除\nThe world is flat.\n有可能不兼容\n\n\n高亮\nI need to highlight these ==very important words==.\n有可能不兼容\n\n\n\nlist\n有序链表\n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item\n无序链表\n- First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item\n有序内嵌无序\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\n表格\n| Syntax      | Description |\n| ----------- | ----------- |\n| Header      | Title       |\n| Paragraph   | Text        |\n\n指定左右对齐\n| Syntax      | Description | Test Text     |\n| :---        |    :----:   |          ---: |\n| Header      | Title       | Here&#39;s this   |\n| Paragraph   | Text        | And more      |\n其它\n\n代码块：看软件\n\n图片：![Tux, the Linux mascot](/assets/images/tux.png)\n\n水平线：使用***，或者使用—-\n\n链接：\n[Duck Duck Go](&lt;https:&#x2F;&#x2F;duckduckgo.com&gt;).\n\n\n增加光标悬停时的title：[Duck Duck Go](&lt;https://duckduckgo.com&gt; &quot;The best search engine for privacy&quot;).\nURLs或Email地址：&lt;https://www.markdownguide.org&gt;、&lt;fake@example.com&gt;\n链接内的特殊字符有的时候需要更改，如[link](&lt;https://www.example.com/my%20great%20page&gt;)（使用%20代替空格）\n\n\n转义字符：使用\\\\来转义前面有特殊意义的字符\n\n\n\n\n2.mermaid\n\n\n\n\n\n\n\n\n文档地址：https://mermaid.js.org/intro/，代码地址：https://github.com/mermaid-js/mermaid\n\n语法结构\n\n部署环境\n\n会影响图标连续性的字符\n\n\n\nDiagram Breakers\nReason\nSolution\n\n\n\nComments\n\n\n\n\nhttps://github.com/mermaid-js/mermaid/issues/1968\nSimilar to https://mermaid.js.org/config/directives.html confuses the renderer.\nIn comments using %%, avoid using “{}”.\n\n\nFlow-Charts\n\n\n\n\n‘end’\nThe word “End” can cause Flowcharts and Sequence diagrams to break\nWrap them in quotation marks to prevent breakage.\n\n\nNodes inside Nodes\nMermaid gets confused with nested shapes\nwrap them in quotation marks to prevent breaking\n\n\n\n基础语法\n\n点\n\n可选择是否填充文本\n可选择节点形状：\n\nflowchart TD\n    id\n    id1[This is the text in the box]\n\t\tid2(text)\n\t\tid3([text])\n\t\tid4[[text]]\n\t\tid5[(Database)]\n\t\tid6&gt;text]\n\t\tid7&#123;&#123;text&#125;&#125;\n\t\tid8[&#x2F;This is the text in the box&#x2F;]\n边：TB（top to bottom，或TD）、BT（bottom to top）、RL（right to left）、LR（left to right）\n\n不可见连接，主要为了排版：A ~~~ B\n加粗：A ==&gt; B\n简化语法：a --&gt; b &amp; c--&gt; d、A &amp; B--&gt; C &amp; D\n\nflowchart TB\n\t\tsubgraph one\n    A --&gt; B\n\t\tA--&gt;|text|B\n\t\tA --- C\n\t\tA-- text ---C\n\t\tA-.-&gt;D\n\t\tA-. text .-&gt; D\n\t\tend\n\t\tsubgraph two \n\t\ta[Start] --&gt; b&#123;Is it?&#125;\n    b -- Yes --&gt; c[OK]\n    c --&gt; d[Rethink]\n    d --&gt; b\n    b -- No ----&gt; e[End]\n\t\tend\n\t\t\n\t\t\n边的级别\n\n\n\nLength\n1\n2\n3\n\n\n\nNormal\n—\n—-\n—–\n\n\nNormal with arrow\n–&gt;\n—&gt;\n—-&gt;\n\n\nThick\n===\n====\n=====\n\n\nThick with arrow\n==&gt;\n===&gt;\n====&gt;\n\n\nDotted\n-.-\n-..-\n-…-\n\n\nDotted with arrow\n-.-&gt;\n-..-&gt;\n-…-&gt;\n\n\n\n\n\n\n\nFlowchart\ngraph TD;\n    A--&gt;B;\n    A--&gt;C;\n    B--&gt;D;\n    C--&gt;D;\nSequence diagramsequenceDiagram\n    participant Alice\n    actor Bob\n    Alice-&gt;&gt;John: Hello John, how are you?\n    loop Healthcheck\n        John-&gt;&gt;John: Fight against hypochondria\n    end\n    Note right of John: Rational thoughts &lt;br&#x2F;&gt;prevail!\n    John--&gt;&gt;Alice: Great!\n    John-&gt;&gt;Bob: How about you?\n    Bob--&gt;&gt;John: Jolly good!\nClass diagram\nclassDiagram\nClass01 &lt;|-- AveryLongClass : Cool\nClass03 *-- Class04\nClass05 o-- Class06\nClass07 .. Class08\nClass09 --&gt; C2 : Where am i?\nClass09 --* C3\nClass09 --|&gt; Class07\nClass07 : equals()\nClass07 : Object[] elementData\nClass01 : size()\nClass01 : int chimp\nClass01 : int gorilla\nClass08 &lt;--&gt; C2: Cool label\nGantt diagram\ngantt\ndateFormat  YYYY-MM-DD\ntitle Adding GANTT diagram to mermaid\nexcludes weekdays 2014-01-10\n\nsection A section\nCompleted task            :done,    des1, 2014-01-06,2014-01-08\nActive task               :active,  des2, 2014-01-09, 3d\nFuture task               :         des3, after des2, 5d\nFuture task2               :         des4, after des3, 5d\nGIt graph\ngitGraph\n       commit\n       commit\n       branch develop\n       commit\n       commit\n       commit\n       checkout main\n       commit\n       commit\n\n3.Latex\n\n\n相关工具和文档\n\n文档：https://www.latex-project.org/\nOverleaf：网上的LaTex编辑器，不用配置本地环境，需要挂VPN否则访问慢\nBibDesk：用于管理文献引用的软件，可以帮助创建、编辑和组织文献数据库，并生成符合各种引用风格的参考文献列表\nLaTeXiT：用于生成高质量数学公式和方程的工具，允许输入LaTex代码并即时预览生成的公式，可以导出为矢量图或其他格式，方便在文档中使用\nTeX Live Utility：一个用于管理TeX Live发行版的使用工具，Tex Live是一个广泛使用的TeX发行版，包含了大量的TeX相关软件包和工具，Tex Live Utility允许安装、更新和删除TeX Live中的软件包，来保证当前系统与最新的TeX发行版保持同步\nTeXShop：是一个用于编辑和编译LaTeX文档的集成开发环境（IDE），提供了一个用户友好的界面并且可以直接从TeXShop中编译生成PDF文档\n\n\n语法\n\n文档类（Document Class）：在LaTeX文档的开头使用\\\\documentclass&#123;&#125;命令指定文档的类型，例如\\\\documentclass&#123;article&#125;表示创建一篇文章。\n导言区（Preamble）：在\\\\documentclass&#123;&#125;命令后，\\\\begin&#123;document&#125;命令前的部分被称为导言区。可以在导言区中加载宏包、定义命令、设置页面布局等。\n章节命令：使用\\\\section&#123;&#125;、\\\\subsection&#123;&#125;、\\\\subsubsection&#123;&#125;等命令来创建章节标题。例如，\\\\section&#123;Introduction&#125;表示创建一个名为”Introduction”的章节。\n标题和作者：使用\\\\title&#123;&#125;和\\\\author&#123;&#125;命令设置文档的标题和作者。在导言区中，可以使用\\\\maketitle命令在文档的正文中生成标题和作者信息。\n内容和段落：LaTeX使用空行来分隔段落。可以使用\\\\par命令来手动分段。使用普通文本编写内容即可。\n数学公式：LaTeX以其强大的数学排版功能而闻名。可以使用$...$或\\\\(...\\\\)将数学表达式嵌入到文本中，也可以使用$$...$$或\\\\[...\\\\]创建独立的行间公式。LaTeX提供了丰富的数学符号和环境，如\\\\frac&#123;&#125;&#123;&#125;表示分数，\\\\sum表示求和符号等。\n列表：使用itemize环境创建无序列表，使用enumerate环境创建有序列表。每个列表项使用\\\\item命令来标记。\n表格：LaTeX提供了tabular环境来创建表格。可以指定表格的列数和每列的对齐方式，并使用&amp;来分隔单元格，使用\\\\\\\\来换行。\n图片和图表：使用\\\\includegraphics&#123;&#125;命令将图片插入到文档中。可以设置图片的大小、位置和标题。使用figure环境可以创建带有标题和标签的浮动图形。\n引用和交叉引用：使用\\\\label&#123;&#125;命令为章节、公式、图表等添加标签，然后使用\\\\ref&#123;&#125;命令进行引用。LaTeX会自动处理引用的编号和页码。\n\n\n公式\n\n行内公式：使用$...$或者\\\\(...\\\\)将数学表达式嵌入到文本中。例如：$E=mc^2$。\n\n行间公式：使用$$...$$或者\\\\[...\\\\]创建独立的行间公式。例如：$$\\\\int_&#123;0&#125;^&#123;1&#125; x^2 dx$$。\n\n上下标：使用^表示上标，_表示下标。例如：$x^2$表示x的平方。\n\n分式：使用\\\\frac&#123;numerator&#125;&#123;denominator&#125;表示分式。例如：\\\\frac&#123;1&#125;&#123;2&#125;表示1/2。\n\n开根号：使用\\\\sqrt&#123;&#125;表示开根号。例如：\\\\sqrt&#123;2&#125;表示根号下2。\n\n求和、积分符号：使用\\\\sum表示求和符号，使用\\\\int表示积分符号。例如：\\\\sum_&#123;i=1&#125;^&#123;n&#125; i表示从1到n的求和。\n\n矩阵和数组：使用matrix环境创建矩阵，使用array环境创建数组。例如：\n复制代码\n\\\\begin&#123;matrix&#125;\n1 &amp; 2 \\\\\\\\\n3 &amp; 4 \\\\\\\\\n\\\\end&#123;matrix&#125;\n方程组：使用cases环境创建方程组。例如：\n复制代码\n\\\\begin&#123;cases&#125;\nx + y &#x3D; 2 \\\\\\\\\nx - y &#x3D; 1 \\\\\\\\\n\\\\end&#123;cases&#125;\n上下限：使用\\\\limits来指定求和、积分等运算符的上下限。例如：\\\\sum\\\\limits_&#123;i=1&#125;^&#123;n&#125; i表示从1到n的求和。\n\n特殊符号：LaTeX提供了许多特殊符号的命令，例如希腊字母、箭头、关系符号等。例如：\\\\alpha表示α，\\\\rightarrow表示→。\n\n\n\n\n4.Git1.Git整体结构\n\n工作区：电脑里能看到的目录\n暂存区：.git目录下的index文件（.git/index），也叫做索引（index）\n本地版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库\n远程仓库\nGit文件状态\n版本\nHEAD：指向master分支的一个游标，所以出现Head的地方可以用master来替换\n分支\n\n2.常用操作\n查看/修改提交用户名、邮箱\n#查看$ git config --list$ git config user.name$ git config user.email#不加--global将只更改本仓库的配置$ git config --global user.name &quot;dajunnnnnn&quot;$ git config --global user.email &quot;1064049895@qq.com&quot;\n操作流程\n\n初始化：git init，在执行完成 git init命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变\n\n克隆：git clone，从现有Git仓库中拷贝项目，例如git clone &lt;https://github.com/Dajun-2019/Learning.git&gt;\n\n暂存：git add .，对工作区执行此命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库（.git/objects）中的一个新对象中，而该对象的ID被记录在暂存区的文件索引中\n\n提交：git commit -m &quot;注释信息”，暂存区的目录树写到版本库（对象库）中，master分支会做相应的更新，即master指向的目录树就是提交时暂存区的目录树\n\n远端操作\n\n首先载入远程仓库（git clone…）、然后进入此仓库，执行git remote -v，输出的origin为远程地址的别名\n显示某个远程仓库的信息：git remote show [remote]\n其他命令\n添加远程版本库：git remote add origin git@github.com:Dajun-2019/Learning\n删除远程仓库：git remote rm name\n修改仓库名：’git remote rename old-name new_name’\n\n\n\n\n拉取：git fetch origin master:temp（将远程仓库（origin为别名）的master分支的代码下载到本地分支上面）\n\n拉去代码并合并：$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;\n\n上传远程代码并合并：git push -u origin master，在git中，“push -u”的意思是将本地的分支版本上传到远程合并，并且记录push到远程分支的默认值；当添加“-u”参数时，表示下次继续push的这个远端分支的时候推送命令就可以简写成“git push”\n\n查看状态（git status）：查看仓库当前状态，显示有变更的文件。git status -s简介输出\n\n分支操作\n\n创建分支：git branch (branch name)\n\n查看分支：git branch\n\n切换分支：\ngit checkout (branch name)\n\n\n当执行 git checkout .或者 git checkout -- &lt;file&gt;命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区中的改动。\n当执行 git checkout HEAD .或者 git checkout HEAD &lt;file&gt;命令时，会用 HEAD 指向的 master 分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改动。\n\n\n删除分支：git branch -d (branch name)\n\n合并分支：git merge （branch name）（branch name会被合并到主分支）\n\n\n\n\n\n\n3.实践\nPull Request流程\n\n把别人的代码fork到自己的仓库\n\n在自己的仓库上修改后的分支，摁下New pull request按钮\n\n\n这时，会进入一个新页面，有Base 和 Head 两个选项。Base 是你希望提交变更的目标，Head 是目前包含你的变更的那个分支或仓库\n\n\n填写说明，帮助别人理解你的提交，然后按下”create pull request”按钮即可\n\ngit am用于将一个patch文件，合并进入当前代码（Github 对每个 PR 会自动生成一个 patch 文件），可以下载该文件，合并进本地代码，就可以在本地查看效果了\n\nGitHub Flow协作流程\n\n克隆 / Fork 仓库：git clone …\n创建分支: git branch …\n修改代码:\n发起 Pull Request: git\nCode Review\nMerge 分支\n删除分支\n\n\n\n\n本地仓库推送到github上的流程\n\n确保Github账户上已经添加了本机的SSH key\n在Github上创建一个新的仓库，并且复制对应链接\n切换到本地项目所在目录，执行git init初始化一个本地仓库\n关联远程库：git remote add origin &lt;https://github.com/&gt;...\n进行代码合并，将远程代码下载到本地：git pull --rebase origin master\n将本地项目添加到本地仓库\ngit add .\ngit commit -m &quot;提交信息&quot;\ngit status\n\n\n推送master分支的所有内容：git push -u origin master\n去github上检查仓库是否提交成功\n\n\nGit用过吗?Git上有敏感数据吗?比如说docker-compose或者是脚本文件中有敏感信息,你怎么来管理?\n通过本地配置文件,比如说docker-compose中配置密钥资源的时候,将敏感数据部署在物理机本地,然后在文件内部通过引用的方式来引用这个密钥,这样的话就可以避免密钥上传到git上,导致泄露,还有的话就是可以通过合理配置权限,具体来说就是在docker-compose的目录中配置一个.ini文件,然后将键值对,例如MYSQL_PASSWORD=123456填入到这个文件中,然后在docker-compose中通过$MYSQL_PASSWORD来引用这个数据,因此docker-compose这些要上git的数据就不会部署到git上了\n\n\n5.Docker1.概念\ndocker 架构（通过docker run hello-world可以展示详细工作流程）\n\n\n命令行工具docker实际上是一个client，他会与Docker Engine里的后台服务Docker daemon通信，镜像存储在远端的Registry里，客户端不能直接访问镜像仓库\nDocker client可以通过build、pull、run等命令向Docker daemon发送请求，而Docker daemon则负责从远端拉去镜像、在本地存储镜像、从镜像生成容器、管理容器等功能\n容器化的应用：指应用程序不再直接和操作系统打交道，而是封装成镜像，再交给容器环境去运行。镜像就是静态的应用容器，容器就是动态的应用镜像\n\n\n隔离怎么实现的\n\n其实奥秘就在于 Linux 操作系统内核之中，为资源隔离提供了三种技术：namespace、cgroup、chroot，虽然这三种技术的初衷并不是为了实现容器，但它们三个结合在一起就会发生奇妙的“化学反应”\nnamespace 是 2002 年从 Linux 2.4.19 开始出现的，和编程语言里的 namespace 有点类似，它可以创建出独立的文件系统、主机名、进程号、网络等资源空间，相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离\ncgroup 是 2008 年从 Linux 2.6.24 开始出现的，它的全称是 Linux Control Group，用来实现对进程的 CPU、内存等资源的优先级和配额限制，相当于给进程的小板房加了一个天花板\nchroot 的历史则要比前面的 namespace、cgroup 要古老得多，早在 1979 年的 UNIX V7 就已经出现了，它可以更改进程的根目录，也就是限制访问文件系统，相当于给进程的小板房铺上了地砖\n\n\n镜像的内部机制\n\n分层（Layer）：容器镜像内部并不是一个平坦的结构，而是由许多的镜像层组成的，每层都是只读不可修改的一组文件，相同的层可以在镜像之间共享，然后多个层像搭积木一样堆叠起来，再使用一种叫“Union FS 联合文件系统”的技术把它们合并在一起，就形成了容器最终看到的文件系统\n\n如果某两层有文件同名，则只能看到上层的文件，下层的就被屏蔽了，可以使用docker inspect nginx:alpine来查看镜像的分层信息（RootFS部分）\n在拉取镜像的时候，只会拉取缺少的层；在删除镜像的时候，只会删除没有未被共享的层\n\n\n\n容器与外部互联互通\n\n文件\n\ndocker cp+源路径+目标路径 命令可以在容器和主机之间互相拷贝文件，适合简单的数据交换，其中目标路径需要用容器名/ID来指明是那个容器的路径，示例：docker cp a.txt 062:/tmp\ndocker run -v命令可以让容器和主机共享本地目录，免去了拷贝操作，提升工作效率，示例：docker run -d --rm -v /tmp:/tmp redis，格式为宿主机路径: 容器内路径，把本机的/tmp路径挂载道容器里的/tmp目录，常用于不同环境运行相同文件\n\n\n网络\n\n网络分类\n\nhost 网络模式让容器与主机共享网络栈，效率高但容易导致端口冲突，docker run -d --rm --net=host nginx:alpine\nbridge 网络模式实现了一个虚拟网桥，容器和主机都在一个私有网段内互联互通，默认使用此模式\n\n\n\ndocker 有一个连接系统允许将多个容器连接在一起，共享连接信息。docker连接会创建一个父子关系，其中父容器可以看到子容器的信息\n\n新建网络：docker network create -d bridge test-net\n\nd：指定Docker网络类型，可以是bridge、overlay（用于Swarm）\n\n\n连接容器\n\n运行一个容器并连接到新建的 test-net 网络:\n$ docker run -itd --name test1 --network test-net ubuntu &#x2F;bin&#x2F;bash\n打开新的终端，再运行一个容器并加入到 test-net 网络:\n$ docker run -itd --name test2 --network test-net ubuntu &#x2F;bin&#x2F;bash\ntest1和test2可以互相ping通，二者建立了互联关系，如果有多个容器，推荐使用Docker Compose\n\n\n\n\n\n网络端口映射：docker run -p命令可以把主机的端口号映射到容器的内部端口号，解决了潜在的端口冲突\n\nP：是容器内部端口随机映射到主机的端口\n$ docker run -d -p 5000:5000 training&#x2F;webapp python app.py\n33e4523d30aaf0258915c368e66e03b49535de0ef20317d3f639d40222ba6bc0\np：是容器内部端口绑定到指定的主机端口，还可以附加绑定网络地址\n$ docker run -d -p 127.0.0.1:5000:5000&#x2F;udp training&#x2F;webapp python app.py\n6779686f06f6204579c1d655dd8b2b31e8e809b245a97b2d3a8e35abe9dcd22a\n\n\n\n\n\n\n\n\n2.常用操作\n安装docker服务\n\n安装docker：sudo apt install -y docker.io\n\n启动docker服务：sudo service docker start\n\n当前用户加入docker组：\nsudo usermod -aG docker $&#123;USER&#125;\n\n，\n\n因为操作 Docker 必须要有 root 权限，而直接使用 root 用户不够安全\nDocker官方推荐将当前用户加入 Docker 用户组\n执行完成之后，还需要退出系统（命令 exit ），再重新登录一次，这样才能让修改用户组的命令 usermod 生效\n\n\n验证docker是否安装成功：docker version和docker info\n\n\n\ndocker使用\n\n\n\n查看镜像/容器：\n\n查看运行的容器：\ndocker ps\n\n\nCONTAINER ID: 容器 ID；IMAGE: 使用的镜像；COMMAND**:** 启动容器时运行的命令；CREATED: 容器的创建时间\nSTATUS: 容器状态，主要有7种状态：created（已创建）、restarting（重启中）、running 或 Up（运行中）、removing（迁移中）、paused（暂停）、exited（停止）、dead（死亡）\nPORTS: 容器的端口信息和使用的连接类型（tcp\\udp）；NAMES: 自动分配的容器名称\n\n\n查看运行完毕的容器：docker ps -a\n\n查看已存储镜像的信息：\ndocker images\n\n\nREPOSITORY：表示镜像的仓库源，通过REPOSITORY:TAG来定义不同的镜像；TAG：镜像的标签，同一仓库可以有多个TAG，代表这个仓库的不同个版本；IMAGE ID：镜像ID；CREATED：镜像创建时间；SIZE：镜像大小\n\n\n可以通过docker logs 容器ID来查看容器内的标准输出\n\n查找镜像：\ndocker search 镜像名\n\n\nNAME: 镜像仓库源的名称；DESCRIPTION：镜像的描述；OFFICIAL: 是否 docker 官方发布；stars: 类似 Github 里面的 star；AUTOMATED: 自动构建\n\n\n\n\n拉取镜像：\ndocker pull busybox\n\n\n获取一个打包了的busybox应用的镜像，里面固化了busybox程序和它所需的完整运行环境\n镜像的完整名字由两个部分组成：名字:标签，标签（tag）是为了区分不同版本的应用而做的额外标记，可以是任意字符串，默认标签是latest\n\n\n启动镜像：\ndocer run+参数+镜像名&#x2F;ID+运行命令\n\n（通过\ndocker run --help\n\n来查看帮助信息）\n\n启动并执行echo输出字符串：docker run busybox echo hello world，提取镜像里的各种信息，运用namespace、cgroup、chroot技术创造出隔离环境，然后运行busybox的echo命令，输出字符串\n\n启动镜像，离开当前操作系统，进入新系统：\ndocker run -it arm64v8&#x2F;ubuntu &#x2F;bin&#x2F;bash\n\n，\nit\n\n表示开启一个交互式操作的Shell，可以直接进入容器内部，通过exit命令或者Crtl+D退出\n\nt: 在新容器内指定一个伪终端或终端\ni: 允许对容器内的标准输入 (STDIN) 进行交互\n\n\n后台运行：docker run -d --name red_srv redis，d表示让容器在后台运行，用于启动服务器程序；-name可以为容器起一个名字，否则会分配一个随机的名字\n\n可以通过IMAGE ID进行短路操作，仅使用前几位就可定位到对应容器\n\ndocker run -d --rm redis：-rm参数告诉Docker不保存容器，再运行完毕后就自动清除\n\n\n\n删除镜像：\ndocker rmi redis\n\n\ndocker rm ID：只会删除容器，不会删除镜像\n\n\n停止容器：\ndocker stop CONTAINER ID\n\n\n只能通过docker ps -a查看已停止运行的容器，通过docker start ID再次启动运行\n\n\n运行的容器中在执行另一程序：\ndocker exec -it red_srv sh\n\n，登陆进入Redis容器，方便查看服务的运行状态和日志。效果同docker run，但因为容器已存在，所以不会创建新容器\n\n进入容器：通过docker exec -it 容器ID /bin/bash来进入后台运行的容器，并且推出时不会导致容器终止，docker attach 容器ID会在退出时终止容器\n\n\n导出和导入容器\n\n导出本地某个容器：docker export 容器ID &gt; ubuntu.tar\n从容器快照文件中再导入为镜像：cat docker/ubuntu.tar | docker import - test/ubuntu:v1\n\n\n\n\n创建容器\n\nDockerfile：一个纯文本，里面记录了一些列的构建指令，比如选择基础镜像、拷贝文件、执行脚本等，每个指令都会生成一个Layer，而Docker顺序执行这个文件的所有步骤，最终会创建出一个新的镜像出来\n\nDockerfile内部指令（不区分大小写，一条指令生成一个镜像）\n\nFROM：所有Dockerfile都要从它开始，表示选择构建使用的基础镜像，相当于打地基，如果关注镜像的安全和大小，一般选择Alpine；如果关注运行的稳定性，则可以选择Ubuntu、CentOS、Debian\nCMD：制定docker run启动容器时默认运行的命令\nCOPY：需要把开发测试产生的一一些源码、配置等文件打包进镜像里，拷贝的源文件必须是构建上下文路径，不能随意指定文件，也就是说，必须把这些文件放在一个专门的目录，然后再docker build里指定构建上下文到这个目录才行\nRUN：执行任意的Shell命令，实现任意的镜像构建步骤，所有RUN指令会在每行的末尾使用续行符\\\\，命令之间也会用&amp;&amp;来连接，这样保证在逻辑上是一行（可以把这些Shell集中到一个脚本文件，然后用COPY命令拷贝进去在用RUN来执行）\nARG：用于创建变量，创建的变量只在镜像构建过程中可见，容器运行时不可见\nENV：用于创建变量，创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用\nEXPOSE：用来声明容器对外服务的端口号，对现在基于Node.js、Tomcat、Nginx、Go等开发的微服务系统来说非常有用\n\n\n构建镜像：docker build -f Dockerfile.busybox .，-f指定Dockerfile文件名，后面必须跟一个文件路径，叫做构建上下文（build‘s context），这里只是一个简单的点号，表示当前路径\n\n新的镜像暂时没有名字（&lt;none&gt;），可以直接使用ID来查看或运行，可以通过t参数来指定镜像的标签（tag），名字需要符合规范，用:分割名字和标签\n\n构建上下文：docker客户端只是把构建上下文目录打包上传，这样服务器才能获得本地的这些文件，就是指定了要打包进镜像的一些依赖文件\n\n为了避免目录中某些不必要文件（例如 readme/.git/.svn 等）拷贝进镜像，可以在构建上下文目录里再建立一个 .dockerignore 文件，语法与 .gitignore 类似，排除那些不需要的文件\n$ cat Dockerfile #每一指令都会在镜像上创建一个新的层，每一指令的前缀都必须是大写\nFROM    centos:6.7 #指定使用哪个镜像源\nMAINTAINER      Fisher &quot;fisher@sudops.com&quot;\n#RUN 指令告诉docker 在镜像内执行命令，安装了什么\nRUN     &#x2F;bin&#x2F;echo &#39;root:123456&#39; |chpasswd\nRUN     useradd runoob\nRUN     &#x2F;bin&#x2F;echo &#39;runoob:123456&#39; |chpasswd\nRUN     &#x2F;bin&#x2F;echo -e &quot;LANG&#x3D;\\\\&quot;en_US.UTF-8\\\\&quot;&quot; &gt;&#x2F;etc&#x2F;default&#x2F;local\nEXPOSE  22\nEXPOSE  80\nCMD     &#x2F;usr&#x2F;sbin&#x2F;sshd -D\n\n\n更新已有镜像并提交：-m提交的描述信息、-a镜像作者、e218edb10161容器ID、runoob/ubuntu:v2镜像名。可以使用dockr images来查看新镜像\n$ docker commit -m&#x3D;&quot;has update&quot; -a&#x3D;&quot;runoob&quot; e218edb10161 runoob&#x2F;ubuntu:v2\nsha256:70bf1840fd7c0d2d8ef0a42a817eb29f854c1af8f7c59fc03ac7bdee9545aff8\n\n\n\n3.实践\nDocker容器引擎\n\n","slug":"Tools","date":"2023-07-30T02:37:14.000Z","categories_index":"","tags_index":"","author_index":"Dajunnnnnn"},{"id":"0b1381c4a63c09e41167c5168339035a","title":"JVM","content":"JVM1.编译执行\n\n\n\n\n\n\n\n\n如何实现跨平台：通过不同平台的JVM实现来将相同的一个Class文件翻译成适配不同机器的机器语言用于执行，即使打包成可还行文件仍需要JVM的支持\n\nJava程序的执行过程\n\nJAVA源代码编译成字节码；字节码校验并把JAVA程序通过类加载器加载到JVM内存中，在加载到内存后针对每个类创建Class对象并放到方法区；字节码指令和数据初始化到内存中；\n找到main方法，并创建栈帧；初始化程序计数器内部的值为main方法的内存地址；\n程序计数器不断递增，逐条执行JAVA字节码指令，把指令执行过程的数据存放到操作数栈中（入栈），执行完成后从操作数栈取出后放到局部变量表中，遇到创建对象，则在堆内存中分配一段连续的空间存储对象，栈内存中的局部变量表存放指向堆内存的引用；遇到方法调用则再创建一个栈帧，压到当前栈帧的上面。\n\n\n编译链接：前端编译、类加载、解释执行、JIT编译执行\n\n\n前端编译：将.java文件编译成.class文件，由javac编译器来完成，通过词法分析、语法分析、语义分析等方法将源代码翻译成字节码，并且还包括特有的注解处理、解语法糖操作\n\n注解处理：JDK6开始，可以开发注解插件，前端编译会调用注解插件如Lombok根据@getter、@setter为类的变量生成方法\n\n解语法糖：Java一开始就注重开发效率，所以Java很适合做业务系统开发，C/C++更适合做底层的系统级开发，为了提高效率，通过对基本语法进行二次封装，来提高易用性，在前端编译的时候会还原为基本语法\n\n泛型：类型擦除，只用于编译时的类型检查\n\n自动拆装箱：方便基本类型和包装类的互相转换，底层使用valueof、xxxvalue实现\n\nfor-each循环：底层依赖迭代器遍历\n\n匿名内部类：单独生成一个class文件，名字由JVM指定如下\npublic class A&#123;&#x2F;&#x2F;A.class\n  public class B&#123;&#125;&#x2F;&#x2F;内部类，编译为A$B.class\n  public void f()&#123;\n    Thread t1 &#x3D; new Thread(new Runnable()&#123;&#x2F;&#x2F;匿名内部类，编译为A$1.class\n      @Override\n      public void run()&#123;\n        System.out.println(&quot;anonymous inner class.&quot;);\n      &#125;\n    &#125;);\n  &#125;\n&#125;\n\n\n\n\n类加载\n\n在Java应用中，类的字节码是按需加载到内存中的，当第一次创建某个类的对象，或调用某个类的方法时，会将这个类加载到内存中，之后便一直保存在内存中\n类加载过程包括验证、准备、解析、初始化等步骤，类的加载遵从双亲委派机制，不同的类有不同的classLoader加载器来加载\n\n\n解释执行\n\n对于C/C++，代码会被事先编译出机器指令（可执行文件），然后再交由CPU来执行；对于Java来说，编译出的.class文件，需要由JVM逐条取出，边解释为机器码，边交由CPU执行\n比如说在使用Demo类对象的创建语句时，如果内存中没有Demo类的字节码信息，会通过类加载器在classpath对应的路径下查找Demo.class，并将其加载到内存中，后续虚拟机根据对象demo中的类指针，找到内存中的Demo类，然后在类的方法中找函数对应的字节码，逐句解释执行\n\n\nJIT编译执行\n\n解释执行的效率比较低，所以引入JIT（Just-In-Time）编译执行，对于一些热点代码，可以将其编译为机器码并存储下来，跳过解释执行\nAOT（Ahead Of Time Compile）编译，运行前编译，类似于C/C++的编译（移植性由程序员来运行），但是仍支持一次编写，到处运行的特点，代码的可移植性有AOT编译器来负责\n\n\n\n\nJIT编译：JIT编译器及JVM运行模式（Client、Server）、分层编译（解释执行、不带/部分/所有编译优化的Client、Server）、热点探测（优化的对象是方法方法，调用计数器、回边计数器、阈值、调用计数器的热度衰减机制）\n\nJIT编译器\nHotSpot支持两种JIT编译器\nClient编译器（C1编译器）：只进行局部的编译优化，编译时间短，编译优化程度低\nServer编译器（C2编译器）：进行局部和全局优化，编译时间长，编译优化程度高\n\n\n有两种JIT编译器引出JVM有两种运行模式，Client模式和Server模式，对于长时间运行的服务器程序，可以使用Server模式\n\n\n分层编译\nJava7之前只能通过参数指定，但是Java7引入了分层编译的技术，JVM可以根据代码、运行情况来选择不同的编译类型，主要分为5个层级（见下），Java8开始默认开启分层编译技术，关闭时直接使用Server编译器\n解释执行\n使用不带编译优化的Client编译器\n使用仅带部分编译优化的Client编译器\n使用带有所有编译优化的Client编译器\n使用Server编译器\n\n\n\n\n热点探测\n热点代码：主要包括被多次执行的方法和被多次执行的循环，JIT编译的对象是方法，对循环的编译是循环编译循环所在方法\n计数器：HotSpot使用计数器来统计方法或循环的执行次数，分别为方法调用计数器（方法的执行次数）和回边计数器（方法内循环的执行次数）\n阈值：调用计数器和回边计数器的总和超过阈值，JVM就会进行JIT编译\nClient编译器：1500\nServer编译器：10000\n分层编译：动态阈值，根据当前编译方法数以及编译线程数动态计算得到\n\n\n热度衰减机制：防止因为运行时间长而超过阈值的代码被判定为成热点代码，此机制（通过-XX:-UseCounterDecay开关）在超过一定的时间（通过-XX:CounterHalfLifeTIme设置）限制之后，如果某个方法没有达到触发JIT编译的阈值要求，那么这个方法的方法计数器的值就减半，回边计数器不存在此机制\n\n\n\n\n编译优化：JIT编译优化策略（方法内联、逃逸分析、无用代码消除、循环展开、消除公共子表达式、范围检查消除、空值检查消除）\n\n减少无效、冗余代码，以便生成高效的机器码\nJIT编译优化策略\n方法内联：达到阈值的方法嵌入、final方法触发内联\n将短小的函数嵌入到函数调用处，通过内存的增加减少时间的消耗\n内联要求：\n函数短小（字节码小于325字节）+调用次数（大于等于100次）\n字节码（小于35字节）+方法调用次数（少于100次）\n\n\nfinal：final方法会触发方法内联，特别是多态的情况下，因为final声明的函数不会被重载，所以可以直接通过类的方法来内联，而不用分析是否有重载变化函数的情况\n\n\n逃逸分析：栈上分配（将对象分配到栈上）、标量替换（使用基本类型替换对象的成员变量）、锁消除（去掉无多线程并发访问的代码）\nJIT编译器通过分析对象的使用范围来优化对象的内存存储方式和访问方式，针对不同的逃逸分析结果，有三种策略\n栈上分配：编译器分析完，发现某个对象使用范围仅限于某个函数内部（没有逃逸到方法外），就可以启动栈上分配编译优化，将对象作为局部变量直接分配在栈上\n标量替换：如果某个对象只在某个函数内使用，并且函数内只访问对象的基本类型成员变量等标量数据，就可以使用基本类型变量替代对象\n锁消除：对不存在多线程并发访问的代码（逃逸到线程外），编译器会去掉其中保证线程安全的加锁逻辑\n\n\n\n\n\n\n\n\n类文件（class文件）结构详解\n\nclass文件由ClassFile结构定义，类似于C语言的结构体\nClassFile &#123;\n    u4             magic; &#x2F;&#x2F;Class 文件的标志，确定这个文件是否是一个能被虚拟机接收的class文件\n    u2             minor_version;&#x2F;&#x2F;Class 的小版本号\n    u2             major_version;&#x2F;&#x2F;Class 的大版本号\n    u2             constant_pool_count;&#x2F;&#x2F;常量池的数量\n    cp_info        constant_pool[constant_pool_count-1];&#x2F;&#x2F;常量池，计数器从1开始，0项代表不引用任何一个常量池选项\n    u2             access_flags;&#x2F;&#x2F;Class 的访问标记，标识class是类还是接口，是否为public或者abstract，是否声明final\n    u2             this_class;&#x2F;&#x2F;当前类\n    u2             super_class;&#x2F;&#x2F;父类 Object的父类索引为0\n    u2             interfaces_count;&#x2F;&#x2F;接口\n    u2             interfaces[interfaces_count];&#x2F;&#x2F;一个类可以实现多个接口\n    u2             fields_count;&#x2F;&#x2F;Class 文件的字段属性\n\n\t\t&#x2F;&#x2F;字段作用域、字段的名称、字段和方法的描述符、字段的额外属性、具体属性具体内容\n    field_info     fields[fields_count];&#x2F;&#x2F;一个类可以有多个字段，用于描述接口或类中声明的变量\n    u2             methods_count;&#x2F;&#x2F;Class 文件的方法数量\n\n\t\t&#x2F;&#x2F;方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项\n    method_info    methods[methods_count];&#x2F;&#x2F;一个类可以有个多个方法\n    u2             attributes_count;&#x2F;&#x2F;此类的属性表中的属性数\n    attribute_info attributes[attributes_count];&#x2F;&#x2F;属性表集合\n&#125;\n示例\n\n\n\n\n类卸载\n\n卸载类需要满足 3 个要求：\n该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象\n该类没有在其他任何地方被引用\n该类的类加载器的实例已被 GC\n\n\n所以，在 JVM 生命周期内，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的\nJDK 自带的BootstrapClassLoader、ExtClassLoader、AppClassLoader负责加载 JDK 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的\n\n\n\n2.类加载器\n类加载：类加载过程（验证、准备、解析、初始化）、类加载器（启动类加载器、扩展类加载器、应用程序类加载器）\n\n类加载过程：JVM将类的二进制字节码加载到内存中，以便创建类的对象或者执行类上的方法\n\n验证：验证所加载的类字节码格式是否符合JVM规范，防止被恶意篡改，由四个检验阶段组成\n\n\n准备：虚拟机为类的静态变量分配内存，并初始化为默认值\n\n对于static final修饰的静态常量，直接初始化指定值\n对于只有static修饰的变量，初始化为默认值（0、null、false等）而不是代码指定值，指定值会在下面的初始化阶段赋予\n\n\n解析：解析类似C++中的链接，把类字节码的常量池中的符号引用（间接引用）转换为直接引用\n\n解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行\n\n\n初始化：虚拟机执行静态变量的初始化代码，包括初始化语句（private static int a = 25;）、静态代码块（static &#123; a = 13; &#125;），这一步JVM才开始真正执行类中定义的Java程序代码，只有以下5种情况必须对类进行初始化\n\n当遇到 new、 getstatic、putstatic 或 invokestatic这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。\n当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象。\n当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。\n当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值。\n当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法。\n\n\n使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(&quot;...&quot;), newInstance() 等等。如果类没初始化，需要触发其初始化。\n初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。\n当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。\nMethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类。\n补充：当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化\n\n\n\n\n类加载机制：启动类加载器、扩展类加载器、应用程序类加载器、双亲委派机制\n\nJVM定义的类加载器\n启动类加载器（BootStrap ClassLoader）：负责加载$JAVA_HOME/jre/lib/rt.jar包中的类\n扩展类加载器（Extension ClassLoader）：负责加载$JAVA_HOME/jre/lib/ext目录下的jar包中的类\n应用程序类加载器（Application ClassLoader）：负责加载classpath所指定路径下的其余类\n\n\n双亲委派机制\n\n\n自定义类加载器：继承自ClassLoader类的子类，重写findClass函数，可以指定父类加载器\n\n定义继承自ClassLoader类的子类，重写findClas函数（loadClass方法可以加载指定二进制名称的类）\n&#x2F;&#x2F;从文件系统的绝对路径下读取类的二进制字节码，通过调用CLassLoader的defineClass函数将二进制的\n&#x2F;&#x2F;字节码转化成Class对象，以此来实现一个加载特定路径下的类的加载器\npublic class FileSystemClassLoader extends ClassLoader&#123;\n  private String rootDir;\n\t&#x2F;&#x2F;通过构造函数指定父类加载器\n  public FileSystemClassLoader(String rootDir)&#123;\n    this.rootDir &#x3D; rootDir;\n  &#125;\n\n  @Override\n  protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException&#123;\n\t\t&#x2F;&#x2F;绝对路径\n    String path &#x3D; rootDir + File.separatorChar + name.replace(&#39;.&#39;, File.separatorChar) + &quot;.class&quot;;\n    byte[] bytecode &#x3D; null;\n\t\t\n    try(InputStream input &#x3D; new FileInputStream(path))&#123;\n      ByteArrayOutputStream byteStream &#x3D; new ByteArrayOutputStream();\n      byte[] buffer &#x3D; new byte[4096];\n      int readSize &#x3D; 0;\n\t\t\t&#x2F;&#x2F;读入二进制字节码\n      while((readSize &#x3D; input.read(buffer)) !&#x3D; -1)&#123;\n\t\t\t\t&#x2F;&#x2F;写出到流中\n        byteStream.write(buffer, 0, readSize);\n      &#125;\n\t\t\t&#x2F;&#x2F;写入到字节数组\n      bytecode &#x3D; byteStream.toByteArray();\n    &#125;catch(FileNotFoundException | IOException e)&#123;\n      e.printStackTrace();\n    &#125;\n    if(bytecode &#x3D;&#x3D; null)&#123;\n      throw new ClassNotFoundException(&quot;class name:&quot; + name);\n    &#125;else&#123;\n\t\t\t&#x2F;&#x2F;生成class对象\n      return defineClass(name, bytecode, 0, bytecode.length);\n    &#125;\n  &#125;\n&#125;\npublic class Demo&#123;\n  public static void main(Stringp[] args) throws ClassNotFoundException&#123;\n    ClassLoader classLoader &#x3D; new FileSystemClassLoader(&quot;&#x2F;Users&#x2F;dajunnnnnn&quot;);\n    Class&lt;?&gt; clazz &#x3D; classLoader.loadCLass(&quot;com.code.hello&quot;);\n    System.out.println(clazz.getClassLoader());&#x2F;&#x2F;打印SystemFileClassLoader对象信息\n  &#125;\n&#125;\nClassLoader是一个模版方法模式类，其中的loadClass函数是模版方法，里面包含类加载的整个逻辑，比如双亲委派机制的实现逻辑，findClass函数为模版方法模式中的抽象方法，被loadClass函数使用，用来根据类名查找类\nprotected Class&lt;?&gt; loadClass(String name, boolean resolve)\n  throws ClassNotFoundException\n&#123;\n  synchronized (getClassLoadingLock(name)) &#123;\n    &#x2F;&#x2F; First, check if the class has already been loaded\n    Class&lt;?&gt; c &#x3D; findLoadedClass(name);\n    if (c &#x3D;&#x3D; null) &#123;\n      long t0 &#x3D; System.nanoTime();\n      try &#123;\n        if (parent !&#x3D; null) &#123;\n          c &#x3D; parent.loadClass(name, false);\n        &#125; else &#123;\n          c &#x3D; findBootstrapClassOrNull(name);\n        &#125;\n      &#125; catch (ClassNotFoundException e) &#123;\n        &#x2F;&#x2F; ClassNotFoundException thrown if class not found\n        &#x2F;&#x2F; from the non-null parent class loader\n      &#125;\n\n      if (c &#x3D;&#x3D; null) &#123;\n        &#x2F;&#x2F; If still not found, then invoke findClass in order\n        &#x2F;&#x2F; to find the class.\n        long t1 &#x3D; System.nanoTime();\n        c &#x3D; findClass(name);\n\n        &#x2F;&#x2F; this is the defining class loader; record the stats\n        sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);\n        sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);\n        sun.misc.PerfCounter.getFindClasses().increment();\n      &#125;\n    &#125;\n    if (resolve) &#123;\n      resolveClass(c);\n    &#125;\n    return c;\n  &#125;\n&#125;\n默认情况下自定义类加载器的父类为应用程序类加载器，可以在构造函数中指定自定义类加载器的父类加载器\n\n\n\n\n\n双亲委派机制：用来确定类加载器，定义了类加载器之间的父子关系，从子类找到父类再从父类开始向下搜索\n\n当JVM无法根据全限定名（java.lang.StringUtils）找到路径和对应的类加载器时，虚拟机需要通过在各个类加载器所负责的路径下查找这个类，当有重复的时候，需要机制来确定加载哪一个类，所以虚拟机设计了双亲委派机制\n\n双亲委派机制：定义了类加载器之间的父子关系\n\nClassLoaderB的父类加载器为ClassloaderA（B继承自A）\nClassLoaderA的父类加载器为AppClassloader\nAppClassLoader的父类加载器为ExtClassLoader\nExtClassLoader的父类加载器为null,实际为BootrapClassLoader\n由于其由C++代码实现，因此无法在打印结果中显示\n\n\n\n在某个类加载器接收到某个类的加载请求时（使用new或反射创建类的对象时，默认为请求应用程序类加载器加载对应的类），如果这个类加载器之前没有加载过这个类，那么他便委托父类加载器加载这个类，如果父类没有加载过则继续向上委托直到有类加载器加载了这个类，如果达到最顶层父类加载器还没有的话，就从上往下请求各个类加载器在自己负责的路径下查找并加载这个类\n\n\n双亲委派机制可以有效防止对核心类的恶意修改，比如在自己的路径下定义一个新的java.util.String类，请求应用程序类加载器来加载，意图覆盖核心类库中的String类，但是，基于双亲委派机制，应用程序类加载器会委托父类加载器来加载java.util.String类，最终仍然会由启动类加载器加载核心类库中的String类（因为是从上往下搜索）\n\n\n\n\n3.Java内存区域\n内存分区：方法区（一些不变的信息）、堆（对象，垃圾回收器管理）、程序计数器、虚拟机栈和本地方法栈、直接内存（操作系统管理）\n\n\n方法区：类信息、方法信息、静态变量、运行时常量池、字符串常量池、JIT编译代码缓存\n\n存储内容：类信息（类的全限定名、访问修饰符、符类、接口列表）、方法信息（方法名、修饰符、入参、返回值、访问标志）、静态变量（隶属于类，存在方法区）、运行时常量池（类字节码中的常量池，存储字面量和符号引用）、字符串常量池、JIT编译代码缓存\n\n常量池中每一项常量都是一个表，这 14 种表有一个共同的特点：开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型（.class文件可以通过javap -v class名，来查看常量池中的信息（temp.txt））\n\n\n运行时常量池：存放编译器生成的各种字面量和符号引用\n\n字面量：源代码中的固定值的表示法，包括整数、浮点数、字符串字面量\n符号引用：类/接口符号引用（全限定名）、字符符号引用（名称和描述符）、方法符号引用（名称和描述符）\n符号引用：以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时无歧义地定位到目标即可，与虚拟机内存布局无关，引用他的目标不一定是已经加载到虚拟机内存中的内容\n直接引用：是可以指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄，与虚拟机内存布局直接相关\n\n\n运行时常量池的功能类似于传统编程语言的符号表，但它包含了比典型符号表更广泛的数据。既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。\n\n\n字符串常量池：\n\nHotSpot 虚拟机中字符串常量池的实现是src/hotspot/share/classfile/stringTable.cpp，StringTable本质上是一个HashSet&lt;String&gt;，容量为StringTableSize，可以通过-XX:StringTableSize参数来设置\nStringTable中保存的是字符串对象的引用，字符串对象的引用指向堆中的字符串对象\nJDK1.7 之前，字符串常量池存放在永久代。JDK1.7 字符串常量池和静态变量从永久代移动了 Java 堆中\n原因：主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存\n\n\n\n\n\n\n方法区是一种抽象分区，不同JVM可以有不同的实现方式\n\nJava7之前实现为永久代\nJava7方法区的字符串常量池和静态变量从永久代中移除，放入堆中\nJava7之后永久代被元空间（Metaspace）取代，但字符串常量池和静态常量仍存储在堆中\n整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小，可以使用 -XX：MaxMetaspaceSize标志设置最大元空间大小\n元空间里面存放的是类的元数据，这样加载多少类的元数据就不由MaxPermSize控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了\n\n\n\n\n方法区常用JVM参数\n\nJDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n-XX:PermSize&#x3D;N &#x2F;&#x2F;方法区 (永久代) 初始大小\n-XX:MaxPermSize&#x3D;N &#x2F;&#x2F;方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是本地内存。下面是一些常用参数\n-XX:MetaspaceSize&#x3D;N &#x2F;&#x2F;设置 Metaspace 的初始（和最小大小）\n-XX:MaxMetaspaceSize&#x3D;N &#x2F;&#x2F;设置 Metaspace 的最大大小\n\n\n\n\n程序计数器：线程私有的，PC寄存器是线程共享的\n\n虚拟机相当于一个抽象的计算机，也有自己的指令集（字节码集），因此也需要一个存储单元（程序计数器）用来存储下一条要执行的字节码的地址\n与PC寄存器不同的地方在于：PC寄存器是线程共享的，PC寄存器会随着线程的切换而进行保存和恢复，程序计数器是线程私有的，每个线程都会分配一个独立的程序计数器，记录当前线程执行到哪一行字节码（因为程序计数器位于内存而不是CPU，资源丰富，线程独享能减少线程上下文切换的信息量，有利于提高线程切换的速度）\n\n\n堆：用来存储Java对象，在Java中对象的回收是有虚拟机中的垃圾收集器自动完成的，堆是垃圾收集器的主要工作分区\n\n虚拟机栈：函数调用中，主要用栈存储函数的局部变量、参数、返回地址等信息。栈是线程私有的，每个线程会有一个栈，因此也叫线程栈。Java中的栈叫做虚拟机栈\n\n栈帧中主要有以下几方面\n局部变量表：存放了编译器可知的各种数据类型、对象引用，分为若干slot，第一个slot存this，其余先是方法参数列表，然后是方法内创建的变量（int、boolean、char、Object这种都只占一个slot，如果遇到long或者double类型的，则占用两个slot来存储）\n操作数栈：主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间结算结果或临时变量\n动态链接：主要服务一个方法需要调用其他方法的场景，常量池中保存了大量的符号引用（方法引用的符号引用），当一个方法调用其他方法时，需要将常量池中指向方法的符号引用转化为其内存地址的直接引用，这个过程就是动态链接\n方法返回地址\n\n\nreturn和抛出异常都会导致栈帧被弹出\n程序运行中栈可能会出现的两种错误\nStackOverFlowError： 若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误\nOutOfMemoryError： 如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常\n\n\n\n\n本地方法栈：服务于native方法（C/C++）的栈，HotSpot中，两个栈被合并为一个栈\n\nJava提供了很多使用C/C++实现的native方法，在JVM规范中，Java将服务于Java方法调用的栈，跟服务于native方法调用的栈做了区分，服务于Java方法调用的栈称为虚拟机栈，服务于native方法调用的栈称为本地方法栈\n两个栈的功能相同，在具体的虚拟机实现中，如HotSpot中，把两栈合并为一个栈，同时存储Java方法调用的栈帧和native方法调用的栈帧\n默认线程栈的大小：不同平台下有区别，HotSpot默认的每个线程的栈大小为1MB（可更改）\n\n\n直接内存：在本地内存分配，直接受操作系统管理，减少垃圾回收对程序的影响，用于NIO基于通道和缓存区的I/O方式，\n\n直接内存是一种特殊的内存缓冲区，并不在 Java 堆或方法区中分配的，而是通过 JNI 的方式在本地内存上分配的。不是虚拟机运行的一部分，但是也会导致OutOfMemoryError错误出现\nJDK1.4 中新加入的NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据\n堆外内存就是把内存对象分配在堆（新生代+老年代+永久代）以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响\n\n\n\n栈内存\n\n堆内存\n\n在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：新生代内存(Young Generation)、老生代(Old Generation)、永久代(Permanent Generation)；JDK8版本之后，PermGen（永久）已被Metaspace（元空间）取代，元空间使用的是本地内存\n\n\n\n\n\n\nJava内存模型\n\n工作内存和主内存：Java 内存模型规定所有的变量都存储在主内存中，每个线程都有自己独立的工作内存，工作内存保存了对应该线程使用的变量的主内存副本拷贝。线程对这些变量的操作都在自己的工作内存中进行，不能直接操作主内存和其他工作内存中存储的变量或者变量副本。线程间的变量传递需通过主内存来完成，并且定义了定义了 8 种操作来完成主内存和工作内存的变量访问，三者的关系如下图所示。\n\n\nJava内存模型的三大特性\n\n原子性：Java 内存模型直接保证的原子性操作包括 read、load、use、assign、store、write、lock、unlock\n可见性：可见性是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改\n实现方式：通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式\nvolatile：volatile 的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新，但是它并不能保证互斥性，也就是说多个线程并发修改某个变量时，依旧会产生多线程问题，一般用在一个线程写，其他线程读的场景\nsynchronized：由 “对一个变量执行 unlock 操作 之前，必须先把此变量同步回主内存中（执行 store、write 操作）” 这条规则获得\n\n\n有序性：单线程没有问题，多线程下由于指令重排，并发执行的正确性会受到影响（通过volatile和synchronized解决）\nvolatile 通过加入内存屏障指令来禁止内存的重排序，\nsynchronized 通过加锁，保证同一时刻只有一个线程来执行同步代码\n\n\n\n\n\n\n\n4.垃圾回收\n可达性分析：判断哪些对象可以被回收，引用计数、STW、安全点、安全区\n\n引用计数：同可达性分析类似的判断对象是否可以被回收的算法，但是存在问题（无法检测循环引用，都为null计数不为0）不被JVM采用\n\n可达性分析：GC Roots、遍历（BFS、DFS）\n\n\n用有向图表示对象之间的引用关系，图中定点表示对象，有向边表示引用\nGC Roots：堆外变量所直接引用的堆内对象，包括虚拟机栈、本地方法栈中的局部变量所直接引用的对象，方法区中静态变量所直接引用的对象等\n虚拟机以GC Roots为起点，遍历（BFS或DFS）整个图，可以遍历到的对象为可达对象，遍历不到的对象被虚拟机当作垃圾回收\n\n\nSTW：运行在虚拟机上的用户线程和垃圾回收线程是并行的，垃圾回收是对象会被用户线程更改，可达性分析结果会有误报/漏报的情况\n\n误报：非存活对象误报为存活对象，等待再次垃圾回收\n漏报：漏报存活对象，从而将其判定为死亡对象。会产生严重的问题，导致本不该被回收的对象被回收，从而导致程序出错，解决此问题最简单的方法就是STW（Stop The World），即停止所有用户线程的执行，知道垃圾回收结束，因此会影响程序性能\n\n\n安全点：使用OopMap动态更新GC Roots，JVM为某个指令记录OopMap，这个指令就叫～\n\nGC Roots的获得方式：遍历栈中的局部变量和方法区中的静态变量，找出引用类型变量，然后，再将引用类型变量所引用的对象放入GC Roots中\n\n虚拟机使用OopMap来存储当前的GC Roots并动态更新，减少遍历耗时\n\n遍历查找GC Roots，初始化OopMap，在代码执行过程中，变量更新引用时，同步更新OopMap\n因为JIT编译后的机器码直接交由CPU执行，所以需要在编译成机器码之前静态分析指令，存储指令执行后的OopMap，浪费内存\n\n\n安全点：为了节约空间，虚拟机采用了时间换空间的策略，将为每个指令存储一个OopMap，改为只选取部分指令存储OopMap。这些被选取的指令称为安全点，当虚拟机启动垃圾回收并需要STW时，会向用户线程发送暂停的中断请求，此时，用户线程并不能立刻停止，而是需要运行到安全点之后才能停止，因为只有安全点处才记录了OopMap，只有所有线程都运行到安全点之后，虚拟机才能得到完整的GC Roots\n\n\n\n\n安全区：不会改变对象引用关系的一段连续的代码区间，安全区代码可以和垃圾回收线程并行执行\n\n大部分情况，用户线程在接收到暂停的中断请求之后，都可以在较短的时间内达到最近的安全点，但是在少数情况下，如果用户线程处于阻塞状态（如等待I/O读写就绪），就无法在较短的时间内达到最近的安全点，为了解决这个问题，虚拟机引入了一个新的概念：安全区，即不会改变对象引用关系的一段连续的代码区间\n当虚拟机执行垃圾回收并发起STW请求时，如果某个线程处于安全区，那么，这个线程并不需要停止执行，而是可以跟垃圾回收线程并行执行。但是，当用户线程离开安全区时，他需要检查虚拟机是否处于STW状态，如果是，用户线程需要阻塞等待STW结束，才能继续往下执行，以免用户线程跳出安全区之后，执行非安全代码导致对象引用关系的改变\n\n\n\n\n垃圾回收算法：基础垃圾回收算法（标记-清除、标记-整理、标记-复制）、分代垃圾回收算法（年轻代和YoungGC、老年代和FullGC）\n\n概念：用于回收死亡对象，主流算法为分代垃圾回收算法，JVM将堆空间分为年轻代和老年代，针对不同的分代单独进行垃圾回收（YoungGC、FullGC）\n针对年轻代的垃圾回收叫做YoungGC\n针对老年代的垃圾回收叫做FullGC，FullGC比YoungGC慢很多\n\n\n基础垃圾回收：标记使用可达性分析找出需要回收的死亡对象，主要有标记-清除、标记-整理、标记-复制\n标记-清除：虚拟机将死亡对象占用的内存空间释放，放入到空闲链表中，创建对象时，JVM从空闲链表中查找合适的空闲空间分配给对象，会出现内存碎片问题，标记-清除后的空闲空间不连续，查找耗时，大对象无法分配\n标记-整理：有叫做标记-压缩，在标记-清除的基础上，增加了整理的环节，先使用可达性分析标记存活对象所占用的内存空间，然后顺序遍历，将存活对象移动到内存的一端，从而解决内存碎片的问题，只需记录空闲空间的起始地址\n标记-复制：将内存分为轮流使用的两块内存，一块内存为对象分配内存空间（工作内存），另一块内存作为复制时备用（备用内存），当工作内存使用完之后，将这块内存的存活对象复制到备用内存中，然后交换两块内存角色，缺点是内存只用了一半，长时间存活的对象会被来回复制多次\n\n\n分代垃圾回收算法\n不需回收：程序计数器内存小，随着线程创建和销毁，不需要回收；虚拟机栈和本地方法栈存储的是方法的栈帧，随着方法退出而销毁，因此这三部分会随着生命周期的结束而立刻被回收，不需要经过虚拟机的垃圾回收线程的处理\n需要回收\n堆中存储的是对象，需要所有线程共享，作用域范围大，生命周期长，使用完不会立即回收，所以堆是进行垃圾回收的重点\n方法区也会涉及垃圾回收，比如方法区中的一些无用的类（类所有对象被回收、类class对象未被引用、类加载器已卸载）、无用的String常量对象（存储在字符串常量池，却没有变量引用的String对象）\n\n\n对堆的垃圾回收：堆中存储的是对象，对生命周期短的希望以较高频率进行回收；对生命周期长的希望以较低频率进行回收\n分代的垃圾回收：JVM将堆分为年轻代（Young Genereation）和老年代（Old Generation）两个分区，年轻代存储生命周期短的对象，老年来存储生命周期长的对象。JVM针对不同的分代使用不同的基础垃圾回收算法\n\n\n年轻代和YoungGC\n新创建的对象会分配在年轻代，选择标记-复制算法进行回收，因为生命周期短所以只需要复制少量存活对象\n为提高内存利用率，将年轻代分为不均等的三个分区：一个Eden区和两个Survivor区（From Survivor区、To Survivor区）\nEden满了之后，会触发Minor GC，存入survivor0中，如果触发MinorGC时survivor0也是满的，则s0和Eden一起进行可达性分析，找出活跃对象复制到s1区（标记-复制算法），请空Eden和s0，并将s0和s1交换\n对象在第一次进入Survivor，年龄初始化为1，在survivor中每熬过一次MinorGC，年龄就增加1\nEden区和Survivor区会被动态调整，或通过参数-XX:SurvivorRatio来设置比例（Eden区：1个Survivor区）\n\n\n空间分配担保机制：ToSurvivor区存不下的时候，会借用老年代的部分空间（老年代也不够时会执行FullGC，仍不够会抛出OOM），缺点是部分生命周期短的对象存储在老年代，等待很长时间才会被回收\n\n\n老年代和FullGC\n新生代存不下的对象、大对象、长期存活的对象都会进入老年代\n如果设置了JVM参数-XX:PretenureSizeThreshold，当对象大小超过这个阈值，对象会直接在老年代创建\n长期存活的对象指的是经过多次年轻代垃圾回收仍然存活的对象，虚拟机在对象的对象头中记录对象的GC年龄，每经过一次GC，GC年龄就增一，当GC年龄超过一定阈值（默认15，或通过-XX:PretenureSizeThreshold设置）之后，对象便从年轻代移动到老年代，但是人工设置的不准需要动态年龄判断机制\n动态年龄判断机制：统计YoungGC后，处于每个GC年龄值的对象占To Survivor区的比例，如果GC年龄&gt;=X（X取最大值）的对象占To Survivor区的比例超过50%（比例通过-XX:TargetSurvivorRatio来设置），那么GC年龄&gt;=X的对象都将直接进入老年代，不等GC年龄大于15\n\n\n老年代的垃圾回收\n因为老年代中的对象生命周期较长，每次垃圾回收之后，存活对象比较多，所以采用标记-整理算法进行回收（标记-清楚有内存碎片，永久代也使用标记-整理）\n老年代的垃圾回收叫做OldGC，但是在HotSpotJVM中，对老年代进行垃圾回收的同时，虚拟机会一并对年轻代和永久代进行垃圾回收，整个过程叫做FullGC\nYoungGC只对年轻代进行垃圾回收，速度快（可达性分析遍历的对象少，需要复制的对象较少），因此也称为MinorGC，而FullGC针对整个堆进行垃圾回收，速度慢（可达性分析遍历的对象多，垃圾回收处理的对象也多），因此也称为MajorGC\n\n\n\n\n增量（Incremental）算法：是一种渐进式的垃圾收集算法，它将垃圾收集的工作分为多个小部分分别执行，不需要一次性完成所有的垃圾收集工作，从而减少了垃圾收集时程序的暂停时间\n\n\n垃圾回收器：垃圾回收算法的具体实现，同一种算法可以有不同的实现\n\n性能指标：吞吐量（业务运行时间/总运行时间）、停顿时间（STW）、资源消耗（CPU资源、内存资源）、回收延迟、回收频率\n\n四大类常用的垃圾回收器：Serial、Parallel、CMS、G1\n\nSerial垃圾回收器：使用单线程进行垃圾回收，回收时需要STW、针对工作分区有以下两类\n\nSerial New：用于年轻代的垃圾回收，基于标记-清楚算法实现\nSerial Old：用于老年代的垃圾回收，基于标记-整理算法实现\n\n\nParallel垃圾回收器：使用多线程进行垃圾回收，回收是需要暂停程序运行，针对工作分区有以下三类\n\nParallel Scavenge（简称PS）：用于年轻代，基于标记-复制算法，与ParOld配合使用\nParallel New（简称ParNew）：用于年轻代，基于标记-复制算法，与CMS配合使用\nParallel Old（简称ParOld）：用于老年代，基于标记-整理算法\n\n\nCMS垃圾回收器：全称Concurrent Mark Sweep，采用多线程，不暂停应用程序，但不能用于年轻代的垃圾回收（Parallel New）\n\n\n垃圾回收过程（见后）：初始标记（需暂停应用程序）、并发标记、重新标记（需暂停应用程序）、并发清理\nCMS垃圾回收器在应用程序并行执行的过程中会争抢CPU资源，因此CMS使用的并发线程数等于（CPU内核数+3）/ 4，并且需要在老年代未满的时候进行垃圾回收，为并发程序预留内存空间\n预留内存通过JVM参数-XX:CMSInitiatingOccupancyFraction来指定已用内存占老年代的比值，超过此阈值就会触发CMS垃圾回收器的执行\n预留内存空间不够时，转而使用SerialOld垃圾回收器执行本次垃圾回收\n\n\n为了减少STW时间，CMS采用标记-清除算法来实现，相对于标记-整理算法，节省了整理空闲空间的时间，并且CMS针对内存碎片问题进行了改进，即在多次垃圾回收之后进行一次内存碎片的整理\n\n\nG1垃圾回收器：全称Garbage First，是一个应用于堆上的垃圾回收器，借鉴分代的处理思路，将整个堆分为2048个小的Region，并进一步分为年轻代（Eden区、Survivor区）、老年代\n\n\n之前的垃圾回收器都是针对整个分代进行垃圾回收，当分代被划分为更小的区域后，每次垃圾回收时，虚拟机可以只回收分代中的部分区域，进一步缩短STW时间\nG1同CMS类似，都是多线程进行垃圾回收，并且回收的过程与应用程序并发执行，不同的地方是G1垃圾回收器整体使用标记-整理算法、局部（每个Region）使用标记-复制\n因为G1的STW时间可以预测，所以可以通过-XX:MaxGCPauseMillis设置可允许的最大STW时间，G1根据这个时间决定每次对多少个区域进行垃圾回收\n\n\nZGC垃圾回收器\n\n与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。在 ZGC 中出现 Stop The World 的情况会更少\n\n\n\n\n垃圾回收器的对比与选择\n\n默认Java7、Java8采用Parallel垃圾回收器，Java9采用G1垃圾回收器，可以通过设置JVM参数来指定项目使用的垃圾回收器\n\n\n实战建议\n\nSerial：单核系统，多个应用程序争用CPU资源的环境下，需要刻意限制虚拟机所占用资源的环境，比如运行在移动端的客户端程序\nParallel与CMS相比，前者吞吐量更大，后者停顿时间更少，对于离线服务，首选吞吐量达的Parallel垃圾回收器，对于实时服务，特别是对响应时间敏感的服务，首选停顿时间更少的CMS垃圾回收器\nJava9中，CMS被标记为Deprecated，使用G1取代，针对比较大的堆（大于6GB），首选停顿时间可控的G1垃圾回收器\n\n\n\n\n并发垃圾回收\n\n并发：并非完全并发，而是大部分时间不需要暂停应用程序，并发垃圾回收整个过程分为4个阶段，分别是：初始标记、并发标记、重新标记、并发清理，其中并发标记和并发清理这两个比较耗时的阶段可以与应用程序并发执行，而其余两个阶段仍需要暂停应用程序的执行\n回收过程：初始标记指的是标记GC Roots。并发标记指的是在应用程序不暂停的情况下，以GC Roots为起点，广度或深度优先遍历所有可达对象（存活对象），在并发标记的过程中，应用程序有可能修改对象之间的引用关系，导致并行标记过程出现误标或漏标的情况，重新标记所做的工作就是对误标和漏标进行修正。并发清理指的是在不暂停应用情况下，对标记出来的垃圾对象进行清理\n并发清理：前面三个阶段属于可达性分析，即标记-清除算法中的标记环节，并发清理是标记-清除算法中的清除环节。在并发清理过程中，如果存活对象变为死亡对象，只需要在下一次垃圾回收中被回收即可；而死亡对象不会再变成存活对象，因为死亡对象不再有变量（局部变量或静态变量）的直接或间接引用，因此应用程序是无法在代码中使用这些死亡对象（比如局部变量在函数执行结束后就被销毁了）\n\n\n三色标记算法\n\n用于可达性分析，将遍历过程中的对象分为白色、灰色、黑色三种\n白色：对象没有遍历过，遍历开始时，所有对象都初始化白色，遍历结束后，仍为白表示对象不可达\n灰色：对象已经被遍历，但是对象所直接引用的对象还没有完全被遍历\n黑色：对象已经被遍历，并且对象所直接引用的对象都已经被遍历\n\n\n可达性分析基于图的广度和深度遍历\n初始化GC Roots为灰色，其余为白色\n从灰色集合中取出一个灰色对象，标记为黑色，将此对象直接引用的所有白色对象标记为灰色\n重复第二步，直到灰色集合中没有对象为止。此时黑色集合存放的是可达对象，也就是存活对象；白色集合中存在的是不可达对象，也就是死亡对象\n\n\n误标（白的标成黑的）和漏标（黑的标成白的）\n误标：灰色对象的引用变为0，应该变成白的，但是最后变成黑的了\n漏标：白色对象的灰色对象引用断开，新增了黑色对象引用，应该被标成黑的，但是只能被标成白的\n\n\n\n\n增量更新和原始快照\n\n并发标记的误标和漏标问题会在重新标记中解决，其中误标问题不大，是可以接受的，只会导致垃圾对象延迟回收，但是漏标问题会导致应用程序运行出错，回收不该回收的对象，漏标产生的原因主要有以下两点，两者缺一不可\n新增引用：新增一个黑色对象对一个白色对象的引用\n删除引用：删除所有灰色对象到此白色对象的直接或间接引用\n\n\n针对以上两点，Java发明了两种漏标解决方案，针对第一点新增引用的漏标解决方案叫做增量更新，是CMS垃圾回收器所使用的方案；针对第二点删除引用的漏标解决方案叫做原始快照，是G1垃圾回收器所使用的方案，方案具体如下：\n增量更新：在并发标记的过程中，如果应用程序新增了一个黑色对象对一个白色对象的引用，虚拟机会将这个白色对象记录下来，在并发标记完成之后，重新标记阶段会以这些记录下的白色对象为起点，重新进行可达性分析，这样漏标的白色对象会被重新标记为黑色对象\n原始快照：在并发标记的过程中，如果应用程序删除了一个灰色对象对一个白色对象的直接/间接引用，那么虚拟机会将这个白色对象记录下来，在并发标记完成之后，重新标记阶段会以这些记录下来的白色对象为起点，重新进行可达性分析，这就相当于虚拟机对引用关系改变之前的原始快照进行可达性分析。不过，这些记录下的白色对象有可能是死亡对象，而重新标记阶段会将这些死亡对象重新标记为存活对象，因此，原始快照这种解决方案会导致误标问题，会导致垃圾对象延迟回收\n\n\n\n\n\n\n\n5.实战调优\nJVM性能优化\n\n性能指标：GC频率、GC时间，印象以上两项的内部性能指标\n年轻代中对象的增长速率\n每次YoungGC之后存活对象大小\n每次YoungGC之后进入老年代的对象大小\n老年代对象的增长速率\n\n\nJVM参数类别：参数稳定性依次下降\n-标准参数\n-X参数\n-XX参数\n\n\n常用参数：堆大小、年轻代和老年代大小、永久代和元空间大小、Eden区和Survivor区大小、线程栈大小、垃圾回收器类别\n设置堆的大小：一般设置为相同的值，避免堆大小的调整而引起的性能损耗\nXms：Java堆内存的初始大小\nXmx：Java堆内存的最大大小\n\n\n设置年轻代和老年代的大小：设置年轻代大小的方法有三种，但是对于老年代的大小只需要通过堆大小减去年轻代大小即可得到\nXmn：年轻代的大小\nXX:NewSize：年轻代的初始大小\nXX:MaXNewSize：年轻代的最大大小\nXX:NewRatio：年轻代与老年的大小比值，值为老年代/年轻代\n\n\n设置永久代或元空间的大小\nXX:PermSize：永久代的初始大小\nXX:MaxPermSize：永久代的最大大小，这两个参数只在1.7之前有效\nXX:MetaspaceSize：元空间的初始大小\nXX:MaxMetaspaceSize：元空间的最大大小，这两个参数只在1.8之后有效\n\n\n设置Eden区和survivor区的大小\nXX:SurvivorRatio：一个Survivor区跟Eden区的大小比例，值为Eden区/Survivor区，注意：一共有两个Survivor区\n\n\n设置线程栈的大小\nXss：每个线程的栈大小，HotSpot JVM不区分虚拟机栈和本地方法栈，使用一个栈同时存储Java方法和本地方法的栈帧，因此这里只有一个栈大小的设置参数，线程栈大小默认为512KB或1MB，除非系统在运行的过程中，出现非代码因素导致的StackOverflow，才需要调整线程栈的大小，否则默认即可\n\n\n设置垃圾回收器\nXX:+UseSerialGC：用Serial垃圾回收器\nXX:+UseParallelGC：用Parallel垃圾回收器\nXX:+UseConcMarkSweepGC：用CMS垃圾回收器\nXX:+UseG1GC：用G1垃圾回收器\n\n\n\n\n\n\nJVM问题排查：jstat、jmap\n\n常见工具\n\njps (JVM Process Status）: 类似 UNIX 的 ps 命令。用于查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息；\n**jstat**（JVM Statistics Monitoring Tool）: 用于收集 HotSpot 虚拟机各方面的运行数据;\njinfo (Configuration Info for Java) : Configuration Info for Java,显示虚拟机配置信息;\njmap (Memory Map for Java) : 生成堆转储快照;\njhat (JVM Heap Dump Browser) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果;\njstack (Stack Trace for Java) : 生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合\n\n\nJVM性能调优\n\n线上通过jstat得到JVM性能统计数据，JVM调优主要方向是减少FullGC频率和FullGC时间\n增大年轻代的大小，增大Survivor区大小，让对象尽量在年轻代就被回收掉，减少老年代中对象的增长速率，从而降低FullGC频率\n增加老年代的大小也会降低FullGC的频率，但会增大FullGC的时间\n\n\n一般来说，如果堆不是很大，没有长期存活的大对象和内存泄漏，那么应用CMS垃圾回收器并调节年轻代、老年代、Survivor区等内存分配，完全可以将FullGC时间优化到合适的范围，否则可以选择GC时间可控的G1垃圾回收器\n大部分情况，不需要刻意的进行调优，只有当通过监控发现GC严重影响系统性能时，才有必要对JVM参数进行调优\n\n\nJVM性能监控和分析工具\n\njstat：通过jps命令查找到要监控的JVM进程ID，然后执行jstat -gcutil [vmid] [time-interval]即可\n\nS0：表示Survivor0的内存使用率；S1：表示Survivor1的内存使用率；E：表示Eden区的内存使用率；O：表示老年代的内存使用率；M：表示Metaspace的内存使用率；YGC：YoungGC的次数；YGCT：YoungGC的总耗时；FGC：FullGC的次数； FGCT：FullGC的总耗时；GCT：GC的总耗时\n\n\n\nGC详细日志分析\n\n-XX:+PrintGCDetails 打印详细GC日志\n-Xloggc:./logs/gc.log 详细GC日志存储的位置\n以上日志可以粗略的分为两类：ParNew日志和CMS日志\nParNew日志：记录GC触发原因、GC发生时间、GC前后年轻代大小变化、GC具体耗时等信息\nCMS日志示例：CMS日志包含的信息比ParNew日志要多很多，其中，CMS Initial Mark、CMS-cocurrent-mark、CMS Final Remark、CMS-concurrent-sweep分别对应并发垃圾回收的四个阶段：初始标记、并发标记、重新标记、并发清理\n\n\n\n\nJVM内存快照获取和分析：jmap\n\n当JVM出现问题时，比如OOM、频繁GC，我们希望得知当前堆中存储的对象情况，比如哪些对象占据了大量堆内存，我们就需要将当下的内存快照dump出来，然后利用工具来查看和分析\n\n常用的dump堆内存快照的方法有两种，一种是使用JVM参数，另一种是使用jmap命令行工具。具体如下所示。dump出来的堆内存快照为二进制文件，我们需要通过工具来查看，常用的查看工具有MAT、jhat等\n方法一：使用JVM参数\n-XX:+HeapDumpBeforeFullGC\n-XX:HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath&#x3D;目录\n\n方法二：使用jmap命令行工具\njmap -dump:format&#x3D;b,file&#x3D;文件名 [pid]\n\n\n\n\n\n\nJVM常见问题\n\nOOM\n当程序申请不到足够的内存空间，并且JVM通过GC也无法释放出足够的内存空间时，JVM便会抛出OOM\n导致内存溢出的常见的原因有如下几种\n设置的堆或永久代（元空间）的大小太小\n一次性创建过多的对象，比如通过SQL查询全表数据。\n应用程序使用完成的对象没有被及时释放，导致对应的内存无法被回收，长期积累，便会导致内存耗尽。我们把这种情况叫做内存泄露\n\n\n如何排查OOM问题\n当JVM出现OOM问题时，应用程序的对应表现一般是无法继续执行，如果应用程序是接口系统，那么接口将出现大量503错误。这时，我们通过查看日志，便会发现大量java.lang.OutOfMemoryError错误信息。为了排查出到底哪些对象长期存在并大量占用内存，我们需要通过jmap或JVM参数获取堆内存快照，并通过MAT等工具来查看和分析\n使用MAT工具可以得知内存泄漏的数据可能集中在哪些代码，然后就可以去分析源代码，看是否代码存在内存泄漏，又或者创建了太多长期存在的对象，最后可以尝试调大堆内存的大小\n\n\n\n\n频繁GC\n一般OOM前会出现频繁GC，主要有两种：频繁YoungGC和频繁FullGC，单纯的频繁YoungGC往往是由年轻代空间太小导致的，只需要适当增大年轻代的大小即可解决这个问题，因为YoungGC只与存活对象的数量有关，与年轻代大小无关\n相对于频繁YoungGC，频繁FullGC会引发更加严重的问题，且解决起来更加复杂。因为FullGC更加消耗CPU资源并且STW停顿时间较长，所以，在发生频繁FullGC时，CPU利用率一般会飙升，并且会出现应用程序变慢的情况（比如接口请求处理速度变慢甚至大量超时）\n触发FullGC的主要原因是老年代空间不足。前面我们已经总结过，老年代的对象一般来源于长期存活的对象、大对象、空间分配担保。接下来，我们从这3个对象来源来分析频繁GC发生的原因。\n长期存活的对象：如果应用程序创建的长期存活的对象比较多，那么，我们可以适当调大老年代的大小，以减少FullGC的频率。不过，这种情况并不常见，大部分应用程序并不会创建太多的长期存活的对象。实际上，内存泄露往往才是导致对象长期存活无法回收的主要原因。如果每次FullGC回收率很低，释放出来的空间很少，那么就说明是存在内存泄露了。频繁FullGC一段时间之后，JVM便会出现OOM\n大对象：前面讲到，大对象会直接进入老年代。过多的大对象是引起频繁FullGC的最常见的原因之一。比如，在某个接口中执行了未分页SQL，一次性加载过多数据到内存中，当高并发下，接口大量被调用，就会导致大量大对象被创建，从而导致老年代空间不足，引发频繁FullGC。定位此种频繁FullGC发生的原因，我们需要在FullGC前（设置JVM参数-XX:+HeapDumpBeforeFullGC）dump内存快照，分析占用堆内存比较多的是哪个对象，以此来定位问题代码\n空间分配担保：前面讲到，在执行YoungGC时，如果To Survivor空间不足，JVM会触发空间分配担保，将对象存储到老年代。因此，如果每次YoungGC，To Survivor都被占满，那么，我们就要考虑增大To Survivor区，避免空间分配担保，减少进入老年代的对象数量\n\n\n\n\nGC时间过长\n堆内存过大：前面讲到，年轻代使用标记-复制垃圾回收算法，并且，年轻代空间增大并不会导致存活对象增多，因此，YoungGC时间跟年轻代的大小无关，但是，老年代使用标记-整理或标记-清除垃圾回收算法，并且，老年代空间增大会导致存活对象增多，因此，FullGC时间跟老年代的大小有关。老年代过大会导致FullGC时间过长。针对比较大的堆内存，我们应该选择GC时间可控的G1垃圾回收器，或者在一台大物理内存的机器上部署多个JVM，以减小单个堆内存的大小\nConcurrent Mode Failure：前面讲到，CMS垃圾回收器采用并发垃圾回收算法，在垃圾回收的某些阶段，应用程序可以与之并发执行。应用程序的执行需要堆内存，因此，JVM在执行垃圾回收前，会预留一定的堆内存空间。但是，在执行垃圾回收的的过程中，如果预留空间不足，应用程序无法继续执行，那么，JVM便会抛出Concurrent Mode Failure错误，并且，暂停CMS垃圾回收器的执行，改为STW停顿时间更长的Serial Old垃圾回收器。垃圾回收器的中止和切换势必会增长FullGC时间。如果我们在GC详细日志中（通过设置JVM参数-XX:+PrintGCDetails得到）发现大量Concurrent Mode Failre字样，那么，我们就需要通过减小JVM参数-XX:CMSInitialOccupancyFraction的值来调大预留空间的大小\n操作系统swap：swap是操作系统中的概念。当物理内存不足时，操作系统会将物理内存中的部分不活跃的数据放入磁盘，当这部分数据重新被使用时，再从磁盘加载到物理内存中。这种数据在物理内存和磁盘之间换入换出的机制，就叫作swap。swap涉及磁盘I/O操作，非常影响进程的性能。如果设置的JVM堆内存大小超过物理内存大小，或者多个应用程序争用有限的物理内存，那么，就有可能触发swap而导致GC时间增长。解决这个问题的方法也很简单，尽量保证JVM堆大小不要超过物理内存的大小，并且为操作系统或者其他软件预留充足的物理内存，比如物理内存有8GB，我们设置JVM堆大小为6GB，预留2GB给操作系统和其他并发运行的软件\n\n\n\n\n\n附录\n如何判断一个常量是废弃常量\n\nJDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)\n假如在字符串常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池了\n\n\nJVM 判定两个 Java 类是否相同的具体规则\n\nJVM 不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即使两个类来源于同一个Clas文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相同\n\n\n打破双亲委派模型方法\n\n自定义加载器的话，需要继承ClassLoader。如果我们不想打破双亲委派模型，就重写ClassLoader类中的findClass()方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写loadClass()方法\nTomcat 服务器为了能够优先加载 Web 应用目录下的类，然后再加载其他目录下的类，就自定义了类加载器WebAppClassLoader来打破双亲委托机制。这也是 Tomcat 下 Web 应用之间的类实现隔离的具体原理（详见深入拆解Tomcat&amp;Jetty）\n\n\nHotSpot虚拟机对象探秘\n\n对象的创建\n\n类加载检查：虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程\n分配内存：在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有“指针碰撞”和“空闲列表”两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定\n指针碰撞 ：堆内存没有内存碎片的情况下，使用该分配方式的 GC 收集器有Serial, ParNew\n空闲列表 ：堆内存有内存碎片的情况下，虚拟机有一个空闲链表记录哪些内存块是可用的，使用该分配方式的 GC 收集器有CMS\n内存并发问题：创建对象是很频繁的，必须保证线程安全，有以下几种方法\nCAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性\nTLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配\n\n\n\n\n初始化零值：内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值\n设置对象头：初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。这些信息存放在对象头中。另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式\n执行init方法：在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来\n\n\n对象的内存布局\n\n对象头\n用于存储对象自身的运行时数据：哈希码、GC分代年龄、锁状态标志等\n类型指针：对象指向他的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例\n\n\n实例数据：对象真正存储的有效信息，程序中所定义的各种类型的字段内容\n对齐填充：仅仅起占位作用，因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全\n\n\n对象的内存访问：通过栈上的reference数据来操作堆上的具体对象，对象的访问方式有虚拟机实现而定，主要有以下几种\n\n句柄：如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息（reference中存储的是稳定的句柄地址，对象被移动只会改变句柄中的实例数据指针，reference本身不需要修改）\n\n\n直接指针：如果使用直接指针访问，reference 中存储的直接就是对象的地址（速度快，节省了一次指针定位的时间开销）\n\n\n\n\nHostSpot VM的实现，里面的GC主要有两类\n\nPartial GC：并不收集整个GC堆的模式\n新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集\n老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集，知有CMS和concurrent collection有这个模式\n混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集，只有G1有这个模式\n\n\n整堆收集 (Full GC)：收集整个 Java 堆和方法区\n\n\n\n\n\n","slug":"JVM","date":"2023-07-30T02:35:00.000Z","categories_index":"","tags_index":"","author_index":"Dajunnnnnn"},{"id":"3fe1ee3f3830128bf539e5f4ed9fbbe9","title":"Linux","content":"Linux1.进程管理\n概念\n\n进程：进程是资源调度的基本单位，启动main函数就是启动一个JVM进程，main函数所在的线程是这个进程的主线程\n\n进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态\n\n一个进程切换到另一个进程运行，称为进程的上下文切换，其中上下文包括CPU 寄存器、程序计数器，切换时保存在进程的 PCB（进程存在的唯一标识）中，切换的时机主要有时间片耗尽、资源不足、高优先级进程调度、主动Sleep、硬件中断\n\n进程切换涉及到更多的内容，包括整个进程的地址空间、全局变量、文件描述符等。因此，进程切换的开销通常比线程切换大\n线程切换只涉及到线程的堆栈、寄存器和程序计数器等，不涉及进程级别的资源，因此线程切换的开销较小\n\n\n分配给进程的资源：虚拟内存、文件描述符、信号\n\n\n\n\n线程：线程是CPU调度的基本单位，线程间共享堆和方法区（代码段、数据段）资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈\n\n用户线程（*User Thread*）：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；\n内核线程（*Kernel Thread*）：在内核中实现的线程，是由内核操作系统管理的线程，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责；\n轻量级进程（*LightWeight Process*）：在内核中来支持用户线程；\n\n\n协程：由编程语言创建，又称为用户态线程；协程是异步非抢占式的，需要用户自己释放使用权来切换协程；线程数量在千万级别，而协程可以达到上万级别，因为线程是多核上并行，而协程是用来实现高并发的\n\n\n\n进程通信的方式：匿名管道、命名管道、信号、消息队列、共享内存、内存映射、套接字、信号量\n\n管道\n匿名管道：Unix系统最古老的通信方式，通过在内核中维护一块内核缓冲区来实现通信，Linux系统中通过pipi()函数创建管道，会生成两个文件描述符，分别对应读端和写端，只能用于具有亲缘关系的进程间通信\n命名管道：为了解决匿名管道只能在具有亲缘关系的进程间通信的问题，通过将管道和一个路径名相关联，以FIFO的文件形式存在于文件系统中，没有亲缘关系的进程也可以像操作文件一样通过命名管道进行数据交换\n\n\n信号：Linux系统最古老的通信方式，是一种异步通信的方式，信号可以导致一个正在运行的进程被另一个正在运行的进程中断，转而处理突发事件，是事件发生时对进程的通知机制\n消息队列：一个消息的链表，链表中的消息有特性格式和优先级，有对应权限的进程可以对其进行读写\n共享内存：允许两个或多个进程共享物理内存的同一块区域（称为段），由于一个共享内存段会成为一个进程用户空间的一部分，因此这种IPC机制无需内核介入，只需要一个进程将数据复制进共享内存中即可\n共享内存：不同进程间共享的一段物理内存，所有进程都可以访问共享内存中的地址，改动对所有进程可见。优点是可以直接访问速度更快；缺点是需要额外的同步机制来互斥访问\n\n\n内存映射：将磁盘文件的数据映射到内存，用户通过修改内存就能修改磁盘文件\n信号量：用于解决进程或线程之间并发执行是的同步问题，对信号量的操作主要有P操作和V操作\nSocket：对网络中不同主机上的应用进程之间进行双向通信的段点的抽象，提供了应用层进程利用网络协议交换数据的机制，用于网络中不同主机上的进程之间进行通信\n\n\n进程调度\n\n算法\n先来先服务（FCFS）：选最早\n短作业/进程优先（SJF/SPF）：选最短（平均等待时间、平均周转时间最少）\n优先级调度：选优先级最高（净：sys&gt;users、交互&gt;非交互、I/O&gt;CPU）\n高相应比优先（HRRN）：响应比最高（等待时间+需服务时间）/需服务时间\n时间片轮转（RR）：分时OS、绝对可抢占\n多级反馈队列：1+3+5、“UNIX”、优先级高到低、时间片小到大（上无才执行下\n\n\n不能调度（处理中断、临界区、屏蔽中断）；剥夺与非剥夺调度（早期批处理）【在进程处于临界区时，只要不破坏临界区资源使用规则就不影响处理机调度】\n评估指标：CPU利用率（忙碌时间/总时间）、系统吞吐量（完成作业数/总时间）、平均周转时间（提交到完成/n=（等待+执行）/n）、带权周转时间（作业周转时间/实际运行时间，越小越好必然大于1）、等待时间（等处理机状态）、响应时间（提交请求到首次响应）\n\n\n死锁：并发执行线程需要加锁主要是为了保护共享数据，防止出现”竞态条件”\n\n死锁定义：多个进程因竞争资源而造成的一种僵局（互相等待）若无外力作用，这些进程都将无法向前推进\n互斥：保证一个线程在临界区执行时，其他线程应该被阻止进入临界区\n同步：并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步\n\n\n产生原因：竞争资源、进程推进非法\n必要条件：互斥访问临界区资源、请求和保持、不剥夺、环路等待（等待的进程成环）\n预防死锁：破坏互斥（资源共享使用）、破坏不剥夺、破坏请求和保持（预先静态分配）、破坏循环等待（顺序资源分配）（必要条件）\n避免死锁：安全状态（不一定是死锁状态）、银行家算法（必须知道将来的资源请求）、安全性算法\n检测死锁：利用死锁定理化简资源分配图（圆圈代表进程，框代表一类资源，一个圆代表一个该类资源，进程到资源为请求边，资源到进程为分配边）把圈都变成孤点\n解除死锁：资源剥夺、撤销进程、进程回退\n死锁检测：银行家算法\nMax、Allocation、Need\nWork、Need、Allocation、W+A（分配一个写一行，一行一行写）\n\n\n\n\n同步\n\n信号量：操作系统提供的一种协调共享资源访问的方法，通常信号量表示资源的数量，对应的变量是一个整型（sem）变量，外，还有两个原子操作的系统调用函数来控制信号量的，分别是：\n\nP 操作：将 sem 减 1，相减后，如果 sem &lt; 0，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；\nV 操作：将 sem 加 1，相加后，如果 sem &lt;= 0（多个线程同时P后，即使有V，sem也会是负的），唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；\n\n\n操作系统实现\n\n\n\n\n线程通信方式：Monitor、Condition\n\n在Java中,常用的线程通信方式有两种,分别是利用Monitor实现线程通信、利用Condition实现线程通信。线程同步是线程通信的前提,所以究竟采用哪种方式实现通信,取决于线程同步的方式。\n如果是采用synchronized关键字进行同步,则需要依赖Monitor（同步监视器）实现线程通信，Monitor就是锁对象。在synchronized同步模式下,锁对象可以是任意的类型,所以通信方法自然就被定义在Object类中了,这些方法包括：wait()、notify()、notifyAll()。一个线程通过Monitor调用wait()时,它就会释放锁并在此等待。当其他线程通过Monitor调用notify()时,则会唤醒在此等待的一个线程。当其他线程通过Monitor调用notifyAll()时,则会唤醒在此等待的所有线程。\nJDK 1.5新增了Lock接口及其实现类,提供了更为灵活的同步方式。如果是采用Lock对象进行同步,则需要依赖Condition实现线程通信，Condition对象是由Lock对象创建出来的,它依赖于Lock对象。Condition对象中定义的通信方法,与Object类中的通信方法类似,它包括await()、signal()、signalAll()。通过名字就能看出它们的含义了,当通过Condition调用await()时当前线程释放锁并等待,当通过Condition调用signal()时唤醒一个等待的线程,当通过Condition调用signalAll()时则唤醒所有等待的线程。\n线程同步是基于同步队列实现的,而线程通信是基于等待队列实现的。当调用等待方法时，即将当前线程加入等待队列。当调用通知方法时,即将等待队列中的一个或多个线程转移回同步队列。因为synchronized只有一个Monitor，所以它就只有一个等待队列。而Lock对象可以创建出多个Condition，所以它拥有多个等待队列。多个等待队列带来了极大的灵活性,所以基于Condition的通信方式更为推荐\n比如，在实现生产消费模型时，生产者要通知消费者、消费者要通知生产者。相反，不应该出现生产者通知生产者、消费者通知消费者这样的情况。如果使用synchronized实现这个模型，由于它只有一个等待队列,所以只能把生产者和消费者加入同一个队列,这就会导致生产者通知生产者、消费者通知消费者的情况出现。采用Lock实现这个模型时，由于它有多个等待队列，可以有效地将这两个角色区分开，就能避免出现这样的问题。\n\n\n\n2.内存管理\n虚拟地址与物理地址映射方式\n\n分段：将用户程序地址空间分成若干个大小不等的段，每段可以定义一组相对完整的逻辑信息，段与段之间可以不相邻接。主要是为了程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护，反应程序的逻辑结构有利于段的共享\n\n会有外部内存碎片，可以通过内存交换（写入磁盘swap区再重新写回），但是因为磁盘读写速度所以效率很低\n\n\n分页：用户程序的地址空间划分为若干个固定大小的页，内存空间分成若干个物理块，页和快的大小相等，可以将任一页放在任一块中。主要用于实现虚拟内存，从而获得更大的地址空间，可以解决内存碎片，提高内存利用率\n\n换入换出\n当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行\n如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为换出（Swap Out）。一旦需要的时候，再加载进来，称为换入（Swap In）\n\n\n多级页表：\nTLB：专门存放程序最常访问的页表项的 Cache，又称快表\n\n\n段页式：段页式存储管理方式即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。系统同时配置段表和页表，每个进程一张段表，没个分段一张页表，利用段表和页表进行用户地址空间到物理内存空间的映射。\n\n段表项包括段号、页表长度、页表起始地址；页表项包括页号、块号。在进行地址转换时，首先通过段表查到页表始址，然后通过页表找到页帧号，最终形成物理地址。\n\n\nLinux实现：页式内存管理（由段式内存管理所映射而成的地址上再加上一层地址映射）\n\nLinux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。\n\n\n32位系统内存空间划分\n\n上图中的内存布局可以看到，代码段下面还有一段内存空间的（灰色部分），这一块区域是「保留区」，之所以要有保留区这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在 C 的代码里会将无效的指针赋值为 NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现 bug，导致读或写了一些小内存地址的数据，而使得程序跑飞\n在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存\n保留区：因为在大多数的系统里，认为比较小数值的地址不是一个合法地址，所以空出来不使用\n代码段，包括二进制可执行代码；\n数据段，包括已初始化的静态常量和全局变量；\nBSS 段，包括未初始化的静态变量和全局变量；\n堆段，包括动态分配的内存，从低地址开始向上增长；\n文件映射段，包括动态库、共享内存等，从低地址开始向上增长。\n栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB，增长顺序为从高到低\n内核空间：\n\n\n\n\n\n\n\n虚拟内存和物理内存\n\n物理内存：使用物理地址进行寻址，寻址的范围取决于CPU的地址线条数（如32位平台下寻址范围为2^32即4G），每次开启一个线程都要给4G物理内存，资源不够时没分配到资源的进程只能等待，并且资源不隔离导致数据不安全\n虚拟内存：让应用程序以为自己拥有连续的可用内存（连续完整的地址空间），实际上被分割成多个物理内存分片，还有部分存储在外部磁盘存储器上，在需要时进行数据交换\n\n\n内存泄漏\n\n程序中已动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果\n避免内存泄漏：动态开辟内存空间、及时释放内存、使用智能指针，采用静态分析技术（LCLink、ccmalloc、Dmalloc、Electric Fence、Leaky、LeakTracer、MEMWATCH、Valgrind、KCachegrind）来进行检测\n\n\n堆和栈的区别\n\n管理方式：堆手动控制，栈由编译器自动管理\n空间大小：栈小于堆，栈会出现OOM问题\n碎片问题：堆频繁的分配和释放会造成空间不连续，利用率低，栈不会\n增长方向：堆是向上的（内存地址增加），栈是向下的（内存地址减小）\n分配方式：堆都是动态分配的，没有静态分配的堆。栈有两种分配方式：静态分配和动态分配，静态分配是编译器完成的，比如局部变量的分配；动态分配由alloca函数进行分配，但是栈的动态分配和堆是不同的，它的动态分配是由编译器实现的，无需我们手工实现\n分配效率：栈有寄存器、底层指令的支持，堆是由C/C++来提供的，根据算法在内存中找可用空间，所以堆的效率要比栈低的多\n\n\nmalloc\n\nCPU Cache的数据写入\n\n写直达（Write Through）：把数据同时写入内存和 Cache 中，写入前会先判断数据是否已经在 CPU Cache 里面了\n如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；\n如果数据没有在 Cache 里面，就直接把数据更新到内存里面。\n\n\n写回（Write Back）：当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中，减少了数据写回内存的频率\n\n\n缓存一致性\n\n第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为写传播（*Write Propagation*）；\n\n第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为事务的串行化（*Transaction Serialization*）。\n\n实现\n\n总线嗅探：当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache\n\nMESI协议：\n\n状态：Modified（已修改）、Exclusive（独占）、Shared（共享）、Invalidated（已失效）\n\n状态流转\n\n\n示例\n\n当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；\n然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；\n当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不一致了。\n如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。\n如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。\n\n\n\n\n\n\n\n\n\n3.网络系统\nIO模型\n同步阻塞IO（BIO）「jdk1.4之前支持」：服务端采用单线程，当accept一个请求后，在recv或send调用阻塞时，将无法accept其他请求，必须等待上一个请求被处理完；服务端采用多线程，当accept一个请求后，开启线程进行recv，通过多个线程可以完成并发处理，但是大量的线程占用大量的内存空间，并且线程切换会带来巨大的开销\n\n阻塞和非阻塞：数据未准备好时请求数据，等待数据就是阻塞，直接返回多次请求就是非阻塞\n同步与异步：同步模式由用户线程的内核态执行内核空间到用户空间的数据拷贝，异步模式由内核来执行数据拷贝执行完通知用户线程并将数据回调给用户线程\n阻塞读/写：执行read或send系统调用时，用户线程从用户态切换到内核态，执行用户空间拷贝到内核空间的Socket发送缓冲区之间的数据拷贝。读时如果没有数据则线程进入阻塞状态，直到有数据后唤醒线程；写时如果无法一次容纳所有的数据，则让出CPU，直到空间够用时执行写流程\n\n\n同步非阻塞IO（NIO）「jdk1.4之后的java.nio包」：服务器accept一个请求后，加入文件描述符集合，每次轮询一遍文件描述符集合来recv数据，没有数据立即返回错误，每次轮询所有文件描述符很浪费时间\n\n阻塞IO的问题是一个线程只能处理一个连接，非阻塞IO就是为了解决这样的问题\n非阻塞读：当无数据时，系统调用立刻返回并带一个EWOULDBLOCK 或 EAGAIN错误，这个阶段用户线程不会阻塞也不会让出CPU，而是会继续轮训直到socket接收缓冲区中有数据为止\n非阻塞写：能写多少写多少，不用一次写完，写不下了返回已经写入的字节数，方便下一次轮训来写剩下的数据\n\n\nIO多路复用（select、poll、epoll、aio、libevent、libuv）：见下\n\n在阻塞IO模型中一个连接就需要分配一个独立的线程去专门处理这个连接上的读写，到了IO多路复用模型中，多个连接可以复用这一个独立的线程去处理这多个连接上的读写，通过使用操作系统内核来执行轮训操作，减少系统调用和内核切换的次数\n\nselect：将轮询的操作交给了内核来完成，调用并阻塞在select系统调用上，并通过select将文件描述符fd数组（BitMap，1表示该fd上有读写事件）通过select系统调用传递给内核。select通过内核来轮询遍历fd数组，有数据来就设置为1否则设置为0，如果有新的数据来就将修改后的fd数组返回给用户线程。用户线程接触阻塞，开始遍历fd数组对值为1的scoket文件描述符发起系统调用。仍需要上下文切换和数据拷贝，还需要遍历结果，所以只能处理1000个左右的并发连接\n&#x2F;&#x2F;select系统调用是在规定的超时时间内，监听（轮询）用户感兴趣的文件描述符集合上的可读,可写,异常三类事件\n&#x2F;&#x2F;这里的fd_set就是前边提到的fd数组，是一个BitMap结构\nint select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)\n&#x2F;&#x2F;用户线程中重新遍历fd数组的过程中，需要用到的API\nvoid FD_ZERO(fd_set *fdset)  &#x2F;&#x2F;清空指定的文件描述符集合，即让fd_set中不在包含任何文件描述符\nvoid FD_SET(int fd, fd_set *fdset)  &#x2F;&#x2F;将一个给定的文件描述符加入集合之中\nint FD_ISSET(int fd, fd_set *fdset)  &#x2F;&#x2F;检查集合中指定的文件描述符是否可以读写。用户线程遍历文件描述符集合,调用该方法检查相应的文件描述符是否IO就绪\nvoid FD_CLR(int fd, fd_set *fdset)  &#x2F;&#x2F;将一个给定的文件描述符从集合中删除\npoll：poll相当于是改进版的select，但是工作原理基本和select没有本质的区别\nint poll(struct pollfd *fds, unsigned int nfds, int timeout)\nstruct pollfd &#123;\n    int   fd;         &#x2F;* 文件描述符 *&#x2F;\n    short events;     &#x2F;* 需要监听的事件 *&#x2F;\n    short revents;    &#x2F;* 实际发生的事件 由内核修改设置 *&#x2F;\n&#125;;  &#x2F;&#x2F;将BitMap结构改成pollfd，即没有固定长度的数组，这样就没有最大描述符数量的限制，只受限于系统文件描述符最大数量\n\n\n信号驱动的IO（SIGIO）\n\n异步IO（POSIX的aio_functions）\n\n异步非阻塞IO，在进行IO操作时，不需要等待操作完成就可以进行其他操作，当操作完成后自动回调通知，jdk1.7之后java.nio包下的java.nio.channels.AsynchronousSocketChannel等。但是linux下AIO支持并不好并且相比NIO性能提升不明显，所以Netty在4.x舍弃了AIO\n\n\n\n\nReactor模式和Proactor模式\nReactor模式：要求主线程只负责监听文件描述符上是否有事件发生，有的话立即将该事件通知工作线程，将socket可读可写事件方去请求队列，交给工作线程来处理，使用epoll实现的工作流程如下：\n主线程向epoll内核事件表中注册socket上的读就绪事件\n可读\n主线程调用epoll_wait等待socket上有数据可读\n当socket上有数据可读时，epoll_wait通知主线程，主线程将socket可读事件放入请求队列\n睡眠在请求队列上的某个工作线程被唤醒，它从socket读取数据，并处理客户请求，然后往epoll内核事件表中注册该socket上的写就绪事件\n\n\n可写\n当主线程调用epoll_wait等待socket可写\n当socket可写时，epoll_wait通知主线程。主线程将socket可写事件放入请求队列\n睡眠在请求队列上的某个工作线程被唤醒，它往 socket上写入服务器处理客户请求的结果\n\n\n\n\nProactor模式\n\n\nIO多路复用技术\nselect模型：采用轮询和遍历的方式，在客户端操作服务器时，会创建三种文件描述符（写描述符、读描述符、异常描述符），select会阻塞这三种文件描述符，等有数据可读、可写、异常或超时都会返回。然后通过遍历文件描述符集合来找到就绪的文件描述符，进行IO操作。由于每次都需要遍历所有文件描述符，所以性能随着描述符的增多而减小\n\n使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符\n\n\n\n  int main()&#123;\n\t&#x2F;&#x2F;socket 建立、地址设置\n\tfd_set read_fs,write_set;\n\tstruct timeval timeout;\n\tint max &#x3D; 0;&#x2F;&#x2F;用于记录最大的fd，在轮询中时刻更新\n\t&#x2F;&#x2F;初始化比特位\n\tFD_ZERO（&amp;read_fs);\n\tFD_ZERO(&amp;write_fs);\n\t\n\tint nfds &#x3D; 0;&#x2F;&#x2F;记录就绪的事件，可以减少遍历的次数\n\twhile(1）&#123;\n\t&#x2F;&#x2F;阻塞获取，每次需要把fd从用户态拷贝到内核态\n\tnfds &#x3D; select(max+1, &amp;read_fd, &amp;write_fd, NULL, &amp;timeout);\n\t&#x2F;&#x2F;每次需要遍历所有fd，判断有无读写事件发生\n\tfor(int i &#x3D; 0; i &lt;&#x3D; max &amp;&amp; nfds; ++i)&#123;\n\t\tif(i &#x3D;&#x3D; listenfd)&#123;\n\t\t\t--nfds;\n\t\t\t&#x2F;&#x2F;处理accept事件\n\t\t\tFD_SET(i, &amp;read_fd);&#x2F;&#x2F;将客户端socket加入到集合中\n\t\t&#125;\n\t\tif(FD_ISSET(i, &amp;read_fd))&#123;\n\t\t\t--nfds;\n\t\t\t&#x2F;&#x2F;处理read事件\n\t\t&#125;\n\t\tif (FD_ISSET(i, &amp;write_fd)) &#123;\n\t\t\t --nfds;\n\t     &#x2F;&#x2F; 这里处理write事件\n    &#125;\n  &#125;\n&#125;\n\n\npoll模型：poll模型的原理与select模型基本一致，也是轮询+遍历，区别在于poll采用链表的方式来存储文件描述符，同select一样，性能随着描述符的增多而减小\n#include &lt;poll.h&gt;\n&#x2F;&#x2F; 数据结构\nstruct pollfd &#123;\n    int fd;                         &#x2F;&#x2F; 需要监视的文件描述符\n    short events;                   &#x2F;&#x2F; 需要内核监视的事件\n    short revents;                  &#x2F;&#x2F; 实际发生的事件\n&#125;;\n\n&#x2F;&#x2F; API\nint poll(struct pollfd fds[], nfds_t nfds, int timeout);\nepoll模型：采用时间通知机制来触发相关的IO操作，没有文件描述符个数的限制，从用户态拷贝到内核态只需要一次。它主要通过系统底层的函数来注册、激活文件描述符，从而触发相关的IO操作，这样大大提高了提高性能（Redis、Nginx等，1G内存大概支持10万个句柄）\n\n特点\n\n将轮询改成了回调，提高了CPU执行效率、没有文件描述符数量限制、性能不会随文件描述符的增加而减小，内存拷贝（利用mmap文件映射内存加速与内核空间的消息传递，减少复制开销）\n\n在内核里使用红黑树来关注进程所有待检测的 Socket，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。\n\n解释io特别密集时 epoll 效率不高\n\n连接密集（短连接特别多），使用epoll的话，每一次连接需要发生epoll_wait-&gt;accpet-&gt;epoll_ctl调用，而使用select只需要select-&gt;accpet，减少了一次系统调用\n读写密集的话，如果收到数据，我们需要响应数据的话，使用epoll的情况下， read 完后也需要epoll_ctl 加入写事件，相比select多了一次系统调用\n\n\n\n\nSocket创建\n\n服务端线程调用accept后阻塞，在客户端连接并完成TCP三次握手之后，kernel创建一个对应的socket作为服务端与内核端通信的内核接口，并将socket保存在当前进程打开的文件列表（Linux内核中一切皆是文件）中管理起来\n进程中管理的文件列表结构：内核通过fd_array来进行组织管理，数组下标为文件描述符，数据中存放的是文件数据结构file。进程中打开的文件列表fd_array定义在内核数据结构struct files_struct中，在struct fdtable结构中有一个指针struct file **fd指向fd_array\n用于封装文件元信息的内核数据结构struct file中的private_data指针指向具体的Socket结构。struct file中的file_operations属性定义了文件的操作函数，不同的文件类型，对应的file_operations是不同的，针对Socket文件类型，这里的file_operations指向socket_file_ops。在用户空间对Socket发起的读写等系统调用，进入内核首先会调用的是Socket对应的struct file中指向的socket_file_ops，如对Socket发起write写操作，在内核中首先被调用的就是socket_file_ops中定义的sock_write_iter\nSocket内核结构：最先创建的是监听socket，之后另外创建新的socket专门用于客户端之间的网络通信，并将监听Socket中的Socket操作函数集合（inet_stream_ops）ops赋值到新的Socket的ops属性中。接着kernel会为已连接的socket创建file结构体并初始化，并把Socket文件操作函数集合（socket_file_ops）赋值给file中的f_ops指针。然后将struct socket中的file指针指向这个新分配申请的file结构体。然后调用socket-&gt;ops-&gt;accept，即inet_accept函数，该函数会在icsk_accept_queue（已完成三次握手）中查找是否有已经建立好的连接，如果有的话，直接从icsk_accept_queue中获取已经创建好的struct sock。并将这个struct sock对象赋值给struct socket中的sock指针（sock对象中定义了接收队列，发送队列，等待队列，数据就绪回调函数指针，内核协议栈操作函数集合）\ninet_stream_ops函数集合中存储给用户提供的接口，socket与内核协议栈之间的接口定义在struct sock中的sk_port指针上\nstruct sock中的等待队列中存放的是系统IO调用发生阻塞的进程fd，以及相应的回调函数\n对Socket发起的系统IO调用，在内核中首先会调用Socket的文件结构struct file中的file_operations文件操作集合，然后调用struct socket中的ops指向的inet_stream_opssocket操作函数，最终调用到struct sock中sk_prot指针指向的tcp_prot内核协议栈操作函数接口集合\n\n\n当struct file，struct socket，struct sock这些核心的内核对象创建好之后，最后就是把socket对象对应的struct file放到进程打开的文件列表fd_array中。随后系统调用accept返回socket的文件描述符fd给用户程序。\n\n\n阻塞IO中用户进程阻塞以及唤醒原理\n\n阻塞\n首先我们在用户进程中对Socket进行read系统调用时，用户进程会从用户态转为内核态\n在进程的struct task_struct结构找到fd_array，并根据Socket的文件描述符fd找到对应的struct file，调用struct file中的文件操作函数结合file_operations，read系统调用对应的是sock_read_iter\n在sock_read_iter函数中找到struct file指向的struct socket，并调用socket-&gt;ops-&gt;recvmsg，这里我们知道调用的是inet_stream_ops集合中定义的inet_recvmsg\n在inet_recvmsg中会找到struct sock，并调用sock-&gt;skprot-&gt;recvmsg,这里调用的是tcp_prot集合中定义的tcp_recvmsg函数\n\n\n唤醒：\n当软中断将sk_buffer放到Socket的接收队列上时，接着就会调用数据就绪函数回调指针sk_data_ready，前边我们提到，这个函数指针在初始化的时候指向了sock_def_readable函数\n在sock_def_readable函数中会去获取socket-&gt;sock-&gt;sk_wq等待队列。在wake_up_common函数中从等待队列sk_wq中找出一个等待项wait_queue_t，回调注册在该等待项上的func回调函数（wait_queue_t-&gt;func）,创建等待项wait_queue_t是我们提到，这里注册的回调函数是autoremove_wake_function，即epoll的回调函数\n在autoremove_wake_function函数中，根据等待项wait_queue_t上的private关联的阻塞进程fd调用try_to_wake_up唤醒阻塞在该Socket上的进程\n\n\n\n\n相关的系统调用\n\nepoll_create：系统启动的时候，在Linux内核里面申请一个B+树结构的文件系统，然后返回epoll对象（一个文件描述符）用来后续使用\n\nepoll_ctl：每新建一个连接的时候，会同步更新epoll对象中的文件描述符，并且绑定一个callback函数，通过此调用向epoll对象中添加、删除、修改感兴趣的事件，返回0表示成功\n\nepoll_wait：轮询所有的callback集合（红黑树），并触发相应的IO操作，通过此调用收集在epoll监控中已发生的事件\n\n工作流程\n\n通过epoll_create创建epoll对象，此时epoll对象的内核结构包含就绪链表和红黑树，就绪队列是用于保存所有读写事件到来的socket。红黑树用于保存所有待检测的socket\n通过 epoll_crt 将待检测的socket，加入到红黑树中，并注册一个事件回调函数，当有事件到来的之后，会调用这个回调函数，进而通知到 epoll 对象\n调用 epoll_wait 等待事件的发生，当内核检测到事件发生后，调用该socket注册的回调函数，执行回调函数就能找到socket对应的epoll对象，然后会将事件加入到epoll对象的绪队列中，最后将就绪队列返回给应用层\n\n\n示意图\n\n\n\n\nepoll水平触发（LT）和边缘触发（ET）的区别\n\nLevel Triggered（LT）水平触发：只要有数据就会触发，只要这个 fd 还有数据可读，每次 epoll_wait 都会返回它的事件，提醒用户程序去操作\n使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，目的是告诉我们有数据需要读取；\n如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作\n\n\nEdge Triggered（ET）边缘触发：只有数据到来，才触发，只会提示一次，直到下次数据流入\n使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；\n如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，边缘触发模式一般和非阻塞 I/O 搭配使用，程序会一直执行 I/O 操作，直到系统调用（如 read 和 write）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK。\n\n\n\n\n代码示例\n#include &lt;sys&#x2F;epoll.h&gt;\n\n&#x2F;&#x2F; 每一个epoll对象都有一个独立的eventpoll结构体\n&#x2F;&#x2F; 用于存放通过epoll_ctl方法向epoll对象中添加进来的事件\n&#x2F;&#x2F; epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可\nstruct eventpoll &#123;\n    &#x2F;*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*&#x2F;\n    struct rb_root  rbr;\n    &#x2F;*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*&#x2F;\n    struct list_head rdlist;\n&#125;;\n\n&#x2F;&#x2F;在epoll中，对于每一个事件，都会建立一个epitem结构体\nstruct epitem&#123;\n    struct rb_node  rbn;&#x2F;&#x2F;红黑树节点\n    struct list_head    rdllink;&#x2F;&#x2F;双向链表节点\n    struct epoll_filefd  ffd;  &#x2F;&#x2F;事件句柄信息\n    struct eventpoll *ep;    &#x2F;&#x2F;指向其所属的eventpoll对象\n    struct epoll_event event; &#x2F;&#x2F;期待发生的事件类型\n&#125;\n\n&#x2F;&#x2F; API\nint epoll_create(int size); &#x2F;&#x2F; 内核中间加一个 eventpoll 对象，把所有需要监听的 socket 都放到 ep 对象中\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); &#x2F;&#x2F; epoll_ctl 负责把 socket 增加、删除到内核红黑树\n&#x2F;&#x2F; 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可\nint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);&#x2F;&#x2F; epoll_wait 负责检测可读队列，没有可读 socket 则阻塞进程\n\n\n\n\nNginx的IO模型：支持多种并发模型，自动选择最高效的模型，也可以使用use指令在配置文件中显示指定某个并发模型，如下所示\nselect：编译时，所用平台没有更高效的并发模型时，select被自动编译\npoll：标准并发模型，同select一样，所用平台没有更高效的并发模型时，pol被自动编译\nepoll：IO多路复用，高效并发模型，Linux2.6+可以使用\nkequeue：IO多路复用，高效并发模型，可在 FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0, and Mac OS X 平台中使用\n/dev/poll：高效并发模型，可在 Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+, and Tru64 UNIX 5.1A+ 平台使用\neventport：建议 使用/dev/poll替代\n\n\nRedis的IO模型：Redis跑在单线程中，所有操作都是按照顺序线性执行的，为了防止IO阻塞整个Redis进程，所以使用IO多路复用技术。Redis的IO复用技术基于epoll实现，另外提供了select和kqueue的实现\n\n附录\n\n\n\n\n\n\n\n\nShell/Python脚本、网络编程、多线程编程\n1.Termianl\n目录结构：/bin：命令、/etc：系统管理所需要的配置文件和子目录、/home：用户的主目录、/lib：系统最基本的动态连接共享库、/opt：额外安装软件所摆放的目录、/root：管理员主目录、/sbin：管理员使用的命令、/tmp：存放临时文件、/usr：用户的应用程序和文件、/var：存不断扩充的文件，如日志文件\n命令\n文件管理：ls、ll、cd、pwd、mkdir、rmdir、rm、cp、mv、touch、chown、chmod、tar、\n\n文档编辑：cat、tac、nl、more、less、head、tail、grep（文本搜索）、awk（自定义函数或正则表达式）、sed（批量编辑文本文件）\n\n进程监控：su、sudo、kill（kill -9 pid）、管道命令、ps（ps -ef | more进程信息）、top（实时显示进程信息）、lsof（查看某一文件的进程信息）、free（查看内存使用情况，如进程、CPU占用率、内存信息）、df（磁盘使用量）、iostat（I/O设备状态）、vmstat（虚拟内存状态）、du（磁盘使用情况）\n\n\n网络监控：iperf（查看带宽和网络）、netstat（查看占用端口的进程）、traceroute（数据包路径）、ping（测试与主机的连通性）、\n\n其它：strace、dtrace、systemtap、uname、history\n\n\n\nShell脚本\nVim编辑器\n常见系统调用\n文件管理：creat、open、close、lseek（定位）、read、write\n进程管理：fork、execve（执行新二进制文件）、waitpid（等待子进程结束）、clone、exit\n内存管理：brk、mmap（内存映射）\n进程间通信：\n消息队列：msgget（创建队列）、msgsnd（发送消息）、msgrcv（接收消息）\n共享内存：shmget（创建共享内存）、shmat（将共享内存映射到内存空间）\n信号量：sem_wait(抢占信号量)、sem_post（释放信号量）\n\n\n网络通信：socket、bind、connect、listen、accept\n信号处理：kill、signaction\n\n\n\n2.系统调用\nmalloc：通过两种系统调用申请堆内存（虚拟内存），malloc()本身不是系统调用\n\n方式一（用户分配的内存小于 128 KB）：通过 brk() 系统调用将堆顶指针向高地址移动，获得新的内存空间\n\n频繁通过 mmap 分配的内存，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大，所以通过brk系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中，等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了\n频繁使用brk，会导致堆内产生越来越多不可用的碎片，导致“内存泄露”，这种“泄露”现象使用 valgrind 是无法检测出来的\n\n\n\n方式二（用户分配的内存大于 128 KB）：通过 mmap() 系统调用在文件映射区域分配内存（私有匿名映射，从文件映射区“偷”了一块内存）\n\nmalloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节，保存了该内存块的描述信息，当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，从而知道释放多大的内存\n\n\n\n其它\n\nmalloc() 在分配内存的时候，并不按用户预期申请的字节数来分配内存空间大小，而是会预分配更大的空间作为内存池\nfree()\nmalloc 通过 brk() 方式申请的内存，free 释放内存的时候，并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用；\nmalloc 通过 mmap() 方式申请的内存，free 释放内存的时候，会把内存归还给操作系统，内存得到真正的释放。\n\n\n\n\n\n\n\n\n3.实战\n操作系统保护模式和实模式\n\n实模式将整个物理内存看成分段的区域，程序代码和数据位于不同区域，系统程序和用户程序并没有区别对待，而且每一个指针都是指向实际的物理地址。这样一来，用户程序的一个指针如果指向了系统程序区域或其他用户程序区域，并修改了内容，那么对于这个被修改的系统程序或用户程序，其后果就很可能是灾难性的。再者，随着软件的发展，1M的寻址空间已经远远不能满足实际的需求了。最后，对处理器多任务支持需求也日益紧迫，所有这些都促使新技术的出现\n为了克服实模式下的内存非法访问问题，并满足飞速发展的内存寻址和多任务需求，处理器厂商开发出保护模式。在保护模式中，除了内存寻址空间大大提高；提供了硬件对多任务的支持；物理内存地址也不能直接被程序访问，程序内部的地址(虚拟地址)要由操作系统转化为物理地址去访问，程序对此一无所知。至此，进程(程序的运行态)有了严格的边界，任何其他进程根本没有办法访问不属于自己的物理内存区域，甚至在自己的虚拟地址范围内也不是可以任意访问的，因为有一些虚拟区域已经被放进一些公共系统运行库。这些区域也不能随便修改，若修改就会有出现linux中的段错误，或Windows中的非法内存访问对话框\n\n\n内存回收：系统内存紧张时会进行内存回收，主要回收以下两类\n\n文件页（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存\n匿名页（Anonymous Page）：这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们回收的方式是通过 Linux 的 Swap 机制，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了\n文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。\nactive_list 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；\ninactive_list 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；\n\n\n\n\n从 a 文件 copy 到另外一个目录， b 作为一个从 a 目录 copy 到一个 b 目录这样的一个文件，操作过程中间包含了哪些系统调用？这里面执行了多少次拷贝的动作？\n\n\n第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。\n第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。\n第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核缓冲区里，这个过程依然还是由 CPU 搬运的。\n第四次拷贝，把内核缓冲区里的数据，拷贝到磁盘，这个过程又是由 DMA 搬运的。\n\n\nPageCache\n\n缓存一些比较常访问的文件到缓存中，这样子的话它就能减少两次从内核空间拷贝的过程，就是来减少查询这个内容的时间\n当用户对文件进行读写时，实际上是对文件的 页缓存 进行读写。所以对文件进行读写操作时，会分以下两种情况进行处理：\n当从文件中读取数据时，如果要读取的数据所在的页缓存已经存在，那么就直接把页缓存的数据拷贝给用户即可。否则，内核首先会申请一个空闲的内存页（页缓存），然后从文件中读取数据到页缓存，并且把页缓存的数据拷贝给用户。\n当向文件中写入数据时，如果要写入的数据所在的页缓存已经存在，那么直接把新数据写入到页缓存即可。否则，内核首先会申请一个空闲的内存页（页缓存），然后从文件中读取数据到页缓存，并且把新数据写入到页缓存中。对于被修改的页缓存，内核会定时把这些页缓存刷新到文件中。\n\n\n\n\n\n","slug":"Linux","date":"2023-05-11T11:26:21.000Z","categories_index":"","tags_index":"tools","author_index":"Dajunnnnnn"},{"id":"0eecce2180060832c1ffa34a76f3eb3b","title":"Spring Family","content":"Spring\n\n1.IOC1.基础知识\nBeanFactory：提供了一种高级配置，能够管理任何类型对象，BeanFactory是ApplicationContext的父接口，ApplicationContext接口的实现类主要有ClassPathXmlApplicationContext、FileSystemXmlApplicationContext、WebApplicationContext、AnnotationConfigApplicationContext等，主要负责Bean的实例化、配置、组装\n\n安装容器：ApplicationContext context = new ClassPathXmlApplicationContext(&quot;services.xml&quot;, &quot;daos.xml&quot;);\n\n\n\nBean：由Spring的IOC容器实例化、组装和管理的对象，一个Bean有全局唯一的name，其它的只能指定别名\n\nbean定义：Class、Name（全局唯一）、Scope、Constructor arguments、Properties、Autowiring mode、Lazy initialization mode、Lazy initialization mode、\n\n生命周期\n\n\n\nScope\nDescription\n\n\n\nsingleton\n(Default) Scopes a single bean definition to a single object instance for each Spring IoC container.\n\n\nprototype\nScopes a single bean definition to any number of object instances.\n\n\nrequest\nScopes a single bean definition to the lifecycle of a single HTTP request. That is, each HTTP request has its own instance of a bean created off the back of a single bean definition.\n\n\nsession\nScopes a single bean definition to the lifecycle of an HTTP Session.\n\n\napplication\nScopes a single bean definition to the lifecycle of a ServletContext.\n\n\nwebsocket\nScopes a single bean definition to the lifecycle of a WebSocket.\n\n\n\n实例化：bean定义本质上是创建一个或多个对象的方法，当被访问到时，容器会查看bean的命名和定义来创建实际对象\n\n通过构造方法实例化：所有普通类（默认空参构造+getter+setter）都可以被Spring使用并兼容，只需要指定bean类\n通过静态工厂方法实例化：使用class属性来指定包含静态工厂方法的类，并使用factory-method的属性来指定工厂方法本身的名称，主要用于在遗留代码中调用静态工厂来实例化bean\n通过实例工厂方法实例化：使用实例工厂中的非静态方法来创建新bean，使用factory-bean指定工厂类对应的bean，使用factory-method的属性来指定工厂方法\n\n\n\n\n容器扩展点\n\nBeanPostProcessor：通过实现其提供的回调方法，来提供定制的实例化逻辑、依赖解析逻辑，BeanPostProcessor的两个方法分别在Bean的初始化前后执行（postProcessBeforeInitialization和postProcessAfterInitialization）\n\nAOP自动代理类：DefaultAdvisorAutoProxyCreator，可以实现自动生效所有的advisor\n\nSpring自动装配的实现类：AutowiredAnnotationBeanPostProcessor\n         public interface BeanPostProcessor &#123;\n             @Nullable\n             default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;return bean;&#125;\n             @Nullable\n             default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;return bean;&#125;\n         &#125;\n\n\nBeanFactoryPostProcessor：与BeanPostProcessor的区别是BeanFactoryPostProcessor对bean的配置元数据进行操作，并可以在容器实例化除BeanFactoryPostProcessor实例之外的任何bean之前更改它\n\n\n\n从外部jdbc.properties文件导入DataSource的某些属性的元数据：PropertySourcesPlaceholderConfigurer\n        &lt;!--加载properties文件--&gt;\n        &lt;context:property-placeholder location&#x3D;&quot;classpath*:jdbc.properties&quot;&#x2F;&gt;\n        &lt;!--数据源--&gt;\n        &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt;\n            &lt;property name&#x3D;&quot;driverClassName&quot; value&#x3D;&quot;$&#123;jdbc.driver&#125;&quot;&#x2F;&gt;\n            &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;$&#123;jdbc.url&#125;&quot;&#x2F;&gt;\n            &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;$&#123;jdbc.username&#125;&quot;&#x2F;&gt;\n            &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&#x2F;&gt;\n        &lt;&#x2F;bean&gt;\n\n\nFactoryBean\n\n\n依赖注入：通过定义需要的依赖，并将这些依赖注入到bean中，可以使得代码更简洁，更方便测试。对强制依赖项使用构造函数，对可选依赖使用setter方法\n\n依赖注入处理流程\n\n通过所有bean的元数据的描述构造并初始化ApplicationContext\n对每一个bean，它的依赖关系以属性、构造函数或静态工厂方法的参数的形式表示，并在实际创建bean时提供给bean\n每个属性或构造函数参数都是要设置的值的实际定义，或是对容器中另一个bean的引用\n作为值的每个属性或构造函数参数都从其指定格式转换为该属性或构造函数参数的实际类型\n\n\n自动装配的四种模式：？\n\nno：不自动装配，bean引用必须有ref元素定义\nbyName：按属性名自动装配，Spring寻找与需要自动装配的属性同名的bean，例如有setMaster()方法的bean会去找名为master的bean定义\nbyType：如果容器中恰好存在一个属性类型的bean，则让属性自动装配；如果存在多个，则需要使用byType自动装配\nconstructor：与byType类似，但适用于构造函数参数，如果容器中没有一个构造函数参数类型的bean，则会引发错误\n\n\n延迟初始化bean：单例bean常常会很早初始化，当不需要提前初始化bean来验证错误时，可以将bean定义标记为延迟初始化来防止单例bean的预实例化。lazy bean不会被太早实例化，not.lazy bean会被很早实例化\n&lt;bean id&#x3D;&quot;lazy&quot; class&#x3D;&quot;com.something.ExpensiveToCreateBean&quot; lazy-init&#x3D;&quot;true&quot;&#x2F;&gt;\n&lt;bean name&#x3D;&quot;not.lazy&quot; class&#x3D;&quot;com.something.AnotherBean&quot;&#x2F;&gt;\n\n\n自动装配原理（条件注解+依赖注入）\n\n首先，Spring Boot会根据classpath下的依赖以及配置文件中的设置，自动扫描并加载相应的自动配置类\n其次，自动配置类中使用了条件注解，根据条件判断是否要进行自动装配。条件注解可以根据一些特定的条件，如某个类是否在classpath中、某个配置是否存在等，来决定是否要进行自动装配\n最后，当条件满足时，自动配置类会自动注册和配置相应的Bean，将它们添加到Spring容器中。这样，在应用程序启动时，Spring Boot就会自动完成大部分配置工作，开发者无需手动配置\n\n\n\n2.设计模式\n单例模式：一个类在同一进程内只允许创建一个对象（或实例），通过将构造函数声明为private，并在内部以static final的方式创建对象并返回的方式实现，具体实现如下：\n\n模版代码\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;不支持延迟加载&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic class IdGenerator&#123;\n  private AtomicLong id &#x3D; new AtomicLong(10);\n  private static final IdGenerator instance &#x3D; new IdGenerator();\n  private IdGenerator()&#123;&#125;\n  private static IdGenerator getInstance()&#123;return instance;&#125;\n  public long getId()&#123;return id.incrementAndGet();&#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;支持延迟加载（需要解决线程安全问题）&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic class IdGenerator&#123;\n  private AtomicLong id &#x3D; new AtomicLong(10);\n  private static final IdGenerator instance;\n  private IdGenerator()&#123;&#125;\n  private static IdGenerator getInstance()&#123;\n    if(instance &#x3D;&#x3D; null)&#123;\n      synchronized(IdGenerator.class)&#123;\n        if(instance &#x3D;&#x3D; null)&#123;\n          instance &#x3D; new IdGenerator();\n        &#125;\n      &#125;\n    &#125;&#x2F;&#x2F;if(instance &#x3D;&#x3D; null)\n    return instance;\n  &#125;\n  public long getId()&#123;return id.incrementAndGet();&#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;有参构造&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic synchronized static Singleton getInstance(int param) &#123; \n    if (instance &#x3D;&#x3D; null) &#123; \n        instance &#x3D; new Singleton(param); \n    &#125; else if (instance.param &#x3D;&#x3D; param) &#123;\n        return instance;\n    &#125; else &#123;\n        instance &#x3D; new Singleton(paramA, paramB);\n    &#125;\n    return instance; \n&#125;\nSingleton singleton &#x3D; Singleton.getInstance(10);\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;利用静态内部类在被调用时才加载（与外部类加载时间无关）的特性来实现&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nprivate static class SingletonHolder&#123;\n  private static final IdGenerator instance &#x3D; new IdGenerator();\n&#125;\npublic static IdGenerator getInstance() &#123;\n  return SingletonHolder.instance;\n&#125;\n\n特殊形式\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;线程唯一单例实现&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nprivate static final ConcurrentHashMap&lt;Long, IdGenerator&gt; instances &#x3D; new ConcurrentHashMap&lt;&gt;();\npublic static IdGenerator getInstance() &#123;\n    Long currentThreadId &#x3D; Thread.currentThread().getId();\n    instances.putIfAbsent(currentThreadId, new IdGenerator());\n    return instances.get(currentThreadId);\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;集群唯一单例实现&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F;不同进程间共享同一个对象需要将单例序列化到外部共享存储区，使用时再反序列化回来，通过给对象加锁保证保证唯一性\nprivate static SharedObjectStorage storage &#x3D; FileSharedObjectStorage(&#x2F;*入参省略，比如文件地址*&#x2F;);\nprivate static DistributedLock lock &#x3D; new DistributedLock();\npublic synchronized static IdGenerator getInstance()&#123;\n    if (instance &#x3D;&#x3D; null) &#123;\n        lock.lock();\n        instance &#x3D; storage.load(IdGenerator.class);\n    &#125;\n\t\treturn instance;\n&#125;\npublic synchroinzed void freeInstance() &#123;\n    storage.save(this, IdGeneator.class);\n    instance &#x3D; null; &#x2F;&#x2F;释放对象\n    lock.unlock();\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;多例模式&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nprivate static final Map&lt;Long, BackendServer&gt; serverInstances &#x3D; new HashMap&lt;&gt;();\nstatic &#123;\n  serverInstances.put(1L, new BackendServer(1L, &quot;192.134.22.138:8080&quot;));\n  serverInstances.put(2L, new BackendServer(2L, &quot;192.134.22.139:8080&quot;));\n  serverInstances.put(3L, new BackendServer(3L, &quot;192.134.22.140:8080&quot;));\n&#125;\npublic BackendServer getInstance(long serverNo) &#123;\n  return serverInstances.get(serverNo);\n&#125;\n&#x2F;&#x2F;同一类型的只能创建一个对象，不同类型的可以创建多个对象\nprivate static final ConcurrentHashMap&lt;String, Logger&gt; instances &#x3D; new ConcurrentHashMap&lt;&gt;();\npublic static Logger getInstance(String loggerName) &#123;\n  instances.putIfAbsent(loggerName, new Logger());\n  return instances.get(loggerName);\n&#125;\n&#x2F;&#x2F;l1&#x3D;&#x3D;l2, l1!&#x3D;l3\nLogger l1 &#x3D; Logger.getInstance(&quot;User.class&quot;);\nLogger l2 &#x3D; Logger.getInstance(&quot;User.class&quot;);\nLogger l3 &#x3D; Logger.getInstance(&quot;Order.class&quot;);\nSpring中的Bean默认都是singleton的，Spring通过ConcurrentHashMap实现单例注册表的特殊方式实现单例模式\n\n线程安全问题：尽量避免定义可变的成员变量（大部分Bean如Dao、Service都没有实例变量，是线程安全的），如果需要定义则使用ThreadLocal成员变量\n\n&#x2F;&#x2F; 通过 ConcurrentHashMap（线程安全） 实现单例注册表\nprivate final Map&lt;String, Object&gt; singletonObjects &#x3D; new ConcurrentHashMap&lt;String, Object&gt;(64);\n\npublic Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123;\n        Assert.notNull(beanName, &quot;&#39;beanName&#39; must not be null&quot;);\n        synchronized (this.singletonObjects) &#123;\n            &#x2F;&#x2F; 检查缓存中是否存在实例\n            Object singletonObject &#x3D; this.singletonObjects.get(beanName);\n            if (singletonObject &#x3D;&#x3D; null) &#123;\n                &#x2F;&#x2F;...省略了很多代码\n                try &#123;\n                    singletonObject &#x3D; singletonFactory.getObject();\n                &#125;\n                &#x2F;&#x2F;...省略了很多代码\n                &#x2F;&#x2F; 如果实例对象在不存在，我们注册到单例注册表中。\n                addSingleton(beanName, singletonObject);\n            &#125;\n            return (singletonObject !&#x3D; NULL_OBJECT ? singletonObject : null);\n        &#125;\n    &#125;\n    &#x2F;&#x2F;将对象添加到单例注册表\n    protected void addSingleton(String beanName, Object singletonObject) &#123;\n            synchronized (this.singletonObjects) &#123;\n                this.singletonObjects.put(beanName, (singletonObject !&#x3D; null ? singletonObject : NULL_OBJECT));\n\n            &#125;\n        &#125;\n&#125;\n\n\n工厂模式\n\n工厂方法：涉及多个if-else和复杂的对象创建语句时，将创建代码抽象出一个新的类作为对象创建的工厂，封装对象的创建过程，将对象的创建和使用相分离，统一调度。还可以利用多态去掉if分支（实现接口，简化插入），利用工厂的工厂来简化使用，符合开闭原则\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;简单实现，构造函数直接抽取出来&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic class RuleConfigParserFactory &#123;\n    public static IRuleConfigParser createParser(String configFormat) &#123;\n        IRuleConfigParser parser &#x3D; null;\n        if (&quot;json&quot;.equalsIgnoreCase(configFormat)) &#123;\n            parser &#x3D; new JsonRuleConfigParser();\n        &#125; else if (&quot;xml&quot;.equalsIgnoreCase(configFormat)) &#123;\n            parser &#x3D; new XmlRuleConfigParser();\n        &#125; else if (&quot;yaml&quot;.equalsIgnoreCase(configFormat)) &#123;\n            parser &#x3D; new YamlRuleConfigParser();\n        &#125; else if (&quot;properties&quot;.equalsIgnoreCase(configFormat)) &#123;\n            parser &#x3D; new PropertiesRuleConfigParser();\n        &#125;\n        return parser;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;多态，利用接口来满足开闭原则，适合每个对象的创建都很复杂的情况，避免设计一个大而全的工厂方法&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic interface IRuleConfigParserFactory &#123; IRuleConfigParser createParser(); &#125;\npublic class JsonRuleConfigParserFactory implements IRuleConfigParserFactory &#123;\n    @Override\n    public IRuleConfigParser createParser() &#123; return new JsonRuleConfigParser(); &#125;\n&#125;\n&#x2F;&#x2F;省略XmlRuleConfigParserFactory，YamlRuleConfigParserFactory，PropertiesRuleConfigParserFactory\n\npublic class RuleConfigParserFactoryMap &#123; &#x2F;&#x2F;工厂的工厂\n    private static final Map&lt;String, IRuleConfigParserFactory&gt; cachedFactories &#x3D; new HashMap&lt;&gt;();\n  \t&#x2F;&#x2F;缓存，为了节省时间提前创建并缓存对象\n    static &#123;\n        cachedFactories.put(&quot;json&quot;, new JsonRuleConfigParserFactory());\n        cachedFactories.put(&quot;xml&quot;, new XmlRuleConfigParserFactory());\n        cachedFactories.put(&quot;yaml&quot;, new YamlRuleConfigParserFactory());\n        cachedFactories.put(&quot;properties&quot;, new PropertiesRuleConfigParserFactory());\n    &#125;\n\n    public static IRuleConfigParserFactory getParserFactory(String type) &#123;\n        if (type &#x3D;&#x3D; null || type.isEmpty()) &#123;\n            return null;\n        &#125;\n        IRuleConfigParserFactory parserFactory &#x3D; cachedFactories.get(type.toLowerCase());\n        return parserFactory;\n    &#125;\n&#125;\n&#x2F;&#x2F;IRuleConfigParserFactory parserFactory &#x3D; \t\tRuleConfigParserFactoryMap.getParserFactory(ruleConfigFileExtension);&#x2F;&#x2F;利用接口接收对应类型的工厂\n&#x2F;&#x2F;IRuleConfigParser parser &#x3D; parserFactory.createParser();\n抽象工厂：传统工厂方法类只有一种分类方式，对应的构造函数会成倍的增长，抽象工厂可以让一个工厂负责多个不同类型的对象创建，而不是只创建一类对象\npublic interface IConfigParserFactory &#123;\n    IRuleConfigParser createRuleParser();\n    ISystemConfigParser createSystemParser();\n    &#x2F;&#x2F;此处可以扩展新的parser类型，比如IBizConfigParser\n&#125;\n\npublic class JsonConfigParserFactory implements IConfigParserFactory &#123;\n    @Override\n    public IRuleConfigParser createRuleParser() &#123;\n        return new JsonRuleConfigParser();\n    &#125;\n\n    @Override\n    public ISystemConfigParser createSystemParser() &#123;\n        return new JsonSystemConfigParser();\n    &#125;\n&#125;\n&#x2F;&#x2F; 省略XmlConfigParserFactory、YamlConfigParserFactory和PropertiesConfigParserFactory代码\n依赖注入（IOC、DI、Container）\n\nDI容器：设计思路基于工厂模式，底层相当于一个大的工厂类，负责在程序启动的时候根据配置（要创建哪些类的对象、每个对象依赖哪些对象的创建）事先创建好对象，当应用程序需要某个类对象的时候，直接从容器中获取即可。称为容器是因为框架持有一堆对象\n核心功能\n配置解析：从配置文件中读取类对象和创建类对象的相关信息，根据配置文件创建对象，比如Spring的xml文件中的bean\n对象创建：通过一个工厂类，如BeansFactory来以反射的方式，在程序运行过程中动态地加载类、创建对象，不需要一次创建完所有需要的对象\n对象声明周期管理：单例模式（全局只有一个）、原型模式（每次都新创建一个）、懒加载（对象用到的时候再创建）\ninit-method：增加此配置，可以在创建对象之前调用指定方法来初始化对象\ndestroy-method：增加此配置，可以在销毁对象之后调用指定方法来做一些清理工作\n\n\n\n\n示例：通过BeanFactory或ApplicationContext创建Bean对象，其中BeanFactory延迟注入，懒加载，在使用到Bean的时候才会注入；而ApplicationContext在容器启动的时候，一次创建了所有Bean，但是ApplicationContext扩展了BeanFactory的功能，有以下实现类\nClassPathXmlApplicationContext，通过解析配置文件得到配置信息（BeanDefinition），根据配置信息使用BeansFactory通过反射来创建对象并返回\nFileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息\nXmlWebApplicationContext：从 Web 系统中的 XML 文件载入上下文定义信息\n\n\n\n\n\n\n观察者模式：在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知\n\n模版代码\npublic interface Subject &#123;\n    &#x2F;&#x2F;也可起名为attach\n    void registerObserver(Observer observer);\n    &#x2F;&#x2F;也可起名为detach\n    void removeObserver(Observer observer);\n    &#x2F;&#x2F;一个一个通知\n    void notifyObservers(Message message);\n&#125;\n&#x2F;&#x2F;被依赖的对象叫被观察者（Observable）；依赖的对象叫观察者（Observer）\npublic interface Observer &#123;\n    void update(Message message);\n&#125;\n&#x2F;&#x2F;Concrete：具体的\npublic class ConcreteSubject implements Subject &#123;\n    private List&lt;Observer&gt; observers &#x3D; new ArrayList&lt;Observer&gt;();\n\n    @Override\n    public void registerObserver(Observer observer) &#123;\n        observers.add(observer);\n    &#125;\n\n    @Override\n    public void removeObserver(Observer observer) &#123;\n        observers.remove(observer);\n    &#125;\n\n    @Override\n    public void notifyObservers(Message message) &#123;  &#x2F;&#x2F;通知所有依赖的对象，响应通知调用对应的代码\n      \t&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;同步阻塞和异步非阻塞二选一&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n      \t&#x2F;&#x2F;同步阻塞方式\n        for (Observer observer : observers) &#123;\n            observer.update(message);\n        &#125;\n      \t&#x2F;&#x2F;异步非阻塞方式，新启线程或直接引入线程池\n      \t&#x2F;&#x2F;private Executor executor &#x2F;&#x2F; 线程池\n      \tfor(Observer observer : observers) &#123;\n          executor.execute(new Runnable()&#123;\n            @Override\n            public void run()&#123;\n              observer.update(messgae);\n            &#125;\n          &#125;)\n        &#125;\n    &#125;\n&#125;\n\npublic class ConcreteObserverOne implements Observer &#123;\n    @Override\n    public void update(Message message) &#123;\n        &#x2F;&#x2F;TODO: 获取消息通知，执行自己的逻辑...\n        System.out.println(&quot;ConcreteObserverOne is notified.&quot;);\n    &#125;\n&#125;\n\npublic class ConcreteObserverTwo implements Observer &#123;\n    @Override\n    public void update(Message message) &#123;\n        &#x2F;&#x2F;TODO: 获取消息通知，执行自己的逻辑...\n        System.out.println(&quot;ConcreteObserverTwo is notified.&quot;);\n    &#125;\n&#125;\n\npublic class Demo &#123;\n    public static void main(String[] args) &#123;\n        ConcreteSubject subject &#x3D; new ConcreteSubject();\n        subject.registerObserver(new ConcreteObserverOne());\n        subject.registerObserver(new ConcreteObserverTwo());\n        subject.notifyObservers(new Message());\n    &#125;\n&#125;\nEventBus框架（事件总线）：提供了观察者模式的骨架代码，如Google Guava中的EventBus，同时支持同步阻塞和异步非阻塞\npublic class UserController &#123;\n    private UserService userService; &#x2F;&#x2F; 依赖注入\n\n    private EventBus eventBus;\n    private static final int DEFAULT_EVENTBUS_THREAD_POOL_SIZE &#x3D; 20;\n\n    public UserController() &#123;\n        &#x2F;&#x2F;eventBus &#x3D; new EventBus(); &#x2F;&#x2F; 同步阻塞模式\n        eventBus &#x3D; new AsyncEventBus(Executors.newFixedThreadPool\n                                     (DEFAULT_EVENTBUS_THREAD_POOL_SIZE)); &#x2F;&#x2F; 异步非阻塞\n    &#125;\n\n    public void setRegObservers(List&lt;Object&gt; observers) &#123;\n        for (Object observer : observers) &#123;\n            &#x2F;&#x2F;用来注册任何类型（Object）的观察者，而在经典的观察者模式的实现中，\n            &#x2F;&#x2F;register() 函数必须接受实现了同一 Observer 接口的类对象。\n            eventBus.register(observer);\n        &#125;\n    &#125;\n\n    public Long register(String telephone, String password) &#123;\n        long userId &#x3D; userService.register(telephone, password);\n        \n\t\t\t\t&#x2F;&#x2F;用来给观察者发送信息，当调用post()函数发送信息的时候，并非把消息发送给所有的观察者，而是发送给可匹配的\n      \t&#x2F;&#x2F;观察者，即接受的消息类型是发送消息（post函数定义中的event）类型的父类\n        eventBus.post(userId);\n\n        return userId;\n    &#125;\n&#125;\npublic class RegPromotionObserver &#123;\n    private PromotionService promotionService; &#x2F;&#x2F; 依赖注入\n\n    @Subscribe&#x2F;&#x2F;定义能接收的消息类型\n    public void handleRegSuccess(Long userId) &#123;\n        promotionService.issueNewUserExperienceCash(userId);\n    &#125;\n&#125;\n\npublic class RegNotificationObserver &#123;\n    private NotificationService notificationService;\n\n    @Subscribe\n    public void handleRegSuccess(Long userId) &#123;\n        notificationService.sendInboxMessage(userId, &quot;...&quot;);\n    &#125;\n&#125;\n&#x2F;&#x2F;@Subscribe注解：当通过 register() 函数将 Observer 类对象注册到 EventBus 的时候，EventBus 会根据 @Subscribe \n&#x2F;&#x2F;注解找到 f1() 和 f2()，并且将两个函数能接收的消息类型记录下来（PMsg-&gt;f1，QMsg-&gt;f2）。当我们通过 post() 函数发送\n&#x2F;&#x2F;消息（比如 QMsg 消息）的时候，EventBus 会通过之前的记录（QMsg-&gt;f2），调用相应的函数（f2）\npublic DObserver &#123;\n  &#x2F;&#x2F;...省略其他属性和方法...\n  \n  @Subscribe\n  public void f1(PMsg event) &#123; &#x2F;&#x2F;... &#125;\n  \n  @Subscribe\n  public void f2(QMsg event) &#123; &#x2F;&#x2F;... &#125;\n&#125;\n应用\n\n事件角色：Spring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)：\n\nContextStartedEvent：ApplicationContext 启动后触发的事件;\nContextStoppedEvent：ApplicationContext 停止后触发的事件;\nContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件;\nContextClosedEvent：ApplicationContext 关闭后触发的事件\n\n\n事件监听者和事件发布者\n\nApplicationListener充当了事件监听者角色，它是一个接口，只定义了一个 onApplicationEvent()方法来处理ApplicationEvent\npackage org.springframework.context;\nimport java.util.EventListener;\n@FunctionalInterface\npublic interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123;\n    void onApplicationEvent(E var1);\n&#125;\nApplicationEventPublisher充当了事件的发布者，它也是一个接口，publishEvent（）这个方法在AbstractApplicationContext 类中被实现，底层通过ApplicationEventMulticaster来广播出去的\n@FunctionalInterface\npublic interface ApplicationEventPublisher &#123;\n    default void publishEvent(ApplicationEvent event) &#123;\n        this.publishEvent((Object)event);\n    &#125;\n\n    void publishEvent(Object var1);\n&#125;\n\n\n事件流程\n\n定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数；\n\n定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法；\n\n使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息\n\n实现\n&#x2F;&#x2F; 定义一个事件,继承自ApplicationEvent并且写相应的构造函数\npublic class DemoEvent extends ApplicationEvent&#123;\n    private static final long serialVersionUID &#x3D; 1L;\n\n    private String message;\n\n    public DemoEvent(Object source,String message)&#123;\n        super(source);\n        this.message &#x3D; message;\n    &#125;\n\n    public String getMessage() &#123;\n         return message;\n          &#125;\n\n&#x2F;&#x2F; 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；\n@Component\npublic class DemoListener implements ApplicationListener&lt;DemoEvent&gt;&#123;\n\n    &#x2F;&#x2F;使用onApplicationEvent接收消息\n    @Override\n    public void onApplicationEvent(DemoEvent event) &#123;\n        String msg &#x3D; event.getMessage();\n        System.out.println(&quot;接收到的信息是：&quot;+msg);\n    &#125;\n\n&#125;\n&#x2F;&#x2F; 发布事件，可以通过ApplicationEventPublisher  的 publishEvent() 方法发布消息。\n@Component\npublic class DemoPublisher &#123;\n\n    @Autowired\n    ApplicationContext applicationContext;\n\n    public void publish(String message)&#123;\n        &#x2F;&#x2F;发布事件\n        applicationContext.publishEvent(new DemoEvent(this, message));\n    &#125;\n&#125;\n结果：当调用 DemoPublisher 的 publish() 方法的时候，比如 demoPublisher.publish(&quot;你好&quot;) ，控制台就会打印出:接收到的信息是：你好\n\n\n\n\n\n\n\n模版模式：在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤（如Java的IO框架），复用部分已有代码\n\n代码实现：模板方法定义为 final，可以避免被子类重写。需要子类重写的方法定义为 abstract，可以强迫子类去实现。不过，在实际项目开发中，模板模式的实现比较灵活，以上两点都不是必须的\npublic abstract class AbstractClass &#123;\n    &#x2F;&#x2F;定义为final，避免子类重写\n    public final void templateMethod() &#123;\n        &#x2F;&#x2F;...\n        method1();\n        &#x2F;&#x2F;...\n        method2();\n        &#x2F;&#x2F;...\n    &#125;\n\t\t&#x2F;&#x2F;方式一：定义为abstract是为了强迫子类去实现（如InputStream的read方法）\n    protected abstract void method1();\n    protected abstract void method2();\n  \t&#x2F;&#x2F;方式二：虽然没有声明为abstract，但是抛了异常，子类不重写来处理异常就无法使用（如AbstractList的add方法）\n    public void add(int index, E element) &#123;\n      throw new UnsupportedOperationException();\n  \t&#125;\n&#125;\n\npublic class ConcreteClass1 extends AbstractClass &#123;\n    @Override\n    protected void method1() &#123;&#125;\n\n    @Override\n    protected void method2() &#123;&#125;\n&#125;\n\npublic class ConcreteClass2 extends AbstractClass &#123;\n    @Override\n    protected void method1() &#123;&#125;\n\n    @Override\n    protected void method2() &#123;&#125;\n&#125;\nAbstractClass demo &#x3D; ConcreteClass1();\ndemo.templateMethod();\n框架的扩展（Java Servlet）\n\nHttpServlet：处理web应用中的get和post请求，在Tomcat、Jetty这样的Servlet容器启动的时候，会自动加载配置文件中URL和XXXServlet间的映射关系（如/hello对应HelloServlet），容器处理URL请求找到对应的servlet执行service方法（定义在父类HttpServlet中，会调用doGet和doPost方法，然后输出网页）\npublic class HelloServlet extends HttpServlet &#123;\n    @Override\n    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;\n        this.doPost(req, resp);\n    &#125;\n\n    @Override\n    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;\n        resp.getWriter().write(&quot;Hello World.&quot;);\n    &#125;\n&#125;\nHttpServlet的service方法：是一个模版方法，实现了整个HTTP请求的执行流程，但其中的doGet和doPost方法可以由子类来定制实现，可以不修改Servlet框架源码的情况下将业务代码通过扩展点镶嵌到框架中执行\npublic void service(ServletRequest req, ServletResponse res) throws ServletException, IOException\n&#123;\n    HttpServletRequest  request;\n    HttpServletResponse response;\n    if (!(req instanceof HttpServletRequest &amp;&amp;\n          res instanceof HttpServletResponse)) &#123;\n        throw new ServletException(&quot;non-HTTP request or response&quot;);\n    &#125;\n    request &#x3D; (HttpServletRequest) req;\n    response &#x3D; (HttpServletResponse) res;\n    service(request, response);\n&#125;\n&#x2F;&#x2F;参数类型不同\nprotected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException&#123;\n    String method &#x3D; req.getMethod();\n    if (method.equals(METHOD_GET)) &#123;\n        long lastModified &#x3D; getLastModified(req);\n        if (lastModified &#x3D;&#x3D; -1) &#123;\n            &#x2F;&#x2F; servlet doesn&#39;t support if-modified-since, no reason\n            &#x2F;&#x2F; to go through further expensive logic\n            doGet(req, resp);\n        &#125; else &#123;\n            long ifModifiedSince &#x3D; req.getDateHeader(HEADER_IFMODSINCE);\n            if (ifModifiedSince &lt; lastModified) &#123;\n                &#x2F;&#x2F; If the servlet mod time is later, call doGet()\n                &#x2F;&#x2F; Round down to the nearest second for a proper compare\n                &#x2F;&#x2F; A ifModifiedSince of -1 will always be less\n                maybeSetLastModified(resp, lastModified);\n                doGet(req, resp);\n            &#125; else &#123;\n                resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED);\n            &#125;\n        &#125;\n    &#125; else if (method.equals(METHOD_HEAD)) &#123;\n        long lastModified &#x3D; getLastModified(req);\n        maybeSetLastModified(resp, lastModified);\n        doHead(req, resp);\n    &#125; else if (method.equals(METHOD_POST)) &#123;\n        doPost(req, resp);\n    &#125; else if (method.equals(METHOD_PUT)) &#123;\n        doPut(req, resp);\n    &#125; else if (method.equals(METHOD_DELETE)) &#123;\n        doDelete(req, resp);\n    &#125; else if (method.equals(METHOD_OPTIONS)) &#123;\n        doOptions(req,resp);\n    &#125; else if (method.equals(METHOD_TRACE)) &#123;\n        doTrace(req,resp);\n    &#125; else &#123;\n        String errMsg &#x3D; lStrings.getString(&quot;http.method_not_implemented&quot;);\n        Object[] errArgs &#x3D; new Object[1];\n        errArgs[0] &#x3D; method;\n        errMsg &#x3D; MessageFormat.format(errMsg, errArgs);\n        resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg);\n    &#125;\n&#125;\n\n\n回调（Callback）\n\n实现：相对于普通的函数调用来说，回调是一种双向调用关系。A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数。这里的 F 函数就是“回调函数”。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”\npublic interface ICallback &#123;\n    void methodToCallback();\n&#125;\n\npublic class BClass &#123;\n    public void process(ICallback callback) &#123;\n        &#x2F;&#x2F;...\n        callback.methodToCallback();\n        &#x2F;&#x2F;...\n    &#125;\n&#125;\n\npublic class AClass &#123;\n    public static void main(String[] args) &#123;\n        BClass b &#x3D; new BClass();\n        &#x2F;&#x2F;使用包裹了回调函数的类对象，我们简称为回调对象\n        &#x2F;&#x2F;A实现了methodToCallback()，并且调用process()，process()函数又调用了methodToCallback()\n        b.process(new ICallback() &#123; &#x2F;&#x2F;回调对象\n            @Override\n            public void methodToCallback() &#123;\n                System.out.println(&quot;Call back me.&quot;);\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n&#x2F;&#x2F;异步回调：通过三方支付系统来实现支付功能，用户在发起支付请求之后，一般不会一直阻塞到支付结果返回，而是注册回调接口\n&#x2F;&#x2F;（类似回调函数，一般是一个回调用的 URL）给三方支付系统，等三方支付系统执行完成之后，将结果通过回调接口返回给用户\nJdbcTemplate：Spring 提供了很多 Template 类，比如，JdbcTemplate、RedisTemplate、RestTemplate。尽管都叫作 xxxTemplate，但它们并非基于模板模式来实现的，而是基于回调来实现的，确切地说应该是同步回调。JdbcTemplate 通过回调的机制，将不变的执行流程抽离出来，放到模板方法execute()中，将可变的部分设计成回调StatementCallback，由用户来定制。query()函数是对execute()函数的二次封装，让接口用起来更加方便\n@Override\npublic &lt;T&gt; List&lt;T&gt; query(String sql, RowMapper&lt;T&gt; rowMapper) throws DataAccessException &#123;\n    return query(sql, new RowMapperResultSetExtractor&lt;T&gt;(rowMapper));\n&#125;\n\n@Override\npublic &lt;T&gt; T query(final String sql, final ResultSetExtractor&lt;T&gt; rse) throws DataAccessException &#123;\n    Assert.notNull(sql, &quot;SQL must not be null&quot;);\n    Assert.notNull(rse, &quot;ResultSetExtractor must not be null&quot;);\n    if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Executing SQL query [&quot; + sql + &quot;]&quot;); &#125;\n\t\t&#x2F;&#x2F;回调，实现了StatementCallback，由用户来定制，封装了execute模版方法\n    class QueryStatementCallback implements StatementCallback&lt;T&gt;, SqlProvider &#123;\n        @Override\n        public T doInStatement(Statement stmt) throws SQLException &#123;\n            ResultSet rs &#x3D; null;\n            try &#123;\n                rs &#x3D; stmt.executeQuery(sql);\n                ResultSet rsToUse &#x3D; rs;\n                if (nativeJdbcExtractor !&#x3D; null) &#123;\n                    rsToUse &#x3D; nativeJdbcExtractor.getNativeResultSet(rs);\n                &#125;\n                return rse.extractData(rsToUse);\n            &#125;\n            finally &#123;\n                JdbcUtils.closeResultSet(rs);\n            &#125;\n        &#125;\n        @Override\n        public String getSql() &#123;\n            return sql;\n        &#125;\n    &#125;\n\n    return execute(new QueryStatementCallback());\n&#125;\n\n@Override\npublic &lt;T&gt; T execute(StatementCallback&lt;T&gt; action) throws DataAccessException &#123;\n    Assert.notNull(action, &quot;Callback object must not be null&quot;);\n\n    Connection con &#x3D; DataSourceUtils.getConnection(getDataSource());\n    Statement stmt &#x3D; null;\n    try &#123;\n        Connection conToUse &#x3D; con;\n        if (this.nativeJdbcExtractor !&#x3D; null &amp;&amp;\n            this.nativeJdbcExtractor.isNativeConnectionNecessaryForNativeStatements()) &#123;\n            conToUse &#x3D; this.nativeJdbcExtractor.getNativeConnection(con);\n        &#125;\n        stmt &#x3D; conToUse.createStatement();\n        applyStatementSettings(stmt);\n        Statement stmtToUse &#x3D; stmt;\n        if (this.nativeJdbcExtractor !&#x3D; null) &#123;\n            stmtToUse &#x3D; this.nativeJdbcExtractor.getNativeStatement(stmt);\n        &#125;\n        T result &#x3D; action.doInStatement(stmtToUse);\n        handleWarnings(stmt);\n        return result;\n    &#125;\n    catch (SQLException ex) &#123;\n        &#x2F;&#x2F; Release Connection early, to avoid potential connection pool deadlock\n        &#x2F;&#x2F; in the case when the exception translator hasn&#39;t been initialized yet.\n        JdbcUtils.closeStatement(stmt);\n        stmt &#x3D; null;\n        DataSourceUtils.releaseConnection(con, getDataSource());\n        con &#x3D; null;\n        throw getExceptionTranslator().translate(&quot;StatementCallback&quot;, getSql(action), ex);\n    &#125;\n    finally &#123;\n        JdbcUtils.closeStatement(stmt);\n        DataSourceUtils.releaseConnection(con, getDataSource());\n    &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;使用&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic class JdbcTemplateDemo &#123;\n    private JdbcTemplate jdbcTemplate;\n\n    public User queryUser(long id) &#123;\n        String sql &#x3D; &quot;select * from user where id&#x3D;&quot;+id;\n        return jdbcTemplate.query(sql, new UserRowMapper()).get(0);\n    &#125;\n\n    class UserRowMapper implements RowMapper&lt;User&gt; &#123;\n        public User mapRow(ResultSet rs, int rowNum) throws SQLException &#123;\n            User user &#x3D; new User();\n            user.setId(rs.getLong(&quot;id&quot;));\n            user.setName(rs.getString(&quot;name&quot;));\n            user.setTelephone(rs.getString(&quot;telephone&quot;));\n            return user;\n        &#125;\n    &#125;\n&#125;\n回调与模版方法的对比，回调相对于模版模式会更加灵活\n\n像 Java 这种只支持单继承的语言，基于模板模式编写的子类，已经继承了一个父类，不再具有继承的能力。\n回调可以使用匿名类来创建回调对象，可以不用事先定义类；而模板模式针对不同的实现都要定义不同的子类。\n如果某个类中定义了多个模板方法，每个方法都有对应的抽象方法，那即便我们只用到其中的一个模板方法，子类也必须实现所有的抽象方法。而回调就更加灵活，我们只需要往用到的模板方法中注入回调对象即可。\n\n\n\n\n\n\n职责链模式：将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止\n\n模版代码\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;实现一&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic abstract class Handler &#123;\n    protected Handler successor &#x3D; null;\n\n    public void setSuccessor(Handler successor) &#123;\n        this.successor &#x3D; successor;\n    &#125;\n\n    public final void handle() &#123;\n        &#x2F;&#x2F;如果没有handled判断，则可以全部处理一遍\n        boolean handled &#x3D; doHandle();\n        if (successor !&#x3D; null &amp;&amp; !handled) &#123;\n            successor.handle();\n        &#125;\n    &#125;\n    protected abstract boolean doHandle();\n&#125;\n\npublic class HandlerA extends Handler &#123;\n    @Override\n    protected boolean doHandle() &#123;\n        boolean handled &#x3D; false;\n        &#x2F;&#x2F;...\n        return handled;\n    &#125;\n&#125;\n\npublic class HandlerB extends Handler &#123;\n    @Override\n    protected boolean doHandle() &#123;\n        boolean handled &#x3D; false;\n        &#x2F;&#x2F;...\n        return handled;\n    &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;实现二&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic interface IHandler &#123;\n    boolean handle();\n&#125;\n\npublic class HandlerA implements IHandler &#123;\n    @Override\n    public boolean handle() &#123;\n        boolean handled &#x3D; false;\n        &#x2F;&#x2F;...\n        return handled;\n    &#125;\n&#125;\n\npublic class HandlerB implements IHandler &#123;\n    @Override\n    public boolean handle() &#123;\n        boolean handled &#x3D; false;\n        &#x2F;&#x2F;...\n        return handled;\n    &#125;\n&#125;\npublic class HandlerChain &#123;\n    private List&lt;IHandler&gt; handlers &#x3D; new ArrayList&lt;&gt;();\n\n    public void addHandler(IHandler handler) &#123;\n        this.handlers.add(handler);\n    &#125;\n\n    public void handle() &#123;\n        for (IHandler handler : handlers) &#123;\n            boolean handled &#x3D; handler.handle();\n            if (handled) &#123;\n                break;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n&#x2F;&#x2F; 使用举例\npublic class Application &#123;\n    public static void main(String[] args) &#123;\n        HandlerChain chain &#x3D; new HandlerChain();\n        chain.addHandler(new HandlerA());\n        chain.addHandler(new HandlerB());\n        chain.handle();\n    &#125;\n&#125;\nServelt Filter：可以实现对HTTP请求的过滤功能，比如鉴权、限流、记录日志、验证参数\npublic class LogFilter implements Filter &#123;\n  @Override\n  public void init(FilterConfig filterConfig) throws ServletException &#123;\n    &#x2F;&#x2F; 在创建Filter时自动调用，\n    &#x2F;&#x2F; 其中filterConfig包含这个Filter的配置参数，比如name之类的（从配置文件中读取的）\n  &#125;\n  @Override\n  public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123;\n    System.out.println(&quot;拦截客户端发送来的请求.&quot;);\n    chain.doFilter(request, response);\n    System.out.println(&quot;拦截发送给客户端的响应.&quot;);\n  &#125;\n\n  @Override\n  public void destroy() &#123;\n    &#x2F;&#x2F; 在销毁Filter时自动调用\n  &#125;\n&#125;\n&#x2F;&#x2F; 在web.xml配置文件中添加filter相关的配置（url、相关类）\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;Tomcat&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic final class ApplicationFilterChain implements FilterChain &#123;\n    private int pos &#x3D; 0; &#x2F;&#x2F;当前执行到了哪个filter\n    private int n; &#x2F;&#x2F;filter的个数\n    private ApplicationFilterConfig[] filters;\n    private Servlet servlet;\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response) &#123;\n        if (pos &lt; n) &#123;\n            ApplicationFilterConfig filterConfig &#x3D; filters[pos++];\n            Filter filter &#x3D; filterConfig.getFilter();\n            filter.doFilter(request, response, this);&#x2F;&#x2F;传入this，进行递归调用，实现前后双向拦截\n            &#x2F;&#x2F;System.out.println(&quot;拦截客户端发送来的请求.&quot;);\n            &#x2F;&#x2F;chain.doFilter(request, response);\n    \t\t\t\t&#x2F;&#x2F;System.out.println(&quot;拦截发送给客户端的响应.&quot;);            \n        &#125; else &#123;\n            &#x2F;&#x2F; filter都处理完毕后，执行servlet\n            servlet.service(request, response);\n        &#125;\n    &#125;\n\n    public void addFilter(ApplicationFilterConfig filterConfig) &#123;\n\t\t\t\t&#x2F;&#x2F;去重+扩容\n        filters[n++] &#x3D; filterConfig;\n    &#125;\n&#125;\nSpring Interceptor：\n\n&#x2F;&#x2F;使用：实现HandlerInterceptor接口，重写preHandle、postHandle、afterCompletion三个方法\n&#x2F;&#x2F;原理：在 Spring 框架中，DispatcherServlet 的 doDispatch() 方法来分发请求，它在真正的业务逻辑执行前后，执行 \n&#x2F;&#x2F;HandlerExecutionChain 中的 applyPreHandle() 和 applyPostHandle() 函数，用来实现拦截的功能\npublic class HandlerExecutionChain &#123;\n    private final Object handler;\n    private HandlerInterceptor[] interceptors;\n\n    public void addInterceptor(HandlerInterceptor interceptor) &#123;\n        initInterceptorList().add(interceptor);\n    &#125;\n\n    boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;\n        HandlerInterceptor[] interceptors &#x3D; getInterceptors();\n        if (!ObjectUtils.isEmpty(interceptors)) &#123;\n            for (int i &#x3D; 0; i &lt; interceptors.length; i++) &#123;\n                HandlerInterceptor interceptor &#x3D; interceptors[i];\n                if (!interceptor.preHandle(request, response, this.handler)) &#123;\n                    triggerAfterCompletion(request, response, null);\n                    return false;\n                &#125;\n            &#125;\n        &#125;\n        return true;\n    &#125;\n\n    void applyPostHandle(HttpServletRequest request, HttpServletResponse response, ModelAndView mv) throws Exception &#123;\n        HandlerInterceptor[] interceptors &#x3D; getInterceptors();\n        if (!ObjectUtils.isEmpty(interceptors)) &#123;\n            for (int i &#x3D; interceptors.length - 1; i &gt;&#x3D; 0; i--) &#123;\n                HandlerInterceptor interceptor &#x3D; interceptors[i];\n                interceptor.postHandle(request, response, this.handler, mv);\n            &#125;\n        &#125;\n    &#125;\n\n    void triggerAfterCompletion(HttpServletRequest request, HttpServletResponse response, Exception ex)\n        throws Exception &#123;\n        HandlerInterceptor[] interceptors &#x3D; getInterceptors();\n        if (!ObjectUtils.isEmpty(interceptors)) &#123;\n            for (int i &#x3D; this.interceptorIndex; i &gt;&#x3D; 0; i--) &#123;\n                HandlerInterceptor interceptor &#x3D; interceptors[i];\n                try &#123;\n                    interceptor.afterCompletion(request, response, this.handler, ex);\n                &#125; catch (Throwable ex2) &#123;\n                    logger.error(&quot;HandlerInterceptor.afterCompletion threw exception&quot;, ex2);\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n\n3.原理及源码分析https://javadoop.com/post/spring-ioc\n2.AOP1.基础知识\n专有名词\n\nAspect：跨多个类的关注点的模块化，例如事务管理\nJoin point：程序执行过程中的一个点，例如方法的执行和异常的处理\nAdvice：Aspect在特定的Join point采取的行动，包括around、before、after（running、throwing、finally）\npointcut：匹配连接点的谓词，Advice和pointcut表达式相关联，并在任何与pointcut匹配的join point运行，例如执行具有特定名称的方法\nIntroduction：代表一个类型声明额外的方法或字段，例如可以使用introduction让bean实现isModified接口，来简化缓存\nTarget object：An object being advised by one or more aspects，即被代理对象\nAOP proxy：由AOP框架创建的对象，用于实现aspect\nWeaving：将apsects和其他应用程序类型或对象链接以创建建议对象\n\n\n使用示例\n\n配置启用@AspectJ支持：增加注解@EnableAspectJAutoProxy\n      @Configuration\n      @EnableAspectJAutoProxy\n      public class AppConfig &#123;\n      \n      &#125;\n声明一个切面（aspect）\n      @Aspect\n      public class NotVeryUsefulAspect &#123;\n      \n      &#125;\n声明一个切点（pointcut），有很多方式，详见文档\n&#x2F;&#x2F;名为anyOldTransfer的切入点，与任何名为transfer的方法执行相匹配\n @Pointcut(&quot;execution(* transfer(..))&quot;) &#x2F;&#x2F; the pointcut expression\n private void anyOldTransfer() &#123;&#125; &#x2F;&#x2F; the pointcut signature\n声明一个advice\n      @Aspect\n      public class BeforeExample &#123;\n          @Before(&quot;com.xyz.myapp.CommonPointcuts.dataAccessOperation()&quot;)\n          public void doAccessCheck() &#123;\n              &#x2F;&#x2F; ...\n          &#125;\n      &#125;\n      @Aspect\n      public class AfterReturningExample &#123;\n          @AfterReturning(&quot;com.xyz.myapp.CommonPointcuts.dataAccessOperation()&quot;)\n          public void doAccessCheck() &#123;\n              &#x2F;&#x2F; ...\n          &#125;\n      &#125;\n      @Aspect\n      public class AroundExample &#123;\n          @Around(&quot;com.xyz.myapp.CommonPointcuts.businessService()&quot;)\n          public Object doBasicProfiling(ProceedingJoinPoint pjp) throws Throwable &#123;\n              &#x2F;&#x2F; start stopwatch\n              Object retVal &#x3D; pjp.proceed();\n              &#x2F;&#x2F; stop stopwatch\n              return retVal;\n          &#125;\n      &#125;\n例子\n      @Aspect\n      public class ConcurrentOperationExecutor implements Ordered &#123;\n      \n          private static final int DEFAULT_MAX_RETRIES &#x3D; 2;\n      \n          private int maxRetries &#x3D; DEFAULT_MAX_RETRIES;\n          private int order &#x3D; 1;\n      \n          public void setMaxRetries(int maxRetries) &#123;\n              this.maxRetries &#x3D; maxRetries;\n          &#125;\n      \n          public int getOrder() &#123;\n              return this.order;\n          &#125;\n      \n          public void setOrder(int order) &#123;\n              this.order &#x3D; order;\n          &#125;\n      \n          @Around(&quot;com.xyz.myapp.CommonPointcuts.businessService()&quot;)\n          public Object doConcurrentOperation(ProceedingJoinPoint pjp) throws Throwable &#123;\n              int numAttempts &#x3D; 0;\n              PessimisticLockingFailureException lockFailureException;\n              do &#123;\n                  numAttempts++;\n                  try &#123;\n                      return pjp.proceed();\n                  &#125;\n                  catch(PessimisticLockingFailureException ex) &#123;\n                      lockFailureException &#x3D; ex;\n                  &#125;\n              &#125; while(numAttempts &lt;&#x3D; this.maxRetries);\n              throw lockFailureException;\n          &#125;\n      &#125;\n\n\n对bean做aop增强问题（？）\n\n子类有则用子类的bean，子类没有则用父类的bean\n在父上下文开启增强，父的bean均被增强，子的bean均未被增强；在子上下文开启增强，子的bean均被增强，父的bean未被增强\n要想都被增强，则需要都开启aop的自动配置并且在父类上定义aop\n\n\n原理\n\n代理模式：Spring AOP基于代理模式实现，主要有两种代理方式，JDK动态代理和CGLIB代理。JDK动态代理要求目标类必须实现接口，而CGLIB代理则可以针对没有实现接口的类进行代理。\n切面（Aspect）：切面是将横切关注点模块化的实现。切面通常包含通知（Advice）和切点（Pointcut）。通知是在特定的切点执行的动作，切点则用于定义通知应该在何处执行。\n连接点（Joinpoint）：连接点代表在应用程序中可以插入切面的点，如方法调用、异常处理等。\n织入（Weaving）：织入是将切面应用到目标对象的过程，从而创建代理对象。在Spring AOP中，织入过程发生在运行时。\n\n\n\n\n2.设计模式\n代理模式：在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问（监控、统计、鉴权、限流、事务、幂等、日志），而非加强功能，这是它跟装饰器模式最大的不同（装饰器主要是加强已有功能）\n\n装饰器模式、代理模式、适配器模式\n&#x2F;&#x2F;代理模式中，代理类附加的是跟原始类无关的功能，而在装饰器模式中，装饰器类附加的是跟原始类相关的增强功能\n&#x2F;&#x2F;适配器模式是一种事后补救措施，增加跟原始类不同的接口，让原本因接口不兼容而不能一起工作的类可以一起工作\n&#x2F;&#x2F; 代理模式的代码结构(下面的接口也可以替换成抽象类)\npublic interface IA &#123;\n    void f();\n&#125;\npublic class A impelements IA &#123;\n    public void f() &#123; \n      &#x2F;&#x2F;... \n    &#125;\n&#125;\npublic class AProxy implements IA &#123;\n    private IA a;\n    public AProxy(IA a) &#123;\n        this.a &#x3D; a;\n    &#125;\n\n    public void f() &#123;\n        &#x2F;&#x2F; 新添加的代理逻辑\n        a.f();\n        &#x2F;&#x2F; 新添加的代理逻辑\n    &#125;\n&#125;\nIA a &#x3D; new AProxy(new A());\n&#x2F;&#x2F; 装饰器模式的代码结构(下面的接口也可以替换成抽象类)\npublic interface IA &#123;\n    void f();\n&#125;\npublic class A implements IA &#123;\n    public void f() &#123; \n      &#x2F;&#x2F;... \n    &#125;\n&#125;\npublic class ADecorator implements IA &#123;\n    private IA a;\n    public ADecorator(IA a) &#123;\n        this.a &#x3D; a;\n    &#125;\n\n    public void f() &#123;\n        &#x2F;&#x2F; 功能增强代码\n        a.f();\n        &#x2F;&#x2F; 功能增强代码\n    &#125;\n&#125;\n&#x2F;&#x2F; 类适配器: 基于继承，将不兼容ITarget接口的Adaptee类“转换”为符合ITarget接口定义的类\npublic interface ITarget &#123;\n    void f1();\n    void f2();\n    void fc();\n&#125;\n\npublic class Adaptee &#123;\n    public void fa() &#123; \n        &#x2F;&#x2F;... \n    &#125;\n    public void fb() &#123;\n        &#x2F;&#x2F;... \n    &#125;\n    public void fc() &#123; \n        &#x2F;&#x2F;... \n    &#125;\n&#125;\n\npublic class Adaptor extends Adaptee implements ITarget &#123;\n    public void f1() &#123;\n        super.fa();\n    &#125;\n\n    public void f2() &#123;\n        &#x2F;&#x2F;...重新实现f2()...\n    &#125;\n\n    &#x2F;&#x2F; 这里fc()不需要实现，直接继承自Adaptee，这是跟对象适配器最大的不同点\n&#125;\n静态代理：有接口时实现相同的接口来实现（见前），没有接口时只能通过继承实现（见下）\npublic class A&#123;\n    public void f() &#123; \n      &#x2F;&#x2F;... \n    &#125;\n&#125;\npublic class AProxy extends A &#123;\n    private A a;\n    public AProxy() &#123;\n        this.a &#x3D; new A();\n    &#125;\n\n    public void f() &#123;\n        &#x2F;&#x2F; 新添加的代理逻辑\n        a.f();\n        &#x2F;&#x2F; 新添加的代理逻辑\n    &#125;\n&#125;\nA a &#x3D; new AProxy();\n动态代理：不事先为每个原始类编写代理类，而是在运行的时候，动态地创建原始类对应的代理类，然后在运行的时候，动态地创建原始类对应的代理类，然后在系统中用代理来替换原始类（如Spring AOP）\n\nSpring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用JDK Proxy去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用Cglib生成一个被代理对象的子类来作为代理（也可以使用AspectJ，AOP属于运行时增强而AspectJ属于编译时增强，基于字节码操作）\n\n&#x2F;&#x2F;动态代理类\npublic class MetricsCollectorProxy &#123;\n    private MetricsCollector metricsCollector;\n\n    public MetricsCollectorProxy() &#123;\n        this.metricsCollector &#x3D; new MetricsCollector();\n    &#125;\n\n    &#x2F;&#x2F;封装了代理类的创建\n    public Object createProxy(Object proxiedObject) &#123;\n        &#x2F;&#x2F;返回Class数组，表示Class对象引用的类所实现的所有接口\n        Class&lt;?&gt;[] interfaces &#x3D; proxiedObject.getClass().getInterfaces();\n        DynamicProxyHandler handler &#x3D; new DynamicProxyHandler(proxiedObject);\n        &#x2F;&#x2F;Proxy provides static methods for creating dynamic proxy classes and instances, \n        &#x2F;&#x2F;and it is also the superclass of all dynamic proxy classes created by those \n        &#x2F;&#x2F;methods.\n        return Proxy.newProxyInstance(proxiedObject.getClass().getClassLoader(), interfaces, handler);\n    &#125;\n\n    private class DynamicProxyHandler implements InvocationHandler &#123;\n        private Object proxiedObject;\n\n        public DynamicProxyHandler(Object proxiedObject) &#123;\n            this.proxiedObject &#x3D; proxiedObject;\n        &#125;\n\n        &#x2F;&#x2F;所有方法的调用都会变成调用invoke方法，参数为生成的代理类、要调用的方法、对应的参数\n        @Override\n        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;\n            long startTimestamp &#x3D; System.currentTimeMillis();\n            Object result &#x3D; method.invoke(proxiedObject, args);\n            long endTimeStamp &#x3D; System.currentTimeMillis();\n            long responseTime &#x3D; endTimeStamp - startTimestamp;\n            String apiName &#x3D; proxiedObject.getClass().getName() + &quot;:&quot; + method.getName();\n            RequestInfo requestInfo &#x3D; new RequestInfo(apiName, responseTime, startTimestamp);\n            metricsCollector.recordRequest(requestInfo);\n            return result;\n        &#125;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;MetricsCollectorProxy使用举例\nMetricsCollectorProxy proxy &#x3D; new MetricsCollectorProxy();\nIUserController userController &#x3D; (IUserController) proxy.createProxy(new UserController());\n\n\n装饰者模式：在不改变原始类接口的情况下，对原始类的功能进行增强，并且支持多个装饰器的嵌套使用，如Java的IO类库\n\n用组合代替继承、装饰器类和原始类继承同一父类，从而可以对原始类嵌套多个装饰器类\npublic abstract class InputStream &#123;\n    &#x2F;&#x2F;...\n    public int read(byte b[]) throws IOException &#123; return read(b, 0, b.length); &#125;\n    public int read(byte b[], int off, int len) throws IOException &#123; &#125;\n    &#x2F;&#x2F;...\n&#125;\n\npublic class BufferedInputStream extends InputStream &#123;\n    protected volatile InputStream in;\n    protected BufferedInputStream(InputStream in) &#123;\n        this.in &#x3D; in;\n    &#125;\n\n    &#x2F;&#x2F;...实现基于缓存的读数据接口...  \n&#125;\n\npublic class DataInputStream extends InputStream &#123;\n    protected volatile InputStream in;\n    protected DataInputStream(InputStream in) &#123;\n        this.in &#x3D; in;\n    &#125;\n\n    &#x2F;&#x2F;...实现读取基本类型数据的接口\n&#125;\nInputStream in &#x3D; new FileInputStream(&quot;&#x2F;user&#x2F;wangzheng&#x2F;test.txt&quot;);\nInputStream bin &#x3D; new BufferedInputStream(in);\nDataInputStream din &#x3D; new DataInputStream(bin);\nint data &#x3D; din.readInt();\n为了避免代码重复，Java IO 抽象出了一个装饰器父类 FilterInputStream。InputStream的所有的装饰器类（BufferedInputStream、DataInputStream）都继承自这个装饰器父类。这样，装饰器类只需要实现它需要增强的方法就可以了，其他方法继承装饰器父类的默认实现。例如：BufferedInputStream直接使用了FilterInputStream的close()，而没有重新实现\n\n\n\n适配器模式：是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口，适配器模式主要是将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作\n\n类适配器（继承）\n&#x2F;&#x2F; 类适配器: 基于继承\npublic interface ITarget &#123;\n    void f1();\n    void f2();\n    void fc();\n&#125;\n\npublic class Adaptee &#123;\n    public void fa() &#123;&#125;\n    public void fb() &#123;&#125;\n    public void fc() &#123;&#125;\n&#125;\n\npublic class Adaptor extends Adaptee implements ITarget &#123;\n    public void f1() &#123;\n        super.fa();\n    &#125;\n\n    public void f2() &#123;\n        &#x2F;&#x2F;...重新实现f2()...\n    &#125;\n\n    &#x2F;&#x2F; 这里fc()不需要实现，直接继承自Adaptee，这是跟对象适配器最大的不同点\n&#125;\n对象适配器（组合）\n&#x2F;&#x2F; 对象适配器：基于组合\npublic interface ITarget &#123;\n    void f1();\n    void f2();\n    void fc();\n&#125;\n\npublic class Adaptee &#123;\n    public void fa() &#123;&#125;\n    public void fb() &#123;&#125;\n    public void fc() &#123;&#125;\n&#125;\n\npublic class Adaptor implements ITarget &#123;\n    private Adaptee adaptee;\n    public Adaptor(Adaptee adaptee) &#123; this.adaptee &#x3D; adaptee; &#125;\n    public void f1() &#123; adaptee.fa(); &#125; &#x2F;&#x2F;委托给Adaptee\n    \n    public void f2() &#123;\n        &#x2F;&#x2F;...重新实现f2()...\n    &#125;\n\n    public void fc() &#123;\n        adaptee.fc();\n    &#125;\n&#125;\n应用：封装有缺陷的接口设计、统一不同接口设计、替换依赖的外部系统、兼容老版本接口、适配不同格式的数据\n\n在 Spring MVC 中，DispatcherServlet根据请求信息调用HandlerMapping，解析请求对应的Handler。解析到对应的Handler（也就是我们平常说的Controller控制器）后，开始由HandlerAdapter适配器处理。HandlerAdapter作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller作为需要适配的类（如果不使用，需要自己判断handler（即controller）的类型）\n\n\n\n\n\n3.原理及源码分析https://javadoop.com/post/spring-aop-source\n3.Data Access1.Transaction Management\nSpring的管理事务的方式\n编程式事务：在代码中硬编码(不推荐使用) : 通过 TransactionTemplate或者 TransactionManager 手动管理事务，实际应用中很少使用，但是对于你理解 Spring 事务管理原理有帮助\n声明式事务：在 XML 配置文件中配置或者直接基于注解（推荐使用） : 实际是通过 AOP 实现（基于@Transactional 的全注解方式使用最多）\n\n\n声明式事务：使用@EnableTransactionManagement开启事务注解，使用@Transactional声明事务，其有以下属性\ntransactionManager\n事务的传播特性：@Transactional注解的propagation属性，一共有七种，默认为PROPAGATION_REQUIRED\nPROPAGATION_REQUIRED：当前有服务已经开始一个事务，则加入到该事务，与其一同提交/回滚；如果当前没有事务，则新建一个事务\nPROPAGATION_SUPPORTS：支持当前事务，如果当前没有事务，则以非事务方式执行，通常用于处理非原子性的非核心业务逻辑操作\nPROPAGATION_MANDATORY：支持当前事务，如果当前没有事务，则抛出异常，防止上下文忘记添加事务的兜底手段，将事务需求托管给上下文\nPROPAGATION_REQUIRES_NEW：新建事务，如果当前存在事务，把当前事务挂起，与PROPAGATION_REQUIRED的区别在于前一事务回滚时（被挂起）当前事务不再需要回滚\nPROPAGATION_NOT_SUPPORTED： 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起，可以帮助减少前一事务的事务范围，减少回滚的影响\nPROPAGATION_NEVER： 以非事务方式执行，如果当前存在事务，则抛出异常，要求上下文不能存在事务\nPROPAGATION_NESTED： Nested的事务和它的父事务是相依的，它的提交是要等和它的父事务一块提交的，父事务因子事务回滚时仅回滚到savepoint点，然后继续执行另一分支\n\n\n事务的隔离级别：@Transactional注解的isolation属性，一共有五种，默认为ISOLATION_DEFAULT\nISOLATION_DEFAULT：使用数据库默认的事务隔离级别\nISOLATION_READ_UNCOMMITTED：读未提交，一个事务可以看到另一事务未提交的数据，会产生脏读、不可重复读、幻读\nISOLATION_READ_COMMITTED：读已提交，一个事务只能看到另一事务已提交的数据，可以避免脏读，但是会产生不可重复读、幻读\nISOLATION_REPEATABLE_READ：可重复读，可以防止脏读，不可重复读，但是可能出现幻读\n幻读和不可重复读的侧重点是不同的，不可重复读侧重于数据修改，两次读取到的同一行数据不一样；而幻读侧重于添加或删除，两次查询返回的数据行数不同\n\n\nISOLATION_SERIALIZABLE：可序列化，事务顺序执行，可防止脏读，不可重复读外，还避免了幻读\n\n\ntimeout\nreadOnly\n\n\n事务回滚的场景\n抛出unchecked exception（runtime exception）后会触发事务的回滚，对于checked异常使用try捕获就不会回滚，也可以配置spring参数让其回滚\nspring的事务边界是在调用业务方法之前开始的，业务方法执行完毕之后来执行commit or rollback（Spring默认取决于是否抛出runtime异常）\n如果抛出runtime exception并在你的业务方法中没有catch到的话，事务会回滚。一般不需要在业务方法中catch异常\n\n\n事务的问题：脏读、不可重复读、幻读\n脏读：指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据， 那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的\n不可重复读：指在一个事务内，多次读同一数据，在这个事务还没有结束时，另外一个事务也访问该同一数据，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读\n幻读：当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。\n\n\n\n2.JDBC\n包层次结构\n\norg.springframework.jdbc.core：JdbcTemplate、simple子包中simpleJdbcInsert、SimpleJdbcCall类、namedparam子包中NamedParameterJdbcTemplate\norg.springframework.jdbc.datasource：DataSource类、embedded子包中内嵌数据库的支持（HSQL、H2、Derby）\norg.springframework.jdbc.object：关系数据库的查询、更新、存储程序\norg.springframework.jdbc.support：SQLException、一些工具类\n\n\nJDBC Processing\n\nJdbcTemplate：可以处理资源的创建和释放，主要用于运行数据库的查询、更新、存储，对ResultSet实例执行迭代并提取返回的参数值，捕获JDBC异常并翻译成通用的。方法有：query、queryForObject、queryForList、update、execute\nNamedParameterJdbcTemplate：对JdbcTemplate的一层封装，简化有参数的SQL\nSQLExceptionTranslator：翻译SQLException和org.sf.dao.DataAccessException的接口，其中一个默认实现是SQLErrorCodeSQLExceptionTranslator，可以解析错误码。Spring会将数据操作的异常转换为DataAccessException，无论使用何种数据访问方式，都能使用一样的异常\n\n\nDataSource\n\n连接工厂，使得一个容器或者一个框架从应用代码中隐藏连接池和事务管理问题，业务开发人员不再需要知道连接数据库的信息。\n\n如果是JDBC，那么可以通过JNDI（通过配置而不是放在一起的单一url）来获得数据源；也可以配置数据库连接池（HikariCP/druid）\n\nDriverManagerDataSource和SimpleDriverDataSource仅应该用在测试的时候：\nDriverManagerDataSource dataSource &#x3D; new DriverManagerDataSource();\ndataSource.setDriverClassName(&quot;org.hsqldb.jdbcDriver&quot;);\ndataSource.setUrl(&quot;jdbc:hsqldb:hsql:&#x2F;&#x2F;localhost:&quot;);\ndataSource.setUsername(&quot;sa&quot;);\ndataSource.setPassword(&quot;&quot;);\n\n\n\n3.O/R Mapping（MyBatis）\nJPA：Java Persistence API，为对象关系映射提供了一种基于POJO的持久化模型，简化数据持久化代码的开发工作，为Java社区屏蔽不同持久化API的差异\nLombok：能够自动嵌入IDE和构建工具，提升开发效率\nMyBatis：持久层框架，支持定制化SQL、存储过程和高级映射。免除了几乎所有的JDBC代码及相关配置，可以通过注解来配置和映射原始类型、接口和Java POJO为数据库中的记录\nMyBatis：持久层框架，支持定制化SQL、存储过程和高级映射。免除了几乎所有的JDBC代码及相关配置，可以通过注解来配置和映射原始类型、接口和Java POJO为数据库中的记录\nMyBatis Generator：MyBatis代码生成器，根据数据库表生成相关代码：POJO、Mapper接口、SQL Map XML\nMyBatis PageHelper：用来做分页，支持多种数据库和多种分页方式\n\n\n\n4.MVC1.基础知识\n一些专有名词\n\n\nPOJO：plain old java objects简单的Java对象，一般用在数据层映射到数据库表的类，类的属性与表字段一一对应\nPOJO持久化后⇒PO，persistent object，增加了一些getter、setter方法\nPOJO传输过程中⇒DTO，data transfer object，比如一张表有100个字段，对应的PO有100个属性，但view层只需要10个字段，所以依靠只有10个属性的DTO来传输数据给client，可以提高性能\nPOJO用作表示层⇒VO，view object，用于页面展示\n\n\nDAO：data access object数据访问接口，用来封装对数据库的访问（CRUD），可以把POJO持久化为PO，用PO组装出VO、DTO\nBO：Business Object，即业务对象。一般用在业务层，当业务比较复杂，用到比较多的业务对象时，可用BO类组合封装所有的对象一并传递。\ncontroller层：控制请求url用哪个service层逻辑\nservice层：带有业务逻辑的数据访问API\nMVC模式\n模型（Model）表示应用程序的数据和业务逻辑。它负责处理数据的读取、存储、验证以及与数据库的交互等操作。\n视图（View）是用户界面的呈现部分，负责展示模型中的数据给用户。它可以是一个网页、一个图形界面或者其他任何形式。\n控制器（Controller）充当模型和视图之间的中介，负责处理用户的请求、更新模型的状态，以及决定要显示哪个视图。它接收用户的输入，调用相应的模型方法来处理请求，并最终将结果返回给视图进行显示\n\n\n\n\ncontroller：@Controller，是@Component的一个特例，可与@ResponseBody组合成@RestController\n\n使用@RequestMapping(&quot;/brand&quot;)来确定该类的顶级域名\n\npath和value用于指定映射路径，value是path的别名\nmethod用于指定请求方法：Get、Post等，可用@GetMapping、@PostMapping、@PutMapping、@DeleteMapping、@PatchMapping替代\nparams指定有哪些参数，或没有哪些参数（!）\nheaders用来指定请求头中有哪些参数，或没有哪些参数（!）\nconsumes和produces用于限定请求与响应格式，通过MediaType来指定\n\n\n通过依赖注入来使用Service接口\n@Autowired\nprivate PmsBrandService demoService;\n定义子域名和相应的处理逻辑\n\n抽象出通用返回对象CommonResult，主要有code+message+data三个属性\n抽象出常用的操作码属性\n\n@RequestMapping(value &#x3D; &quot;&#x2F;create&quot;, method &#x3D; RequestMethod.POST)\n    @ResponseBody\n    public CommonResult createBrand(@RequestBody PmsBrand pmsBrand) &#123;\n        CommonResult commonResult;\n        int count &#x3D; demoService.createBrand(pmsBrand);\n        if (count &#x3D;&#x3D; 1) &#123;\n            commonResult &#x3D; CommonResult.success(pmsBrand);\n            LOGGER.debug(&quot;createBrand success:&#123;&#125;&quot;, pmsBrand);\n        &#125; else &#123;\n            commonResult &#x3D; CommonResult.failed(&quot;操作失败&quot;);\n            LOGGER.debug(&quot;createBrand failed:&#123;&#125;&quot;, pmsBrand);\n        &#125;\n        return commonResult;\n    &#125;\n\n\nservice\n\nservice接口：写实体类的增删改查等方法\n\nserviceImpl：@Service\n\n实现service接口\n\n通过依赖注入，使用Mapper接口\n@Autowired\nprivate PmsBrandMapper brandMapper;\n使用xxxMapper接口提供的方法实现service接口中的方法\n\n\n\n\n\n定义处理方法\n\n@RequestBody：方法参数是一个请求体，通过HttpMessageConverter读取请求正文并将其反序列化Object\n@GetMapping(&quot;&#x2F;accounts&#x2F;&#123;id&#125;&quot;)\n@ResponseBody\npublic Account handle() &#123;\n    &#x2F;&#x2F; ...\n&#125;\n@ResponseBody：方法返回一个响应体，通过HttpMessageConverter将返回序列化到响应正文\n@PostMapping(&quot;&#x2F;accounts&quot;)\npublic void handle(@RequestBody Account account) &#123;\n    &#x2F;&#x2F; ...\n&#125;\n@ResponseStatus：返回的状态码，通过HttpStatus来指定\n\n@PathVariable：请求URI中的变量\n&#x2F;&#x2F;样例一\n@GetMapping(&quot;&#x2F;owners&#x2F;&#123;ownerId&#125;&#x2F;pets&#x2F;&#123;petId&#125;&quot;)\npublic Pet findPet(@PathVariable Long ownerId, @PathVariable Long petId) &#123;\n    &#x2F;&#x2F; ...\n&#125;\n&#x2F;&#x2F;样例二\n@Controller\n@RequestMapping(&quot;&#x2F;owners&#x2F;&#123;ownerId&#125;&quot;)\npublic class OwnerController &#123;\n    @GetMapping(&quot;&#x2F;pets&#x2F;&#123;petId&#125;&quot;)\n    public Pet findPet(@PathVariable Long ownerId, @PathVariable Long petId) &#123;\n        &#x2F;&#x2F; ...\n    &#125;\n&#125;\n@RequestParam：请求URI中的参数，可以将Servlet请求参数绑定到控制器中的方法参数\n@GetMapping\npublic String setupForm(@RequestParam(&quot;petId&quot;) int petId, Model model) &#123;\n    Pet pet &#x3D; this.clinic.loadPet(petId);\n    model.addAttribute(&quot;pet&quot;, pet);\n    return &quot;petForm&quot;;\n&#125;\n@MatrixVariable：匹配URI中多个参数中的一个，Matrix Variable中，多个变量用;分隔\n&#x2F;&#x2F; GET &#x2F;pets&#x2F;42;q&#x3D;11;r&#x3D;22\n@GetMapping(&quot;&#x2F;pets&#x2F;&#123;petId&#125;&quot;)\npublic void findPet(@PathVariable String petId, @MatrixVariable int q) &#123;\n    &#x2F;&#x2F; petId &#x3D;&#x3D; 42\n    &#x2F;&#x2F; q &#x3D;&#x3D; 11\n&#125;\n\n\nDIspatcherServlet：首先通过一个dispatchservlet去转接请求，到handlermapping去返回一个执行链，就比如拦截器到哪个controller，返回以后就到handler适配器获取这个请求要求的controller，然后去controller这里返回一个数据或者页面modelandview，然后给前端\n\n请求处理（DispatcherServlet里面）\n\n首先，调用doService方法首先绑定一些属性（应用上下文、还有一些Resolver）\n\n然后，调用doDispatch方法为请求确定Handler，执行Controller前后的处理器逻辑\n&#x2F;&#x2F; Determine handler for the current request.\nmappedHandler &#x3D; getHandler(processedRequest);\n&#x2F;&#x2F;前置处理\nif (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return;&#125;\n&#x2F;&#x2F; Actually invoke the handler.\nmv &#x3D; ha.handle(processedRequest, response, mappedHandler.getHandler());\n&#x2F;&#x2F;后置处理\nmappedHandler.applyPostHandle(processedRequest, response, mv);\n最后，doDispatch中处理Handler返回的Model，呈现视图\nprocessDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);\n\n\n视图解析（xxxResolver）\n\nDispatcherServlet中的initStrategies()初始化了对应的ViewResolver\n前面提到的processDispatchResult会做从视图名到视图的解析，通过render呈现视图，在render中解析出view对象\n\n\n异常处理：实现HandlerExceptionResolver接口，例如拦截所有controller\n@RestControllerAdvice\npublic class GlobalControllerAdvice &#123;\n    @ExceptionHandler(ValidationException.class)\n    @ResponseStatus(HttpStatus.BAD_REQUEST)&#x2F;&#x2F;返回400\n    public Map&lt;String, String&gt; validationExceptionHandler(ValidationException exception) &#123;\n        Map&lt;String, String&gt; map &#x3D; new HashMap&lt;&gt;();\n        map.put(&quot;message&quot;, exception.getMessage());\n        return map;\n    &#125;\n&#125;\n\n\n\n\n2.原理及源码分析\nSpringMVC 的组件\nDispatcherServlet：前置控制器，负责接收 HTTP 请求并委托给 HandlerMapping、HandlerAdapter 和 ViewResolver 等组件处理。\nHandler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：负责将请求映射到对应的 Handler 即控制器(Controller)。\nHandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和 HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。\nHandlerAdapter：负责调用处理器方法并封装处理结果，将其传递给 DispatcherServlet。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。\nViewResolver：视图解析器，负责根据视图名称解析出对应的 View，最终将渲染结果响应给客户端。\n\n\n\n\n5.Testing\nSpringBoot提供了@SpringBootTest注解，当需要SpringBoot的特性时，他是Spring-test的@ContextConfiguration注解的替代品，这个注解会通过SpringApplication创建测试中使用的应用上下文来工作，默认是不开启服务器的，但可以通过webEnvironment来重定义：\nMOCK（默认）：加载一个Web ApplicationContext来提供一个模拟Web环境，不启动嵌入式服务器，但如果类路径上没有可用的Web环境，则此模式会回退到非Web ApplicationContext，\nRANDOM_PORT：加载一个WebServerApplicationContext并提供一个真实的Web环境。嵌入式服务器启动并侦听随机端口。\nDEFINED_PORT：加载一个WebServerApplicationContext并提供一个真实的Web环境，嵌入式服务器侦听定义的端口或默认的端口（8080）\nNONE：使用SpringApplication加载ApplicationContext但不提供任何Web环境\n\n\n@Transactional的测试，这个事务会在测试方法结束后默认回滚\n检测测试配置：@Test会自动搜索主要配置，@TestConfiguration类可以覆盖配置并通过@Import导入配置\n使用模拟（mock）环境测试：@AutoConfigureMockMvc\n模拟和窥探Beans：@MockBean用于定义一个应用上下文中的一个bean的Mockito模拟\nAuto-configured Tests：用来自动载入测试需要的程序片段，可以选择一个@…Test注释并手动包含其它片段的@AutoConfigure…注释\n\n6.MyBatis1.使用\n与Spring整合\n\n引入依赖（org.mybatis.generator）\n\n增加配置文件（generatorConfig.xml），可以根据官网的模版更改\n&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE generatorConfiguration\n        PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD MyBatis Generator Configuration 1.0&#x2F;&#x2F;EN&quot;\n        &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-generator-config_1_0.dtd&quot;&gt;\n\n&lt;!-- root element of a MyBatis Generator configuration file--&gt;\n&lt;generatorConfiguration&gt;\n\n    &lt;properties resource&#x3D;&quot;generator.properties&quot;&#x2F;&gt;\n&lt;!--    flat为任何表生成一个domain类，保存表中的所有字段--&gt;\n    &lt;context id&#x3D;&quot;MySQL&quot; targetRuntime&#x3D;&quot;MyBatis3&quot; defaultModelType&#x3D;&quot;flat&quot;&gt;\n        &lt;property name&#x3D;&quot;beginningDelimiter&quot; value&#x3D;&quot;&#96;&quot;&#x2F;&gt;\n        &lt;property name&#x3D;&quot;endingDelimiter&quot; value&#x3D;&quot;&#96;&quot;&#x2F;&gt;\n        &lt;property name&#x3D;&quot;javaFileEncoding&quot; value&#x3D;&quot;UTF-8&quot;&#x2F;&gt;\n\n        &lt;!-- 为模型生成序列化方法--&gt;\n        &lt;plugin type&#x3D;&quot;org.mybatis.generator.plugins.SerializablePlugin&quot;&#x2F;&gt;\n        &lt;!-- 为生成的Java模型创建一个toString方法 --&gt;\n        &lt;plugin type&#x3D;&quot;org.mybatis.generator.plugins.ToStringPlugin&quot;&#x2F;&gt;\n\n        &lt;!--可以自定义生成model的代码注释--&gt;\n        &lt;commentGenerator type&#x3D;&quot;com.rent.mbg.CommentGenerator&quot;&gt;\n            &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt;\n            &lt;property name&#x3D;&quot;suppressAllComments&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;\n            &lt;property name&#x3D;&quot;suppressDate&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;\n            &lt;property name&#x3D;&quot;addRemarkComments&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;\n        &lt;&#x2F;commentGenerator&gt;\n\n\n        \n        &lt;jdbcConnection driverClass&#x3D;&quot;$&#123;jdbc.driverClass&#125;&quot;\n                        connectionURL&#x3D;&quot;$&#123;jdbc.connectionURL&#125;&quot;\n                        userId&#x3D;&quot;$&#123;jdbc.userId&#125;&quot;\n                        password&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;\n            &lt;!--解决mysql驱动升级到8.0后不生成指定数据库代码的问题--&gt;\n            &lt;property name&#x3D;&quot;nullCatalogMeansCurrent&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;\n        &lt;&#x2F;jdbcConnection&gt;\n\n&lt;!--        &lt;javaTypeResolver &gt;--&gt;\n&lt;!--            &lt;property name&#x3D;&quot;forceBigDecimals&quot; value&#x3D;&quot;false&quot; &#x2F;&gt;--&gt;\n&lt;!--        &lt;&#x2F;javaTypeResolver&gt;--&gt;\n\n        &lt;!--指定生成model的路径--&gt;\n        &lt;javaModelGenerator targetPackage&#x3D;&quot;com.rent.mbg.model&quot; targetProject&#x3D;&quot;src&#x2F;main&#x2F;java&quot;&gt;\n&lt;!--            &lt;property name&#x3D;&quot;enableSubPackages&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;--&gt;\n&lt;!--            &lt;property name&#x3D;&quot;trimStrings&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;--&gt;\n        &lt;&#x2F;javaModelGenerator&gt;\n\n        &lt;!--指定生成mapper.xml的路径--&gt;\n        &lt;sqlMapGenerator targetPackage&#x3D;&quot;com.rent.mbg.mapper&quot;  targetProject&#x3D;&quot;src&#x2F;main&#x2F;resources&quot;&gt;\n&lt;!--            &lt;property name&#x3D;&quot;enableSubPackages&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;--&gt;\n        &lt;&#x2F;sqlMapGenerator&gt;\n\n        &lt;!--指定生成mapper接口的的路径--&gt;\n        &lt;javaClientGenerator type&#x3D;&quot;XMLMAPPER&quot; targetPackage&#x3D;&quot;com.rent.mbg.mapper&quot;  targetProject&#x3D;&quot;src&#x2F;main&#x2F;java&quot;&gt;\n&lt;!--            &lt;property name&#x3D;&quot;enableSubPackages&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;--&gt;\n        &lt;&#x2F;javaClientGenerator&gt;\n\n        &lt;table tableName&#x3D;&quot;pms_brand&quot; &gt;\n&lt;!--            &lt;property name&#x3D;&quot;useActualColumnNames&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;--&gt;\n            &lt;generatedKey column&#x3D;&quot;id&quot; sqlStatement&#x3D;&quot;MySql&quot; identity&#x3D;&quot;true&quot; &#x2F;&gt;\n&lt;!--            &lt;columnOverride column&#x3D;&quot;DATE_FIELD&quot; property&#x3D;&quot;startDate&quot; &#x2F;&gt;--&gt;\n&lt;!--            &lt;ignoreColumn column&#x3D;&quot;FRED&quot; &#x2F;&gt;--&gt;\n&lt;!--            &lt;columnOverride column&#x3D;&quot;LONG_VARCHAR_FIELD&quot; jdbcType&#x3D;&quot;VARCHAR&quot; &#x2F;&gt;--&gt;\n        &lt;&#x2F;table&gt;\n\n    &lt;&#x2F;context&gt;\n&lt;&#x2F;generatorConfiguration&gt;\n写Generator：官网有示例\nList&lt;String&gt; warnings &#x3D; new ArrayList&lt;String&gt;();\nboolean overwrite &#x3D; true;\nFile configFile &#x3D; new File(&quot;generatorConfig.xml&quot;);\nConfigurationParser cp &#x3D; new ConfigurationParser(warnings);\nConfiguration config &#x3D; cp.parseConfiguration(configFile);\nDefaultShellCallback callback &#x3D; new DefaultShellCallback(overwrite);\nMyBatisGenerator myBatisGenerator &#x3D; new MyBatisGenerator(config, callback, warnings);\nmyBatisGenerator.generate(null);\nfor (String warning : warnings) &#123;\n\t\tSystem.out.println(warning);\n&#125;\n添加MyBatis的Java配置\n&#x2F;**\n * MyBatis配置类\n * Created by macro on 2019&#x2F;4&#x2F;8.\n *&#x2F;\n@Configuration\n@MapperScan(&quot;com.macro.mall.tiny.mbg.mapper&quot;)\npublic class MyBatisConfig &#123;\n&#125;\nMyBatis使用XML和Annotation来配置、映射原语、映射接口、和Java POJO到数据库记录\n\nPOJO：只有setter、getter、toString的简单类\n\n配置数据源、编写XML文件（包含sql语句）:https://mybatis.org/mybatis-3/sqlmap-xml.html\n\nMapper XML files\nresultMap：how to load your objects from the database result sets.\nsql：A reusable chunk of SQL that can be referenced by other statements.\ninsert：A mapped INSERT statement.\nupdate：A mapped UPDATE statement.\ndelete：A mapped DELETE statement.\nselect：A mapped SELECT statement.\n\n\nDynamic SQL\nif：条件\nchoose (when, otherwise)：switch选择\ntrim (where, set)：\nforeach：循环\n\n\n\n\n依据XML文件来构建*SqlSessionFactory*\nString resource &#x3D; &quot;org&#x2F;mybatis&#x2F;example&#x2F;mybatis-config.xml&quot;;\nInputStream inputStream &#x3D; Resources.getResourceAsStream(resource);\nSqlSessionFactory sqlSessionFactory &#x3D;\n  new SqlSessionFactoryBuilder().build(inputStream);\n&#x2F;&#x2F;从SqlSessionFactory获得一个SqlSession\ntry (SqlSession session &#x3D; sqlSessionFactory.openSession()) &#123;\n  BlogMapper mapper &#x3D; session.getMapper(BlogMapper.class);\n  Blog blog &#x3D; mapper.selectBlog(101);\n&#125;\n\n\n\n\n用于通过数据库的table信息来生成MyBatis代码，可以合并已有XML重写Java文件\n\n实体类（xxx）：table属性、get、set、toString、序列化、注释（commonGenerator）\n\n映射文件（xxxMapper.xml）：具体的sql语句\n\nMapper接口（xxxMapper）：使用了xxxExample中的条件，与xxxMapper.xml中的sql语句一一对应\nlong countByExample(PmsBrandExample example);&#x2F;&#x2F;按条件计数\nint deleteByExample(PmsBrandExample example);&#x2F;&#x2F;按条件删除\n\nint deleteByPrimaryKey(Long id);&#x2F;&#x2F;按主键删除\n\nint insert(PmsBrand record);&#x2F;&#x2F;插入数据，返回值为ID\n\nint insertSelective(PmsBrand record);&#x2F;&#x2F;插入数据，只插入值不为null的字段，内部动态sql判断\n\nList&lt;PmsBrand&gt; selectByExampleWithBLOBs(PmsBrandExample example);\n\nList&lt;PmsBrand&gt; selectByExample(PmsBrandExample example);&#x2F;&#x2F;按条件查询，传入null表示查询所有\n\nPmsBrand selectByPrimaryKey(Long id);&#x2F;&#x2F;按主键查询\n&#x2F;&#x2F;按条件更新值不为null的字段\nint updateByExampleSelective(@Param(&quot;record&quot;) PmsBrand record, @Param(&quot;example&quot;) PmsBrandExample example);\n&#x2F;&#x2F;BLOB：二进制大对象\nint updateByExampleWithBLOBs(@Param(&quot;record&quot;) PmsBrand record, @Param(&quot;example&quot;) PmsBrandExample example);\n&#x2F;&#x2F;按条件更新\nint updateByExample(@Param(&quot;record&quot;) PmsBrand record, @Param(&quot;example&quot;) PmsBrandExample example);\n&#x2F;&#x2F;按主键更新值不为null的字段\nint updateByPrimaryKeySelective(PmsBrand record);\n\nint updateByPrimaryKeyWithBLOBs(PmsBrand record);\n&#x2F;&#x2F;按主键更新\nint updateByPrimaryKey(PmsBrand record);\n条件扩展类（xxxExample）：定义了一系列方法用来做条件，比如排序、去重、大于、小于、等于、模糊查询、数据在某某之间等\n\nGeneratedCriteria：定义了一系列条件方法，最后都会拼接在SQL中（where语句），但是一般不使用它\nCriteria：一般使用这个子类来进行操作，继承了GeneratedCriteria类\nCriterion：将条件需要的属性抽象出来表示组成一个包装类\n\n\n\n\n\n\n2.设计模式\n工厂模式，工厂模式在 MyBatis 中的典型代表是 SqlSessionFactory\n建造者模式，建造者模式在 MyBatis 中的典型代表是 SqlSessionFactoryBuilder\n单例模式，单例模式在 MyBatis 中的典型代表是 ErrorContext\n适配器模式，适配器模式在 MyBatis 中的典型代表是 Log\n代理模式，代理模式在 MyBatis 中的典型代表是 MapperProxyFactory\n模板方法模式，模板方法在 MyBatis 中的典型代表是 BaseExecutor\n装饰器模式，装饰器模式在 MyBatis 中的典型代表是 Cache\n\n3.原理分析\nMyBatis中创建了一个Mapper接口，在写一个xml文件，java的接口是要实现的，为什么这没有实现呢？\n\nMyBatis中的Mapper接口并不需要实现，它只是定义了一组方法签名。MyBatis会根据Mapper接口中的方法名、参数类型和返回值类型，自动生成实现方法。因此，Mapper接口中的方法不需要实现，也不需要在该接口中编写任何方法体\n相反，你需要编写一个与Mapper接口同名的XML文件，来实现这些方法的具体SQL操作。这样，当你在Java代码中调用Mapper接口中的方法时，MyBatis会自动将该方法映射到对应的XML文件中的SQL语句，并执行该语句\n\n\n与传统的JDBC相比，MyBatis的优点\n\nmybatis的全局配置文件中可以设置数据库连接池，和spring整合可以配置数据库连接\nmybatis把sql和代码分离，提供了Mapper.xml映射文件，在映射文件中通过标签来写sql\nmybatis中自动完成java对象和sql中参数的映射\nmybatis中通过ResultSetHandler自动将结果集映射到对应的java对象中\n\n\nJDBC连接数据库的操作：注册JDBC驱动、打开链接执行查询（sql字符串）、输出所有结果（循环）\n\n加载数据库驱动程序：使用Class.forName()方法加载对应的数据库驱动程序，例如：Class.forName(“com.mysql.jdbc.Driver”);\n\n建立数据库连接：使用DriverManager.getConnection()方法建立与数据库的连接，需要指定数据库的URL、用户名和密码，例如：Connection conn = DriverManager.getConnection(“jdbc:mysql://localhost/mydatabase”, “username”, “password”);\n\n创建Statement对象：使用Connection对象的createStatement()方法创建一个Statement对象，用于执行SQL语句，例如：Statement stmt = conn.createStatement();\n\n执行SQL语句：使用Statement对象的executeQuery()或executeUpdate()方法执行SQL语句，例如：ResultSet rs = stmt.executeQuery(“SELECT * FROM mytable”);\n\n处理查询结果：如果执行的是查询语句，需要使用ResultSet对象来处理查询结果，例如：while (rs.next()) { String name = rs.getString(“name”); int age = rs.getInt(“age”); }\n\n关闭数据库连接：在程序结束时，需要使用Connection对象的close()方法关闭数据库连接，例如：conn.close();\npackage com.runoob.test;\n \nimport java.sql.*;\n \npublic class MySQLDemo &#123;\n \n    &#x2F;&#x2F; MySQL 8.0 以下版本 - JDBC 驱动名及数据库 URL\n    static final String JDBC_DRIVER &#x3D; &quot;com.mysql.jdbc.Driver&quot;;  \n    static final String DB_URL &#x3D; &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;RUNOOB&quot;;\n \n    &#x2F;&#x2F; MySQL 8.0 以上版本 - JDBC 驱动名及数据库 URL\n    &#x2F;&#x2F;static final String JDBC_DRIVER &#x3D; &quot;com.mysql.cj.jdbc.Driver&quot;;  \n    &#x2F;&#x2F;static final String DB_URL &#x3D; &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;RUNOOB?useSSL&#x3D;false&amp;allowPublicKeyRetrieval&#x3D;true&amp;serverTimezone&#x3D;UTC&quot;;\n\n \n    &#x2F;&#x2F; 数据库的用户名与密码，需要根据自己的设置\n    static final String USER &#x3D; &quot;root&quot;;\n    static final String PASS &#x3D; &quot;123456&quot;;\n \n    public static void main(String[] args) &#123;\n        Connection conn &#x3D; null;\n        Statement stmt &#x3D; null;\n        try&#123;\n            &#x2F;&#x2F; 注册 JDBC 驱动\n            Class.forName(JDBC_DRIVER);\n        \n            &#x2F;&#x2F; 打开链接\n            System.out.println(&quot;连接数据库...&quot;);\n            conn &#x3D; DriverManager.getConnection(DB_URL,USER,PASS);\n        \n            &#x2F;&#x2F; 执行查询\n            System.out.println(&quot; 实例化Statement对象...&quot;);\n            stmt &#x3D; conn.createStatement();\n            String sql;\n            sql &#x3D; &quot;SELECT id, name, url FROM websites&quot;;\n            ResultSet rs &#x3D; stmt.executeQuery(sql);\n        \n            &#x2F;&#x2F; 展开结果集数据库\n            while(rs.next())&#123;\n                &#x2F;&#x2F; 通过字段检索\n                int id  &#x3D; rs.getInt(&quot;id&quot;);\n                String name &#x3D; rs.getString(&quot;name&quot;);\n                String url &#x3D; rs.getString(&quot;url&quot;);\n    \n                &#x2F;&#x2F; 输出数据\n                System.out.print(&quot;ID: &quot; + id);\n                System.out.print(&quot;, 站点名称: &quot; + name);\n                System.out.print(&quot;, 站点 URL: &quot; + url);\n                System.out.print(&quot;\\n&quot;);\n            &#125;\n            &#x2F;&#x2F; 完成后关闭\n            rs.close();\n            stmt.close();\n            conn.close();\n        &#125;catch(SQLException se)&#123;\n            &#x2F;&#x2F; 处理 JDBC 错误\n            se.printStackTrace();\n        &#125;catch(Exception e)&#123;\n            &#x2F;&#x2F; 处理 Class.forName 错误\n            e.printStackTrace();\n        &#125;finally&#123;\n            &#x2F;&#x2F; 关闭资源\n            try&#123;\n                if(stmt!&#x3D;null) stmt.close();\n            &#125;catch(SQLException se2)&#123;\n            &#125;&#x2F;&#x2F; 什么都不做\n            try&#123;\n                if(conn!&#x3D;null) conn.close();\n            &#125;catch(SQLException se)&#123;\n                se.printStackTrace();\n            &#125;\n        &#125;\n        System.out.println(&quot;Goodbye!&quot;);\n    &#125;\n&#125;\n\n\n\n\n7.Spring Boot1.Annotation\n@SpringBootApplication：默认加在主类的main方法上，可以看作是@Configuration、@EnableAutoConfiguration、@ComponentScan注解的集合\n\n@EnableAutoConfiguration：启用 SpringBoot 的自动配置机制\n\n@ComponentScan： 扫描被@Component (@Repository,@Service,@Controller)注解修饰的 bean，注解默认会扫描该类所在的包下所有的类\n\n@Configuration：允许在 Spring 上下文中注册额外的 bean 或导入其他配置类\n\n用@Configuration注释一个类表明一个对象时bean定义的来源，此外，Configuration类允许通过调用同一类中的其它@Bean方法来定义bean间的依赖关系\n@Configuration\npublic class AppConfig &#123;\n    @Bean\n    public MyService myService() &#123;\n        return new MyServiceImpl();\n    &#125;\n&#125;\n&#x2F;&#x2F;与下面xml语句相同：\n&#x2F;&#x2F;&lt;beans&gt;\n&#x2F;&#x2F;    &lt;bean id&#x3D;&quot;myService&quot; class&#x3D;&quot;com.acme.services.MyServiceImpl&quot;&#x2F;&gt;\n&#x2F;&#x2F;&lt;&#x2F;beans&gt;\n当@Configuration类作为输入提供时，@Configuration类本身被注册为bean定义，并且类中所有声明的@Bean方法也被注册为bean定义，使用AnnotationConfigApplicationContext访问\n       public static void main(String[] args) &#123;\n           ApplicationContext ctx &#x3D; new AnnotationConfigApplicationContext(AppConfig.class);\n           MyService myService &#x3D; ctx.getBean(MyService.class);\n           myService.doStuff();\n       &#125;\n\n\n\n\nBean相关的注解：@Autowired、@Component、@Repository、@Service、@Controller、@RestController、@Controller、@ResponseBody、@Scope、@Configuration、@Component\n\n@Bean：用于表示一个方法实例化、配置和初始化一个由IoC容器管理的新对象，与元素的作用相同。可以使用此方法来制定为方法返回值的类型的ApplicationContext中注册bean定义。可以和任何Spring的@Component一起使用，但是通常与@Configurationbean一起使用\n     @Configuration\n     public class AppConfig &#123;\n         @Bean&#x2F;&#x2F;默认情况，bean名称与方法名称相同\n         public TransferServiceImpl transferService() &#123;\n             return new TransferServiceImpl();\n         &#125;\n         &#x2F;&#x2F;可以有任意数量的参数来描述构建该bean所需的依赖项\n         &#x2F;&#x2F;@Bean\n         &#x2F;&#x2F; public TransferService transferService(AccountRepository accountRepository) &#123;\n         &#x2F;&#x2F;     return new TransferServiceImpl(accountRepository);\n         &#x2F;&#x2F;&#125;\n     &#125;\n@Autowired：自动导入对象到类中，被注入进的类同样要被Spring容器管理\n\n示例\n@Service\npublic class UserService &#123;\n  &#x2F;&#x2F;......\n&#125;\n\n@RestController\n@RequestMapping(&quot;&#x2F;users&quot;)\npublic class UserController &#123;\n   @Autowired\n   private UserService userService;\n   &#x2F;&#x2F;......\n&#125;\n其他\n\n用在构造函数上，Spring4.3开始不再需要，但如果有多个构造函数则需要使用\npublic class MovieRecommender &#123;\n    private final CustomerPreferenceDao customerPreferenceDao;\n    @Autowired\n    public MovieRecommender(CustomerPreferenceDao customerPreferenceDao) &#123;\n        this.customerPreferenceDao &#x3D; customerPreferenceDao;\n    &#125;\n    &#x2F;&#x2F; ...\n&#125;\n用于传统的setter方法：\npublic class SimpleMovieLister\n    private MovieFinder movieFinder;\n    @Autowired\n    public void setMovieFinder(MovieFinder movieFinder) &#123;\n        this.movieFinder &#x3D; movieFinder;\n    &#125;\n    &#x2F;&#x2F; ...\n&#125;\n用于字段，甚至可以将其与构造函数混用\npublic class MovieRecommender &#123;\n \n     private final CustomerPreferenceDao customerPreferenceDao;\n     @Autowired\n     private MovieCatalog movieCatalog;\n     @Autowired\n     public MovieRecommender(CustomerPreferenceDao customerPreferenceDao) &#123;\n         this.customerPreferenceDao &#x3D; customerPreferenceDao;\n     &#125;\n     &#x2F;&#x2F; ...\n&#125;\n\n\n\n\n@Component,@Repository,@Service, @Controller：用于将类标识为可自动装配的bean\n\n@Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。\n\n任何满足存储库角色（DAO）的类的标记，用途是异常的自动翻译，类似的其它原型注解有：@Controller（表示层）、@Service（服务层）、@Repository（持久层），尽量不选@Component而选后面那三个\n\nSpring能自动检测原型类，并使用ApplicationContext注册相应的BeanDefinition实例，要自动检测这些类并注册相应的bean，需要将@ComponentScan(basePackages = &quot;org.example&quot;)添加到@Configuration类中\n\n用@Component定义bean元数据：将bean定义元数据贡献给容器\n         @Component\n         public class FactoryMethodComponent &#123;\n             @Bean &#x2F;&#x2F;表示工厂方法和其它bean定义属性\n             @Qualifier(&quot;public&quot;)\n             public TestBean publicInstance() &#123;\n                 return new TestBean(&quot;publicInstance&quot;);\n             &#125;\n         \n             public void doWork() &#123;\n                 &#x2F;&#x2F; Component method implementation omitted\n             &#125;\n         &#125;\n\n\n@Repository: 对应持久层即 Dao 层，主要用于数据库相关操作（dao层的代码注入到ServiceImpl中）\n\n使用Mybatis不需要Repository：关键在于ClassPathMapperScanner对指定包的扫描并且扫描过程，Spring本身只扫描实现类，但是MyBatis的扫描器扫了接口，并且为接口配了个BeanDefinition，其BeanClass是MapperFactoryBean\n@Configuration\n&#x2F;&#x2F;配置了MapperScan，在对应路径下扫描class文件\n@MapperScan(&#123;&quot;com.rent.mbg.mapper&quot;,&quot;com.rent.dao&quot;&#125;)\npublic class MyBatisConfig &#123;\n&#125;\nMapperScannerRegistrar.registerBeanDefinitions()：扫描Mapper并注册是注册BeanDefinitions到Spring中\n\n创建一个扫描器ClassPathMapperScanner，设置好一些属性后，执行doScan()方法区扫描@MapperScan提供的包\n\ndoScan()方法调用父类ClassPathBeanDefinitionScanner的doScan()方法，也就是Spring扫描BeanDefinition的方法，在其中对所有候选者调用isCandidateComponent()方法判断是否为符合要求的BeanDefinition（这里有两组过滤器来过滤扫描到的资源，Spring默认的过滤器是排除掉抽象类/接口，而Mybatis的扫描器重新注册了过滤器，默认对接口放行）\npublic Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123;\n\t&#x2F;&#x2F;调用父类的doscan，即Spring扫描BeanDefinition方法，但是重新注册了过滤器，可以对接口放行\n  Set&lt;BeanDefinitionHolder&gt; beanDefinitions &#x3D; super.doScan(basePackages);\n\n  if (beanDefinitions.isEmpty()) &#123;\n    logger.warn(&quot;No MyBatis mapper was found in &#39;&quot; + Arrays.toString(basePackages) + &quot;&#39; package. Please check your configuration.&quot;);\n  &#125; else &#123;\n\t\t&#x2F;&#x2F;通过接口注册BeanDefinition后，在此实例化Bean对象，因为正常接口无法实例化对象\n    processBeanDefinitions(beanDefinitions);\n  &#125;\n\n  return beanDefinitions;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;processBeanDefinitions里面&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F; the mapper interface is the original class of the bean\n&#x2F;&#x2F; but, the actual class of the bean is MapperFactoryBean\n\t\tdefinition.getConstructorArgumentValues().addGenericArgumentValue(definition.getBeanClassName()); &#x2F;&#x2F; 使用了Object[] argsToUse将String转换成了Object\n    definition.setBeanClass(this.mapperFactoryBean.getClass());\n以UserDao为例，自动装配时，Spring根据注册时候的BeanDefinition，去工厂mapperFactoryBean里面扔了个UserDao.class参数进去，工厂的getObject方法返回了工厂制造的userDao，其实工厂的getObject使用的是DefaultSqlSession.getMapper(Class type)方法，返回的事MapperProxy代理的类，而这个代理的类的invoke方法并不像平常调用原始目标的 method.invoke，而是去找MapperMethod执行\n\n\n\n加了 @Repository 注解有什么影响：仅仅只能解决 Intellij 静态查找 bean 的问题，没有实际作用。即使加了注解，比如@Controller，@Service 等等，也会被 Spring 的扫描器给忽略掉，因为扫描器会过滤掉接口\n\n\n\n@Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层（service的实现serviceImpl会注入到controller）\n\n@Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面\n\n\n\n@RestController\n\n@RestController注解是@Controller和@ResponseBody的合集，表示这是个控制器bean，并且是将函数的返回值直接填入HTTP响应体中，是REST风格的控制器\n单独使用@Controller不加@ResponseBody的话一般是用在要返回一个视图的情况，这种情况属于比较传统的Spring MVC的应用，对应于前后端不分离的情况，@Controller+@ResponseBody返回 JSON 或 XML 形式数据，写入到HTTP响应中，提供给前端页面进行解析\n\n\n@Scope\n\n声明Spring Bean的作用域，主要有四种常见的作用域：\n\nsingleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。\nprototype : 每次请求都会创建一个新的 bean 实例。\nrequest : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。\nsession : 每一个 HTTP Session 会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效\n\n@Bean\n@Scope(&quot;singleton&quot;)\npublic Person personSingleton() &#123;\n    return new Person();\n&#125;\n\n\n@Configuration：一般用来声明配置类，可以使用 @Component注解替代，不过使用@Configuration注解声明配置类更加语义化\n@Configuration\npublic class AppConfig &#123;\n    @Bean\n    public TransferService transferService() &#123;\n        return new TransferServiceImpl();\n    &#125;\n\n&#125;\n\n\n处理常见的 HTTP 请求类型：@GetMapping、@PostMapping、@PutMapping、@DeleteMapping、@PatchMapping\n\nGET ：请求从服务器获取特定资源。举个例子：GET /users（获取所有学生）\n&#x2F;&#x2F;等价于 @RequestMapping(value&#x3D;&quot;&#x2F;users&quot;,method&#x3D;RequestMethod.GET)\n@GetMapping(&quot;&#x2F;users&quot;)\npublic ResponseEntity&lt;List&lt;User&gt;&gt; getAllUsers() &#123;\n return userRepository.findAll();\n&#125;\nPOST ：在服务器上创建一个新的资源。举个例子：POST /users（创建学生）\n&#x2F;&#x2F;等价于 @RequestMapping(value&#x3D;&quot;&#x2F;users&#x2F;&#123;userId&#125;&quot;,method&#x3D;RequestMethod.PUT)\n@PostMapping(&quot;&#x2F;users&quot;)\npublic ResponseEntity&lt;User&gt; createUser(@Valid @RequestBody UserCreateRequest userCreateRequest) &#123;\n return userRespository.save(userCreateRequest);\n&#125;\nPUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /users/12（更新编号为 12 的学生）\n&#x2F;&#x2F;等价于 @RequestMapping(value&#x3D;&quot;&#x2F;users&#x2F;&#123;userId&#125;&quot;,method&#x3D;RequestMethod.PUT)\n@PutMapping(&quot;&#x2F;users&#x2F;&#123;userId&#125;&quot;)\npublic ResponseEntity&lt;User&gt; updateUser(@PathVariable(value &#x3D; &quot;userId&quot;) Long userId,\n  @Valid @RequestBody UserUpdateRequest userUpdateRequest) &#123;\n  ......\n&#125;\nDELETE ：从服务器删除特定的资源。举个例子：DELETE /users/12（删除编号为 12 的学生）\n&#x2F;&#x2F;等价于 @RequestMapping(value&#x3D;&quot;&#x2F;users&#x2F;&#123;userId&#125;&quot;,method&#x3D;RequestMethod.DELETE)\n@DeleteMapping(&quot;&#x2F;users&#x2F;&#123;userId&#125;&quot;)\npublic ResponseEntity deleteUser(@PathVariable(value &#x3D; &quot;userId&quot;) Long userId)&#123;\n  ......\n&#125;\nPATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，一般是PUT不够用了之后才用 PATCH 请求去更新数据\n@PatchMapping(&quot;&#x2F;profile&quot;)\npublic ResponseEntity updateStudent(@RequestBody StudentUpdateRequest studentUpdateRequest) &#123;\n      studentRepository.updateDetail(studentUpdateRequest);\n      return ResponseEntity.ok().build();\n  &#125;\n\n\n前后端传值：@PathVariable、@RequestParam、@RequestBody\n\n@PathVariable 和 @RequestParam：@PathVariable用于获取路径参数，@RequestParam用于获取查询参数\n@GetMapping(&quot;&#x2F;klasses&#x2F;&#123;klassId&#125;&#x2F;teachers&quot;)\npublic List&lt;Teacher&gt; getKlassRelatedTeachers(\n   @PathVariable(&quot;klassId&quot;) Long klassId,\n   @RequestParam(value &#x3D; &quot;type&quot;, required &#x3D; false) String type ) &#123;\n\t&#x2F;&#x2F;...\n&#125;\n&#x2F;&#x2F;请求的url是：klasses&#x2F;123456&#x2F;teachers?type&#x3D;web\n&#x2F;&#x2F;获取到的数据就是：klassId&#x3D;123456 type&#x3D;web\n@RequestBody\n\n用于读取Request请求（可能是POST、PUT、DELETE、GET请求）的body部分并且Content-Type为application/json，接收到数据之后会自动将数据绑定到 Java 对象上去。系统会使用HttpMessageConverter或者自定义的HttpMessageConverter将请求的 body 中的 json 字符串转换为 java 对象\n\n一个请求方法只可以有一个@RequestBody，但是可以有多个**@RequestParam和@PathVariable**\n&#x2F;&#x2F;发送POST请求到这个接口，并且body携带JSON数据\n&#x2F;&#x2F;&#123;&quot;userName&quot;:&quot;coder&quot;,&quot;fullName&quot;:&quot;shuangkou&quot;,&quot;password&quot;:&quot;123456&quot;&#125;\n&#x2F;&#x2F;后端就可以直接把 json 格式的数据映射到 UserRegisterRequest\n@PostMapping(&quot;&#x2F;sign-up&quot;)\npublic ResponseEntity signUp(@RequestBody @Valid UserRegisterRequest userRegisterRequest) &#123;\n  userService.save(userRegisterRequest);\n  return ResponseEntity.ok().build();\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;UserRegisterRequest&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n@Data\n@AllArgsConstructor\n@NoArgsConstructor\npublic class UserRegisterRequest &#123;\n    @NotBlank\n    private String userName;\n    @NotBlank\n    private String password;\n    @NotBlank\n    private String fullName;\n&#125;\n\n\n\n\n\n读取配置信息：@Value、@ConfigurationProperties、@PropertySource\nwuhan2020: 2020年初武汉爆发了新型冠状病毒，疫情严重，但是，我相信一切都会过去！武汉加油！中国加油！\n\nmy-profile:\n  name: Guide哥\n  email: koushuangbwcx@163.com\n\nlibrary:\n  location: 湖北武汉加油中国加油\n  books:\n    - name: 天才基本法\n      description: 二十二岁的林朝夕在父亲确诊阿尔茨海默病这天，得知自己暗恋多年的校园男神裴之即将出国深造的消息——对方考取的学校，恰是父亲当年为她放弃的那所。\n    - name: 时间的秩序\n      description: 为什么我们记得过去，而非未来？时间“流逝”意味着什么？是我们存在于时间之内，还是时间存在于我们之中？卡洛·罗韦利用诗意的文字，邀请我们思考这一亘古难题——时间的本质。\n    - name: 了不起的我\n      description: 如何养成一个新习惯？如何让心智变得更成熟？如何拥有高质量的关系？ 如何走出人生的艰难时刻？\n\n\n@Value(常用)：通常用于注入外部化属性\n&#x2F;&#x2F;applicationConfig会被定义在application.yml中\n@Value(&quot;$&#123;wuhan2020&#125;&quot;)\nString wuhan2020;\n\n&#x2F;&#x2F;示例二\n@Component\n public class MovieRecommender &#123;\n \n     private final String catalog;\n     &#x2F;&#x2F;application.properties中有catalog.name&#x3D;MovieCatalog\n     &#x2F;&#x2F;catalog的值就为MovieCatalog\n     public MovieRecommender(@Value(&quot;$&#123;catalog.name&#125;&quot;) String catalog) &#123;\n         this.catalog &#x3D; catalog;\n     &#125;\n &#125;\n &#x2F;&#x2F;还需以下配置\n @Configuration\n @PropertySource(&quot;classpath:application.properties&quot;)\n public class AppConfig &#123; &#125;\n@ConfigurationProperties(常用)：通过@ConfigurationProperties读取配置信息并与 bean 绑定\n@Component\n@ConfigurationProperties(prefix &#x3D; &quot;library&quot;)\nclass LibraryProperties &#123;\n    @NotEmpty\n    private String location;\n    private List&lt;Book&gt; books;\n\n    @Setter\n    @Getter\n    @ToString\n    static class Book &#123;\n        String name;\n        String description;\n    &#125;\n  省略getter&#x2F;setter\n  ......\n&#125;\n@PropertySource：读取指定 properties 文件\n@Component\n@PropertySource(&quot;classpath:website.properties&quot;)\n\nclass WebSite &#123;\n    @Value(&quot;$&#123;url&#125;&quot;)\n    private String url;\n\n  省略getter&#x2F;setter\n  ......\n&#125;\n\n\n参数校验\n\n数据的校验的重要性就不用说了，即使在前端对数据进行校验的情况下，我们还是要对传入后端的数据再进行一遍校验，避免用户绕过浏览器直接通过一些 HTTP 工具直接向后端请求一些违法数据\n\n验证请求体\n\n字段上加@NotNull(message = &quot;classId 不能为空&quot;)\n参数上加public ResponseEntity&lt;Person&gt; getPerson(@RequestBody @Valid Person person)\n\n\n验证请求参数(Path Variables 和 Request Parameters)\n\n类上加@Validated ****注解，告诉Spring校验方法参数\n@RestController\n@RequestMapping(&quot;&#x2F;api&quot;)\n@Validated\npublic class PersonController &#123;\n\n    @GetMapping(&quot;&#x2F;person&#x2F;&#123;id&#125;&quot;)\n    public ResponseEntity&lt;Integer&gt; getPersonByID(@Valid @PathVariable(&quot;id&quot;) @Max(value &#x3D; 5,message &#x3D; &quot;超过 id 的范围了&quot;) Integer id) &#123;\n        return ResponseEntity.ok().body(id);\n    &#125;\n&#125;\n\n\n\n\nJSR(Java Specification Requests）是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，这样就可以在需要校验的时候进行校验了，非常方便，*一些常用的字段验证的注解如下*\n\n@NotEmpty 被注释的字符串的不能为 null 也不能为空\n@NotBlank 被注释的字符串非 null，并且必须包含一个非空白字符\n@Null 被注释的元素必须为 null\n@NotNull 被注释的元素必须不为 null\n@AssertTrue 被注释的元素必须为 true\n@AssertFalse 被注释的元素必须为 false\n@Pattern(regex=,flag=)被注释的元素必须符合指定的正则表达式\n@Email 被注释的元素必须是 Email 格式。\n@Min(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值\n@Max(value)被注释的元素必须是一个数字，其值必须小于等于指定的最大值\n@DecimalMin(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值\n@DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值\n@Size(max=, min=)被注释的元素的大小必须在指定的范围内\n@Digits(integer, fraction)被注释的元素必须是一个数字，其值必须在可接受的范围内\n@Past被注释的元素必须是一个过去的日期\n@Future 被注释的元素必须是一个将来的日期\n\n\n校验的时候我们实际用的是 Hibernate Validator 框架。Hibernate Validator 是 Hibernate 团队最初的数据校验框架，Hibernate Validator 4.x 是 Bean Validation 1.0（JSR 303）的参考实现，Hibernate Validator 5.x 是 Bean Validation 1.1（JSR 349）的参考实现，目前最新版的 Hibernate Validator 6.x 是 Bean Validation 2.0（JSR 380）的参考实现。SpringBoot 项目的 spring-boot-starter-web 依赖中已经有 hibernate-validator 包，不需要引用相关依赖\n\n需要注意的是： 所有的注解，推荐使用 JSR 注解，即javax.validation.constraints，而不是org.hibernate.validator.constraints\n\n\n\n\n全局处理Controller层异常\n\n@ControllerAdvice ：注解定义全局异常处理类\n@ExceptionHandler ：注解声明异常处理方法\n\n@ControllerAdvice\n@ResponseBody\npublic class GlobalExceptionHandler &#123;\n\n    &#x2F;**\n     * 请求参数异常处理\n\t\t * 处理controller抛出的MethodArgumentNotValidException异常\n     *&#x2F;\n    @ExceptionHandler(MethodArgumentNotValidException.class)\n    public ResponseEntity&lt;?&gt; handleMethodArgumentNotValidException(MethodArgumentNotValidException ex, HttpServletRequest request) &#123;\n       ......\n    &#125;\n&#125;\nJPA相关\n\n@Entity声明一个类对应一个数据库实体\n\n@Table设置表名\n\n@Id声明一个字段为主键\n\n@GeneratedValue指定主键生成策略\n\nTABLE：使用一个特定的数据库表格来保存主键，持久化引擎通过关系数据库的一张特定的表格来生成主键\nSEQUENCE：在某些数据库中,不支持主键自增长,比如Oracle、PostgreSQL其提供了一种叫做”序列(sequence)”的机制生成主键\nIDENTITY：主键自增长\nAUTO：把主键生成策略交给持久化引擎(persistence engine)，根据数据库在以上三种主键生成策略中选择其中一种\n\n\n@Column 声明字段\n\n@Column(name = &quot;user_name&quot;, nullable = false, length=32)\n设置字段类型并且加默认值@Column(columnDefinition = &quot;tinyint(1) default 1&quot;)\n\n\n@Transient声明不需要与数据库映射的字段，在保存的时候不需要保存进数据库\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;************除了 @Transient关键字声明， 还可以采用下面几种方法&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;************\nstatic String secrect; &#x2F;&#x2F; not persistent because of static\nfinal String secrect &#x3D; &quot;Satish&quot;; &#x2F;&#x2F; not persistent because of final\ntransient String secrect; &#x2F;&#x2F; not persistent because of transient\n@Enumerated(EnumType.STRING)声明为枚举类型字段\n\n\n\n事务\n\nException 分为运行时异常 RuntimeException 和非运行时异常，在@Transactionsl注解中如果不配置rollbackFor属性，那么事务只会在遇到RuntimeException的时候才会回滚，加上rollbackFor=Exception.class，可以让事务在遇到非运行时异常时也回滚\n\n\njson数据处理\n\n过滤json数据\n\n@JsonIgnoreProperties 作用在类上用于过滤掉特定字段不返回或者不解析\n@JsonIgnore一般用于类的属性上，作用和上面的@JsonIgnoreProperties 一样\n\n\n@JsonFormat一般用来格式化 json 数据\n@JsonFormat(shape&#x3D;JsonFormat.Shape.STRING, pattern&#x3D;&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSS&#39;Z&#39;&quot;, timezone&#x3D;&quot;GMT&quot;)\nprivate Date date;\n使用@JsonUnwrapped扁平对象，声明在每一个字段上\n&#123;\n    &quot;location&quot;: &#123;\n        &quot;provinceName&quot;:&quot;湖北&quot;,\n        &quot;countyName&quot;:&quot;武汉&quot;\n    &#125;,\n    &quot;personInfo&quot;: &#123;\n        &quot;userName&quot;: &quot;coder1234&quot;,\n        &quot;fullName&quot;: &quot;shaungkou&quot;\n    &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;扁平化后&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#123;\n  &quot;provinceName&quot;:&quot;湖北&quot;,\n  &quot;countyName&quot;:&quot;武汉&quot;,\n  &quot;userName&quot;: &quot;coder1234&quot;,\n  &quot;fullName&quot;: &quot;shaungkou&quot;\n&#125;\n\n\n测试相关\n\n@ActiveProfiles一般作用于测试类上， 用于声明生效的 Spring 配置文件\n@SpringBootTest(webEnvironment &#x3D; RANDOM_PORT)\n@ActiveProfiles(&quot;test&quot;)\n@Slf4j\npublic abstract class TestBase &#123;\n  ......\n&#125;\n@Test声明一个方法为测试方法\n\n@Transactional被声明的测试方法的数据会回滚，避免污染测试数据\n\n@WithMockUser Spring Security 提供的，用来模拟一个真实用户，并且可以赋予权限\n@Test\n@Transactional\n@WithMockUser(username &#x3D; &quot;user-id-18163138155&quot;, authorities &#x3D; &quot;ROLE_TEACHER&quot;)\nvoid should_import_student_success() throws Exception &#123;\n    ......\n&#125;\n\n\n其他\n\n@Primary表示当多个bean成为自动装配的候选者时，应该优先考虑特定的bean\n@Configuration\npublic class MovieConfiguration &#123;\n\n    @Bean\n    @Primary\n    public MovieCatalog firstMovieCatalog() &#123; ... &#125;\n\n    @Bean\n    public MovieCatalog secondMovieCatalog() &#123; ... &#125;\n    &#x2F;&#x2F; ...\n&#125;\npublic class MovieRecommender &#123;\n  &#x2F;&#x2F;自动装配firstMovieCatalog\n    @Autowired\n    private MovieCatalog movieCatalog;\n    &#x2F;&#x2F; ...\n&#125;\n@Qualifier：可以将限定符值与特定参数相关联，缩小类型匹配的范围，以便为每个参数选择特定的bean：\npublic class MovieRecommender &#123;\n\n    private MovieCatalog movieCatalog;\n    private CustomerPreferenceDao customerPreferenceDao;\n\n    @Autowired\n    public void prepare(@Qualifier(&quot;main&quot;) MovieCatalog movieCatalog,\n                        CustomerPreferenceDao customerPreferenceDao) &#123;\n        this.movieCatalog &#x3D; movieCatalog;\n        this.customerPreferenceDao &#x3D; customerPreferenceDao;\n    &#125;\n    &#x2F;&#x2F; ...\n&#125;\n@Resource：采用名称属性，默认将该值解释为要注入的bean名称，如果没有明确指定名称，如果是字段，则采用字段名称，如果是setter方法，则采用bean属性名称\n     public class SimpleMovieLister &#123;\n     \n         private MovieFinder movieFinder;\n         @Resource(name&#x3D;&quot;myMovieFinder&quot;)\n         public void setMovieFinder(MovieFinder movieFinder) &#123;\n             this.movieFinder &#x3D; movieFinder;\n         &#125;\n     &#125;\n\n\n\n2.AutoConfiguration\nSPI扩展机制（Service Provider Interface 服务提供者的接口）\n\nAPI\n\n示例：spring项目中，写service层代码前，会约定俗成的添加一个接口层，然后通过spring中的依赖注入（@Autowired）注入这个接口的实现类的实例对象，之后对于service的调用也基于接口操作（虽然有的时候一个接口只有一个实现类，但是面向接口编程可以降低耦合度、方便日后扩展、提高了代码的灵活性和可维护性）\n\n\n\n\nSPI\n\n示例：slf4j框架的LoggerFactory中的findServiceProviders方法返回所有SLF4JServiceProvider（日志服务提供者，一个接口，需要具体的提供者来实现）\n\n服务调用方定义一个接口规范，可以由不同的服务提供者实现，并且，调用方通过某种机制来发现服务提供方，并通过接口调用它的能力\n\nAPI接口是服务提供者向服务调用者提供的一个功能列表（），而SPI机制是服务调用者提供对服务的一种约束（遥控器约束所有空调只有开关等功能），服务提供者根据约束实现的服务，可以被服务调用者发现\n\n\n\n\nSPI工作流程（ServiceLoader）\n\n首先通过ServiceLoader提供的load方法传入接口的class，返回ServiceLoader对象，然后遍历ServiceLoader对象通过接口来引用（接口.getClass）其中的实现类\n\nacc是一个安全管理器，通过System.getSecurityManager()判断并赋值，这里为null\n\nServiceLoader实现了Iterable接口，它的遍历通过Iterator()方法来实现，再Iterator()方法中，首先在一个LinkedHashMap\n实现的名为provider缓存中遍历，没有的时候在LazyIterator类型的懒加载的lookupIterator对象中查找\n\nhasNext方法，逻辑实现为hasNextService方法，取出接口的实现类的全限定名放到nextName中\nnext方法，逻辑实现为nextService方法，通过反射加载（Class.forName）这个实现类，然后实例化对象（newInstance），最后放入之间的provider缓冲中\n\n\n\n\n\n\nspring.factories\n\n@EnableAutoConfiguration\n\n\n3.SpringApplication\n从Java的main方法启动一个Spring应用的过程\n\nSpringApplication的run方法\n\n运行SpringApplication，创建并更新ApplicationContext\nrun方法中的refresh方法\n初始化BeanFactory，加载Bean，注册Bean\n定义的一个个Bean会被转换成BeanDefinition，存在于Spring的BeanFactory中，即Bean可以理解为BeanDefinition的实例，里面保存了Bean的信息（Bean指向哪个类，是否是单例，是否懒加载，依赖了哪些Bean）\nrefreshBeanFactory中的loadBeanDefinitions：根据配置，加载各个Bean，然后放到BeanFactory中\n\n\n\n\n记录应用启动时间\n声明异常链表，报告启动错误\n\n\n两个结构\n\nApplicationContext\n\n\nBeanFactory（Application其实就是一个BeanFactory，里面使用DefaultListableBeanFactory）\n\n\n\n\n启动步骤\n\n根据classpath创建一个\nApplicationContext\n\n\nApplicationContext可以从多个不同数据源读入数据，\n\n\n注册一个CommandLinePropertySource，将命令行参数扩展成Spring属性\n\n刷新应用上下文，载入单例类（PropertyContext）\n\n触发CommandLineRunner的bean\n\n\n\n\n\n\n8.Spring Security\nSpringSecurity是一个强大的可高度定制的认证和授权框架，对于Spring应用来说它是一套Web安全标准\n\nDelegatingFilterProxy：将Filter委托给实现了Filter接口的Bean来处理\n\nFilterChain：一个客户端向应用发送一个request，会经过一系列的Filter组成的FilterChain，最后交由Servlet来处理\npublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) &#123;\n\t&#x2F;&#x2F; do something before the rest of the application\n    chain.doFilter(request, response); &#x2F;&#x2F; invoke the rest of the application\n    &#x2F;&#x2F; do something after the rest of the application\n&#125;\nDelegatingFilterProxy：桥接Servlet container的生命周期和Spring的ApplicationContext，DelegatingFilterProxy能被通过标准servlet container机制来注册，但是将所有工作委托给Filter的Spring Bean，FilterChainProxy被包装在DelegatingFilterProxy中，来链接进Servlet container filter chain\npublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) &#123;\n\t&#x2F;&#x2F; Lazily get Filter that was registered as a Spring Bean\n\t&#x2F;&#x2F; For the example in DelegatingFilterProxy delegate is an instance of Bean Filter0\n\tFilter delegate &#x3D; getFilterBean(someBeanName);\n\t&#x2F;&#x2F; delegate work to the Spring Bean\n\tdelegate.doFilter(request, response);\n&#125;\nFilterChainProxy是Spring Security提供的一个特殊的Filter，将Filter实例委托给SecurityFilterChain，通过内部的一个链表filterChains，来遍历（doFilterInternal中的doFilter方法）确定哪个Spring Security Filter应该被调用（在调试的时候，可以在FilterChainProxy加一个断点，来看所有Spring Security的Servlet）\n\nThe WebSecurity is created by WebSecurityConfiguration to create the FilterChainProxy known as the Spring Security Filter Chain (springSecurityFilterChain).\n\n\n\n常见Filter顺序如下\n\nUsernamePasswordAuthenticationFilter（AbstractAuthenticationProcessingFilter）\nBasicAuthenticationFilter\nExceptionTranslationFIlter（处理异常）\nFilterSecurityInterceptor\n\n\n处理安全异常（未授权通过的抛出异常）\n\nThe ExceptionTranslationFilter allows translation of AccessDeniedException and AuthenticationException into HTTP responses.（检查抛出的异常是否有AccessDeniedException类型的异常）\nprotected void sendStartAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain,\n\t\t\tAuthenticationException reason) throws ServletException, IOException &#123;\n\t\t&#x2F;&#x2F; SEC-112: Clear the SecurityContextHolder&#39;s Authentication, as the\n\t\t&#x2F;&#x2F; existing Authentication is no longer considered valid\n\t\tSecurityContext context &#x3D; SecurityContextHolder.createEmptyContext();\n\t\tSecurityContextHolder.setContext(context);\n\t\t&#x2F;&#x2F;保存HttpServletRequest，用来在授权成功时replay最初的request\n\t\tthis.requestCache.saveRequest(request, response);\n\t\t&#x2F;&#x2F;启动（commence）身份验证方案\n\t\t&#x2F;&#x2F;一般authenticationEntryPoint是LoginUrlAuthenticationEntryPoint的实例\n\t\tthis.authenticationEntryPoint.commence(request, response, reason);\n\t&#125;\n\n\n\n\nUsernamePasswordAuthenticationFilter\n\n\n通过attemptAuthentication方法来返回Authentication对象\n\n从request创造出一个Authentication\n通过username和password来生成一个token传递给request，再使用ProviderManager来看是否授权成功，并返回Authentication对象\n\n\nProviderManager：通过构建一个AuthenticationProvider链表，来看哪个Provider（或ProviderManager的父类）能处理这个authentication\n\nAuthentication对象：保存在SecurityContextHolder的SecurityContext中\n\n\n\n\nAuthorization（授权）\n\n所有Authorization存储在GrantedAuthority对象链表，代表已授权的对象。GrantedAuthority对象由AuthenticationManager插入到Authorization对象中，由AuthorizationManager读取来做出授权决策\nSpring Security包括一个具体的GrantedAuthority实现SimpleGrantedAuthority，允许将任何用户定义的String转换成一个GrantedAuthority\nAccessDecisionManager：Spring Security提供拦截器来控制对安全对象（方法调用或web请求）的访问，AccessDecisionManager在调用发生前决定是否允许调用（取代了AccessDecisionManager 和 AccessDecisionVoter）\nAuthorizationFilter有一个AuthorizationManager链表，每一个AuthorizationManager有两个方法\nverify：做决定，调用check方法检查返回值，来看是否授权\ncheck：传递做授权所需的所有消息\n\n\nAuthorizationManager的实现\nRequestMatcherDelegatingAuthorizationManager：匹配Request到最适合的AuthorizationManager进行处理\nAuthorityAuthorizationManager：配置了一组给定authorities来找到当前的Authentication\nAuthenticatedAuthorizationManager：用来区分anonymous、full-authenticated、remember-me的用户\n\n\n\n\n\n\nJWT是JSON WEB TOKEN的缩写，它是基于 RFC 7519 标准定义的一种可以安全传输的的JSON对象，由于使用了数字签名，所以是可信任和安全的\n\nMD5是不安全的\n\nJWT的格式：header.payload.signature\n\nheader中用于存放签名的生成算法，如：&#123;&quot;alg&quot;: &quot;HS512&quot;&#125;\n\npayload中用于存放用户名、token的生成时间和过期时间，如：&#123;&quot;sub&quot;:&quot;admin&quot;,&quot;created&quot;:1489079981393,&quot;exp&quot;:1489684781&#125;\n\nsignature是以header和payload生成的签名，用来检测header和payload是否被篡改\nString signature &#x3D; HMACSHA512(base64UrlEncode(header) + &quot;.&quot; +base64UrlEncode(payload),secret)\n\n\n实现认证和授权的原理\n\n用户调用登陆接口，登录成功后获取JWT的token（原来是Cookie）\n之后用户每次调用接口都在http的header中添加一个Authorization头，值为JWT的token\n后台程序通过对Authorization头中的信息的解码姐数字签名的校验来获取其中的用户信息，从而实现认证和授权\n\n\n\n\n使用\n\n\n附录\n源码知识点\n\nBean解析流程\nBeanFactory\nFactoryBean\nApplicationContext\nIOC\nBean生命周期\nAOP\n\n\nSpring给我们提供了很多扩展点\n\nBeanFactoryPostProcessor：允许在Spring容器实例化bean之前修改bean的定义。常用于修改bean属性或改变bean的作用域。\nBeanPostProcessor：可以在bean实例化、配置以及初始化之后对其进行额外处理。常用于代理bean、修改bean属性等。\nPropertySource：用于定义不同的属性源，如文件、数据库等，以便在Spring应用中使用。\nImportSelector和ImportBeanDefinitionRegistrar：用于根据条件动态注册bean定义，实现配置类的模块化。\nSpring MVC中的HandlerInterceptor：用于拦截处理请求，可以在请求处理前、处理中和处理后执行特定逻辑。\nSpring MVC中的ControllerAdvice：用于全局处理控制器的异常、数据绑定和数据校验。\nSpring Boot的自动配置：通过创建自定义的自动配置类，可以实现对框架和第三方库的自动配置。\n自定义注解：创建自定义注解，用于实现特定功能或约定，如权限控制、日志记录等。\n\n\n如果让你设计一个SpringIoc，你觉得会从哪些方面考虑这个设计？\n\nBean的生命周期管理：需要设计Bean的创建、初始化、销毁等生命周期管理机制，可以考虑使用工厂模式和单例模式来实现。\n依赖注入：需要实现依赖注入的功能，包括属性注入、构造函数注入、方法注入等，可以考虑使用反射机制和XML配置文件来实现。\nBean的作用域：需要支持多种Bean作用域，比如单例、原型、会话、请求等，可以考虑使用Map来存储不同作用域的Bean实例。\nAOP功能的支持：需要支持AOP功能，可以考虑使用动态代理机制和切面编程来实现。\n异常处理：需要考虑异常处理机制，包括Bean创建异常、依赖注入异常等，可以考虑使用try-catch机制来处理异常。\n配置文件加载：需要支持从不同的配置文件中加载Bean的相关信息，可以考虑使用XML、注解或者Java配置类来实现。\n\n\n源码核心技术点\n\nSpring IOC：Bean生命周期、依赖自动注入\nSpring AOP：AOP源码、事务源码\nSpring后置处理器\nSpring循环依赖\n@Configuration、@Bean注解底层原理\nSpringBoot：底层源码、factories扩展机制、自动配置类、过滤机制、启动过程、第三方starter机制\n\n\n循环依赖\n\n循环依赖\n\n概念\n\nBeanDefinition：Spring核心bean的配置信息，通过扫描注解（@Compoent、@Service、@Configuration）把需要的bean初始化为BeanDefinition的列表\nSpringBean：Spring管理的已经创建好的以后可以使用的实例\nJava Bean：Java通过构造函数创建的对象，Spring推断构造方法后使用反射调用构造函数创建的对象\n\n\n产生原因：在框架启动时会进行bean的加载\n\n\n出现场景\n\n构造器内的循环依赖\nsetter方式单例\nsetter方式原型\nfield属性循环依赖\n\n\n\n\nSpring如何解决\n\n三级缓存解决循环依赖\n\n一级缓存：\nprivate final Map&lt;String, Object&gt; singletonObjects &#x3D; new ConcurrentHashMap&lt;&gt;(256);\n\n\n最基础的单例缓存，限制Bean在beanFactory中只存一份，即实现singleton scope\n\n\n二级缓存：\nprivate final Map&lt;String, Object&gt; earlySingletonObjects &#x3D; new HashMap&lt;&gt;(16);\n\n\n未初始化未填充属性提前暴露的bean\n当调用三级缓存里的对象工厂的getObject方法之后，getEarlyBeanReference 就会把返回值放入二级缓存，删除三级缓存，后续其他依赖该对象的Bean获取的都是同一个earlyBean，保证singleton原则\n\n\n三级缓存：private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);\n\n\n\nAOP循环依赖\n\n\n\n\n\n\n","slug":"Spring","date":"2023-05-06T05:09:43.000Z","categories_index":"","tags_index":"tools","author_index":"Dajunnnnnn"},{"id":"a978a5e93d8e6628e9f4ee713be55be8","title":"Redis","content":"Redis\n\n\n\n\n\n\n\n\nRedis是一个高性能（内存+Reactor+优化的数据结构）的开源键值数据库，其value支持丰富的数据类型（string、hash、set、list、zset「有序集合」），具有数据可持久化（AOF+RDB）、支持master-slave备份、读写性能高（MySQL的QPS大概1w左右，Redis读11w次/s，写8w次/s）等特点，其单个操作是原子性的，多个连续操作支持事务，常用于缓存，消息队列、分布式锁等场景\n1.高性能1.数据结构\n数据类型及其应用\n\n\n\n类型\n简介\n特性\n场景\n大小\n底层数据结构\n\n\n\nString\n二进制安全\n可以包含任何数据（字符串、整数、浮点数、图片的base64编码、序列化后的对象）\n1.存储数据；2.计数（单位时间请求数，单位时间访问数）\n一个键最大能存储 512MB\nSDS（简单动态字符串）\n\n\nHash\n键值对集合,即编程语言中的Map类型\n适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去)\n1.存储、读取、修改用户属性；2.对象数据存储（用户信息、文章信息、购物车信息）\n每个 hash 可以存储 2^32 -1 键值对（4294967295）\nLinkedLIst ZipList\n\n\nList\n链表(双向链表)，按照插入顺序排序\n增删快，提供了操作某一段元素的API（没法用来做排行榜，分页显示时会有串行的问题，使用Sorted Set的score可以解决）\n1、最新消息排行等功能(比如朋友圈的时间线) 2、消息队列\n列表最多可存储 2^32 - 1 元素（4294967295）\nZipList HashTable；3.2后使用QuickList\n\n\nSet\n哈希表实现，元素不重复，可以求交集、并集、差集\n1、添加、删除、查找的复杂度都是O(1) ；2、为集合提供了求交集、并集、差集等操作；3、数据量大时可以选择一个从库\n1、共同好友 2、利用唯一性，统计访问网站的所有独立ip 3、好友推荐时，根据tag求交集，大于某个阈值就可以推荐\n集合中最大的成员数为 2^32 - 1（4294967295）\nZipList Intset\n\n\nZSet（Sorted Set）\n将Set中的元素增加一个权重参数score,元素按score有序排列\n数据插入集合时，已经进行天然排序。类似于Set，但是多了一个权重参数score，使得集合中的元素能够按score进行有序排列，还可以通过score的范围来获取元素的列表\n1、排行榜 2、带权重的消息队列\n—\nZipList SkipList\n\n\nBitmap\n位存储，支持按位与、或、异或\n存储连续的二进制数字，可以看成是存储0/1的数组，数组下标称为offset（活跃用户统计）\n1、用来做二值统计（元素的取值只有0和1），如签到统计\nGETBIT、SETBIT、BITCOUNT\nString（底层为二进制字节数组）\n\n\nHyperLogLogs\n基数统计，元素数量多时仍可保证消耗的空间是固定的\n基数计数概率算法为了节省内存并不会直接存储元数据，而是通过一定的概率统计方法预估基数值（集合中包含元素的个数）\n主要用于数据量巨大的计数场景（热门网站每日访问ip数统计），只存估计值\nHyperLogLogs只需要12KB内存，可以计算接近 2^64 个元素的基数\n基于概率进行统计，给出的结果有偏差\n\n\nGeospatial\n地理位置\n基于Sorted Set实现，将经纬度信息通过GeoHash算法转换成一个整数，将这个整数作为Sorted Set的score使用（实现附近的人功能）\n主要用于存储地理位置信息\nGEOADD、GEORADIUS（根据输入的经纬度，查找以这个经纬度为中心的一定范围内的其他元素）\nSorted Set + GeoHash编码（二分区间、区间编码）\n\n\n\nHash 类型底层结构什么时候使用压缩列表\n\nHash 集合中写入的元素个数超过了hash-max-ziplist-entries，或者写入的单个元素大小超过了hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表\n一旦从压缩列表转为了哈希表，Hash 类型就会一直用哈希表进行保存，而不会再转回压缩列表了\n\n\n补充数据结构\n\n\n\n类型\n简介\n特性\nAPI\n底层原理\n\n\n\nRedisTimeSeries\n记录时间序列数据\n支持直接在Redis实例上进行聚合计算（求平均、最值、和），其它方案都需要传输数据到客户端上进行聚合计算\nTS.CREATE；TS.ADD；TS.GET；TS.MGET；TS.RANGE（av g、max/min、sum）\nRedis的扩展模块，使用前需要编译成动态链接库 redistimeseries.so，再使用 loadmodule 命令进行加载\n\n\nStreams\n专门为消息队列设计的数据类型\n消息格式是键值对形式，插入的每一条消息自动生成一个全局唯一ID，读取时可以指定读取起始位置。支持创建消费组\nXADD、XREAD、XREADGROUP、SPENDING、XACK\n自动使用内部队列留存消费者读取的消息直到消费者使用SACK命令，重启时使用命令XPENDING继续处理\n\n\n\n自定义数据类型\n\nRedisObject\ntype：表示值的类型，涵盖了前面学习的五大基本类型\nencoding：是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等\nlru：记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对\nrefcount：记录了对象的引用计数\n*ptr：是指向数据的指针，借助*ptr指针，就可以指向不同的数据类型\n\n\n定义一个新的数据类型的步骤（未完待续）\n\n\n\n\n数据结构对应命令\n\n命令行方式（https://www.runoob.com/redis/redis-tutorial.html、https://redis.io/commands/）\n\n\n\n类型\n命令\n\n\n\nstring\nSET、GET、EXIST、STRLEN、DEL、MSET、MGET、INCR、DECR、EXPIRE、SETNX、TTL\n\n\nlist\nLPUSH、LPOP、RPUSH、RPOP、LRANGE（实现分页）、LLEN\n\n\nhash\nHSET、HSETNX、HMSET、HGET、HMGET、HGETALL（所有）、HEXIXTS、HDEL、HLEN、HINCRBY（增加多少）\n\n\nset\nSADD、SMEMBERS（内容）、SCARD（数量）、SISMEMBER（有无）、求交/并/差集（SINTER、SUNION、SDIFF）、SPOP key count、SRANDMEMBER key count\n\n\nzset\nZADD、ZCARD、ZSCORE、ZINTERSTORE（一共三个）、ZRANGE、ZREVRANGE、ZREVRANK\n\n\nkey\nSET key value、DEL key、EXISTS key（seconds）、TTL key、TYPE key\n\n\npub/sub\nPUBLISH channel message、SUBSCRIBE channel、UNSUBSCRIBE channel\n\n\n事务\nMULTI（开始）、EXEC（执行）、DISCARD（取消）、WATCH、UNWATCH\n\n\n其它\nPING、PONG、QUIT、AUTH password、INFO、FLUSHALL、BGSAVE、BGREWRITEAOF\n\n\n\n使用批量操作减少网络传输\n\nRedis每条命令都会通过网络与服务器交互，可以使用批量操作命令（mget、hmget），但是在Redis Cluster下无法保证所有key都在同一个hash slot上，但仍个减少网络交互耗时\n对于不支持批量操作的命令，可以用pipeline将一批Redis命令封装成一组（非原子操作），但是需要控制批量传输的元素个数，避免网络传输的数据量大，同前一点一样，在Redis Cluster会有问题\n\n\nJedis方式（https://redis.io/commands/）\nimport redis.clients.jedis.Jedis;\n \n import java.util.List;\n import java.util.Set;\n \n public class redisDemo &#123;\n     public static void main(String[] args) &#123;\n         Jedis jedis &#x3D; new Jedis(&quot;localhost&quot;,6379);\n         &#x2F;&#x2F;ping下，看看是否通的\n &#x2F;&#x2F;        System.out.println(&quot;Server is running: &quot; + jedis.ping());\n         &#x2F;&#x2F;String\n         jedis.set(&quot;foo&quot;, &quot;bar&quot;);\n         String value &#x3D; jedis.get(&quot;foo&quot;);\n         &#x2F;&#x2F;List，双端队列可设置为阻塞获取，可返回&#x2F;删除一个范围内的元素，可通过索引设置元素\n         jedis.lpush(&quot;mylist&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         jedis.rpush(&quot;mylist&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         List&lt;String&gt; mylist &#x3D; jedis.lrange(&quot;mylist&quot;, 0, -1);&#x2F;&#x2F;0表示第一个，-1表示最后一个\n         System.out.println(mylist);\n         &#x2F;&#x2F;Set\n         jedis.sadd(&quot;myset&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;);\n         jedis.sadd(&quot;myset2&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;);\n         Set&lt;String&gt; setdiff &#x3D; jedis.sdiff(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;差集\n         Set&lt;String&gt; setinter &#x3D; jedis.sinter(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;交集\n         Set&lt;String&gt; sunion &#x3D; jedis.sunion(&quot;myset&quot;, &quot;myset2&quot;);&#x2F;&#x2F;并集\n         &#x2F;&#x2F;Hash\n         jedis.hset(&quot;myhash&quot;, &quot;a&quot;, &quot;b&quot;);\n &#x2F;&#x2F;        jedis.hincrBy(&quot;myhash&quot;, &quot;a&quot;, 1);&#x2F;&#x2F;a的值加1\n         &#x2F;&#x2F;Sorted Set\n         jedis.zadd(&quot;myzset&quot;, 1, &quot;a&quot;);\n         jedis.zadd(&quot;myzset&quot;, 2, &quot;b&quot;);\n         jedis.zadd(&quot;myzset&quot;, 3, &quot;c&quot;);\n         jedis.zlexcount(&quot;myzset&quot;, &quot;-&quot;, &quot;+&quot;);&#x2F;&#x2F;返回有序集合中指定区间内成员的数量\n         jedis.zlexcount(&quot;myzset&quot;, &quot;[b&quot;, &quot;[c&quot;);&#x2F;&#x2F;返回有序集合中指定区间(b到c)内成员的数量\n \n     &#125;\n &#125;\n\n\n底层原理\n\n全局哈希：实现从键到值的访问，具体的数据再根据值的类型不同进行不同的查找（默认使用两个全局哈希表）\n\n哈希冲突解决方法：拉链法，增加next指针，缺点是冲突越多在链上的查找越慢\nrehash：增加hash表长度，减少单个桶中的元素数量，rehash的过程包括以下三步\n给hash表2分配更大的空间\n把hash表1的数据重新映射到hash表2，会产生大量的数据拷贝，导致Redis的线程阻塞，所以使用渐进式rehash\nRedis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries\n处理请求：渐进式rehash过程中，使用两个hash表，t1和t2。针对查找操作，先在t1里面查找，如果没找到就去t2里查找；针对插入操作，一律保存到t2里，保证t1数据只减不增\n\n\n释放hash表1的空间，表1留作下一次rehash使用\n每个 hash table 都有存着一个 used 字段，每次单步 rehash 完成的时候，最后都会检查老表即  ht[0].used 是否变成了 0，变成 0 后，就说明老的哈希表里已经没有数据了，此时就会去 free 掉老表，交换老表新表的指针，rehashidx 置为 -1，然后就完成了整个 rehash\n\n\n\n\n\n\n\n压缩列表：基于压缩列表实现了List、Hash、Set、Sorted Set，节省了dictEntry数量，好多个值共用一个dictEntry\n\n类似于一个数组，不同之处是在表头有三个字段lbytes、zltail和zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束\n\n查找：查找头尾元素复杂度是O(1)，查找其它元素复杂度是O(N)\n\n\n\n节省内存：使用一系列entry保存数据，每个entry包括以下几部分，通过连续存储不使用指针连接来节省指针占用的空间\n\nprev_len，表示前一个 entry 的长度，prev_len 有两种取值情况1 字节或 5 字节\n\n取值 1 字节时，表示上一个 entry 的长度小于 254 字节。虽然 1 字节的值能表示的数值范围是 0 到 255，但是压缩列表中 zlend 的取值默认是 255，因此，就默认用 255 表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。所以，当上一个 entry 长度小于 254 字节时prev_len 取值为 1 字节，否则，就取值为 5 字节\n\n\nlen：表示自身长度，4 字节\n\nencoding：表示编码方式，1 字节\n\ncontent：保存实际数据\n\n代码（byte==B），一个存储Long类型的entry占用1+4+1+8（向上取整为16）字节\ntypedef struct zlentry &#123;\n    unsigned int prevrawlensize; &#x2F;* Bytes used to encode the previous entry len*&#x2F;\n    unsigned int prevrawlen;     &#x2F;* Previous entry len. *&#x2F;\n    unsigned int lensize;        &#x2F;* Bytes used to encode this entry type&#x2F;len.\n                                    For example strings have a 1, 2 or 5 bytes\n                                    header. Integers always use a single byte.*&#x2F;\n    unsigned int len;            &#x2F;* Bytes used to represent the actual entry.\n                                    For strings this is just the string length\n                                    while for integers it is 1, 2, 3, 4, 8 or\n                                    0 (for 4 bit immediate) depending on the\n                                    number range. *&#x2F;\n    unsigned int headersize;     &#x2F;* prevrawlensize + lensize. *&#x2F;\n    unsigned char encoding;      &#x2F;* Set to ZIP_STR_* or ZIP_INT_* depending on\n                                    the entry encoding. However for 4 bits\n                                    immediate integers this can assume a range\n                                    of values and must be range-checked. *&#x2F;\n    unsigned char *p;            &#x2F;* Pointer to the very start of the entry, that\n                                    is, this points to prev-entry-len field. *&#x2F;\n&#125; zlentry;\n\n\n二级编码技巧\n\n使用集合类型保存单值的键值对，把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value\n以图片 ID 1101000060 和图片存储对象 ID 3302000080 为例，可以把图片 ID 的前 7 位（1101000）作为 Hash 类型的键，把图片 ID 的最后 3 位（060）和图片存储对象 ID 分别作为 Hash 类型值中的 key 和 value\n\n\n\n\n跳表\n\n在链表的基础上，增加了多级索引，通过索引位置的几个跳转就可以实现数据的快速定位，查找的复杂度是O(logN)\n\n如何设计层高的：跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。\n\n示例\n\n\n时间复杂度：二叉查找树的时间复杂度是O(logn)，空间复杂度是O(n)；跳表的时间复杂度是O(log_{k}n)，k为跳表索引步长，空间复杂度是O(n)\n\n\n\nString的底层实现\n\n相比于c语言的字符串\n\n拼接时会先考虑内存空间，防止内存溢出\n使用len保存了当前字符串的长度，计算长度的时间复杂度是O(1)的\n减少内存分配次数：为了避免修改（增加/减少）字符串时，每次都需要重新分配内存（C 语言的字符串是这样的），SDS 实现了空间预分配和惰性空间释放两种优化策略。当 SDS 需要增加字符串时，Redis 会为 SDS 分配好内存，并且根据特定的算法分配多余的内存，这样可以减少连续执行字符串增长操作所需的内存重分配次数。当 SDS 需要减少字符串时，这部分内存不会立即被回收，会被记录下来，等待后续使用（支持手动释放，有对应的 API）\n二进制安全：C 语言中的字符串以空字符\\\\\\\\0作为字符串结束的标识，这存在一些问题，像一些二进制文件（比如图片、视频、音频）就可能包括空字符，C 字符串无法正确保存。SDS 使用 len 属性判断字符串是否结束，不存在这个问题\n\n\nString 还是 Hash 存储对象数据更好\n\nString 存储的是序列化后的对象数据，存放的是整个对象。Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改或者添加部分字段，节省网络流量。如果对象中某些字段需要经常变动或者经常需要单独查询对象中的个别字段信息，Hash 就非常适合\nString 存储相对来说更加节省内存，缓存相同数量的对象数据，String 消耗的内存约是 Hash 的一半。并且，存储具有多层嵌套的对象时也方便很多。如果系统对性能和资源消耗非常敏感的话，String 就非常适合\n在绝大部分情况，使用 String 来存储对象数据即可\n\n\nString内存占用：以key和value都为10位的整数为例，key16B、value16B、dictEntry32B，合计64B\n\n针对初始化的长度决定用多少字节的struct（支持1、2、4、8），可以减少内存的使用，数据用char buf[]存储\n\nString本身空间占用\n\nint编码：当保存的是 Long 类型整数（8B）时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了\n\nembstr编码：当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片\n\nraw编码模式：当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构\n\n\n\n\n全局哈希的dictEntry结构\n\ndictEntry结构：key指针、value指针、next指针分别为8B\n\n\nRedis的内存分配库jemalloc：分配比所需空间大的最小2次幂走位分配空间，减少分配次数，例如上面的dictEntry结构占用24字节空间\n\n\n\n\n\n\n\nzset\n\n底层实现：\n如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构；\n如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构；\n\n\n范围查询：Redis的ZSet的范围查询命令ZRANGE的时间复杂度是O(log(N)+M)，其中N是有序集合的元素数量，M是返回的元素数量。\n\n\n\n\n\n2.线程模型\n单线程机制：多线程机制会带来不必要的开销，出现并行变串行的情况；通过优化（内存上进行操作、高效的数据结构、多路复用机制）单线程提高性能\n\n阻塞点\n\n客户端：网络 IO（多路复用机制优化），键值对增删改查操作（O(N)操作会阻塞，如全量查询、聚合统计），数据库操作\n\n查询：keys * （获取所有的 key 操作）、Hgetall（返回哈希表中所有的字段和）、smembers（返回集合中所有成员）\n\n优化：可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合计算\n\n\nbigkey删除：短时释放大量内存，删除操作需要释放内存，将空闲内存插入到空闲内存块链表，内存块过多会影响链表操作时间，从而造成Redis主线程的阻塞。bigkey删除即删除包含大量元素的集合，其不同类型常见耗时如下：\n\n优化1：从Redis4.0开始，当集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，建议使用 UNLINK 命令\n优化2：4.0之前，先使用集合类型提供的 SCAN 命令读取数据，然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避免一次性删除大量 key 给主线程带来的阻塞\n\n\n\n清空数据库：如 FLUSHDB 和 FLUSHALL 操作原理同上bigkey删除\n\n优化：从Redis4.0开始，可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库\n\n\n\n\n磁盘：生成 RDB 快照（子进程），记录 AOF 日志，AOF 日志重写（子进程）\n\n记录 AOF 日志：会根据不同的写回策略对数据做落盘保存。一个同步写磁盘的操作的耗时大约是 1～2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程\n\n\n主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件\n\n接收RDB文件：主库在复制的过程中，创建和传输 RDB 文件都是由子进程来完成的，不会阻塞主线程。但是，对于从库来说，它在接收了 RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库，这会阻塞主线程\n加载RDB文件：从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和 RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢\n优化：把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以较快的速度加载\n\n\n\n\n切片集群实例：向其他实例传输哈希槽信息，数据迁移\n\nRedsi动态扩缩容时，为保证数据一致性，迁移操作都是同步操作，当没有 bigkey 时，切片集群的各实例在进行交互时不会阻塞主线程，一旦有bigkey且内存占用过大时，会触发集群内的故障转移，造成不必要的切换\n\n\n异步子线程机制：除了查询和加载RDB文件这两个读操作，都可以使用异步子线程机制\n\nRedis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行\n惰性删除（Redis4.0）：主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成，数据会在子线程获取任务后才开始删除（使用UNLINK而不是DEL）\nAOF日志的everysec选项：主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了\n\n\n\n\n6.0版本的多线程：为了提高网络IO读写性能，这部分时Redis的一个性能瓶颈，多线程默认是禁用的，不建议开启\n\nredis性能变慢的检测方法\n\n基于当前环境下的Redis基线性能判定Redis是否真的变慢（./redis-cli --intrinsic-latency 120    ）\n\n系统排查及应对方案\n\n自身操作特性：看日志是否有慢查询命令、看是否有key集中过期的情况（EXPIREAT、EXPIRE）、是否存在bigkey、是否在进行自动内存整理\n\n操作系统：Redis是内存数据库，操作系统的内存机制会直接影响Redis的内存效率，如swap机制、内存大页机制\n\n触发swap的原因：物理机器内存不足，Redis实例使用了大量内存、机器上其它进程进行读写操作占用内存，解决方法为，增加机器的内存或使用Redis集群\n$ redis-cli info | grep process_id\nprocess_id: 5332\n$ cd &#x2F;proc&#x2F;5332\n$cat smaps | egrep &#39;^(Swap|Size)&#39;\nSize: 584 kB #一块内存大小\nSwap: 0 kB #有多少内存被swap到磁盘上\nSize: 4 kB\nSwap: 4 kB\nSize: 462044 kB\nSwap: 462008 kB #出现几百MB时，表明Redis实例的内存压力很大，很可能变慢\n大量短连接请求：Redis处理大量短连接请求，TCP三次握手和四次挥手也会增加耗时，可以使用长连接操作Redis\n\n内存大页机制：持久化时通过写时复制机制保证继续响应请求，即使只改小部分数据也需要拷贝整个大页，影响Redis正常的访存操作，所以一般关闭内存大页机制\necho never &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n\n\n文件系统：Redis会持久化到磁盘，文件系统写磁盘的机制会影响Redis持久化的效率，如AOF模式不同的写回策略会导致不同的延迟（检查配置）\n\nAOF日志提供了三种日志写回策略no、everysec、always，依赖底层的系统调用write和fsync，后两种写回策略都使用了fsync，但是everysec使用了后台子线程异步完成fsync操作而always没有使用，fsync需要等写回磁盘才返回\nAOF重写会进行大量的IO操作，阻塞fsync操作（等待写完磁盘才返回），主线程虽不等待fsync操作，但是会导致主线程的下一次fsync操作被阻塞（等待上一次的fsync），从而阻塞主线程\n是否运行了 Redis 主从集群，如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞\n\n\n\n\n\n\n\n\n基于多路复用的高性能I/O模型（epoll网络框架）\n\n传统做法：阻塞IO模型\n为了处理一个 Get 请求，需要监听客户端请求（bind/listen），和客户端建立连接（accept），从 socket 中读取请求（recv），解析客户端发送请求（parse），根据请求类型读取键值数据（get），最后给客户端返回结果，即向 socket 中写回数据（send）\n其中的accept、recv操作都是潜在的阻塞点，如果发生阻塞，Redis就不能再响应其它请求\n\n\n基于多路复用的高性能I/O模型（select/epoll）\nRedis向内核注册事件和对应的事件回调函数，由内核来同时保存并监听多个套接字（FD）上的连接请求或数据请求，一旦有请求到达，通过select/epoll提供的基于事件的回调机制（不同事件的不同处理函数）来实现。select/epoll在检测到FD上有请求到达时（事件发生），就将对应事件插入到事件队列中，Redis一直在对事件队列进行处理（如调用epoll_wait函数取事件队列的数据），这样就不会阻塞在某一具体的请求上了\n\n\n\n\n事务：不支持原子性、不支持回滚、每条命令都与服务器交互，所以不推荐使用Redis的事务\n\nRedis 可以通过MULTI（开始事务），EXEC（执行事务），DISCARD（取消事务） 和 WATCH 等命令来实现事务功能\nWATCH 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行\n通过WATCH命令监听指定的 Key，当调用 EXEC命令执行事务时，如果一个被 WATCH命令监视的 Key 被 其他客户端/Session 修改的话，整个事务都不会被执行\n不过，如果 WATCH与 事务在同一个 Session 里，并且被 WATCH监视的 Key 被修改的操作发生在事务内部，这个事务是可以被执行成功\n\n\nRedis事务不支持原子性：Redis 事务在运行错误的情况下，除了执行过程中出现错误的命令外，其他命令都能正常执行。并且，Redis 事务是不支持回滚（roll back）操作的。因此，Redis 事务其实是不满足原子性的（而且不满足持久性）\n如果事务中使用的命令语法没问题时，可以保证原子性，所以需要严格按照 Redis 的命令规范进行程序开发，并且通过 code review 确保命令的正确性\n\n\n一致性（支持）：在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的\n实例发生故障时，如果有RDB则可以保证一致性；如果有AOF也可以保证一致\n\n\n隔离性（支持）\n并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证\n并发操作在 EXEC 命令后执行，此时，隔离性可以保证\n\n\n持久性：AOF的三种配置会导致数据丢失、RDB快照间隙宕机也会丢失数据\n除了不满足原子性之外，事务中的每条命令都会与 Redis 服务器进行网络交互，这是比较浪费资源的行为。明明一次批量执行多个命令就可以了，这种操作实在是看不懂。因此，Redis 事务是不建议在日常开发中使用的\n\n\n并发安全性：针对读-改-写操作\n\n原子操作\n\n方法一：Redis每个命令是原子性的，可以通过把多个操作在 Redis 中实现成一个操作，实现单命令操作\n如：数据修改涉及读-改-写三个步骤，可以通过INCR/DECR命令可以对数据进行增值 / 减值操作\n\n\n方法二：使用Lua脚本，用于Redis没有提供原子命令的情况\n\n\n加分布式锁\n\n缺点：将低并发安全性，分布式锁的实现困难\n\n单个Redis实现分布式锁\n\n实现：赋予锁变量一个变量名，把这个变量名作为键值对的键，而锁变量的值，则是键值对的值\n\n原子操作\n\n使用SETNX命令实现加锁操作：执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置\n释放锁：可以在执行完业务逻辑后，使用 DEL 命令删除锁变量\n\n\n问题\n\n释放失败：给锁加一个过期时间，即使持有锁的客户端发生了异常，无法主动地释放锁，Redis 也会根据锁变量的过期时间，在锁变量过期后，把它删除\n\n加锁后被另一客户端误删再创建新锁：区分来自不同客户端的锁操作，让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端\n\n优化后的实现\n# 加锁, unique_value作为客户端唯一性的标识\n# 使用了 NX 选项，SET 命令只有在键值对不存在时，才会进行设置，否则不做赋值操作\n# PX 10000 则表示 lock_key 会在 10s 后过期，以免客户端在这期间发生异常而无法释放锁\nSET lock_key unique_value NX PX 10000\n\n\n# 释放锁 比较unique_value是否相等，避免误释放，使用的Lua脚本实现的释放锁操作的伪代码\nif redis.call(&quot;get&quot;,KEYS[1]) &#x3D;&#x3D; ARGV[1] then\n    return redis.call(&quot;del&quot;,KEYS[1])\nelse\n    return 0\nend\n## 执行上面的脚本\nredis-cli  --eval  unlock.script lock_key , unique_value \n\n\n\n\n多个Redis实现分布式锁（Redlock）\n\n算法思路：让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失\n算法执行步骤\n客户端获取当前时间\n客户端按顺序依次向 N 个 Redis 实例执行加锁操作\n使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识\n客户端获取锁的总耗时没有超过锁的有效时间\n\n\n一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时\n需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况\n\n\n\n\n\n\n\n\n\n\n其它\n\n\n3.内存管理\n内存管理\n\n设置过期时间：内存有限，保存所有数据迟早会OOM\n\nRedis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间\n很多时候业务场景就是需要某个数据只在某一时间段内存在，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效，使用传统的数据库来处理的话，一般都是自己判断过期，这样更麻烦并且性能要差很多\n\n\n如何判断数据过期（过期字典）\n\n通过过期字典（可看作hash表）来保存数据过期的时间，过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）\n过期数据的删除策略：Redis采用定期删除+惰性删除，对于漏掉的过期key使用内存淘汰机制\n惰性删除 ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。\n定期删除 ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响\n\n\n大量key集中过期的问题\n给 key 设置随机过期时间\n开启 lazy-free（惰性删除/延迟释放） 。lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程（不建议使用）\n\n\n\n\nRedis 内存淘汰机制：干净数据直接删除，脏数据需要写回数据库\n\n不进行数据淘汰\nno-eviction：==禁止驱逐数据==，也就是说当内存不足以容纳新写入数据时，新写入操作会报错，很少用\n\n\n在设置了过期时间（EXPIRE命令）的数据中进行淘汰\nvolatile-lru（least recently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰，LRU策略的实现如下：\nRedisObject 结构来保存数据的，RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳\n并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选\n\n\nvolatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰\nvolatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰\nvolatile-lfu（4.0版 least frequently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰\n缓存污染问题：大量不再访问的数据滞留在缓存中，影响应用的性能；所以需要在写满之前就经常淘汰数据\nLFU策略\n从两个维度筛选并淘汰数据，数据访问的时效性（访问时间离当前时间的远近）和数据的被访问次数\nLFU 缓存策略的优化：在LRU策略基础上，为每个数据增加一个计数器，来统计这个数据的访问次数。筛选时先淘汰访问次数少的，访问次数相同时再淘汰掉距离上一次访问时间更久的数据\nLFU具体实现：把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分，ldt 值（前 16bit，表示数据的访问时间戳）；counter 值（后 8bit，表示数据的访问次数）\nLFU计数规则：每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1，可以减慢counter值达到255的速度\nLFU衰减机制：使用衰减因子配置项 lfu_decay_time 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以lfu_decay_time值，所得的结果就是数据 counter 要衰减的值\n\n\n\n\n\n\n在所有数据中进行淘汰\nallkeys-lru（least recently used）：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（==这个是最常用的==）\nallkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰，适用于没有明显冷热数据的情况\nallkeys-lfu（4.0版 least frequently used）：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key\n\n\n\n\n内存碎片\n\n产生的原因\n\nOS的内存分配机制：默认使用jemalloc内存分配器，其按照2的幂次大小来分配内存空间，减少分配次数的情况下产生了内部碎片\nRedis的负载特征：频繁修改 Redis 中的数据，当 Redis 中的某个数据删除时，Redis 通常不会轻易释放内存给操作系统\n\n\n查看内存碎片率（&gt;1.5才需要清理）\n\n内存碎片率：mem_fragmentation_ratio = used_memory_rss / used_memory\n\nredis-cli -p 6379 info | grep mem_fragmentation_ratio\n如何清理：Redis4.0-RC3 版本以后自带了内存整理，可以避免内存碎片率过大的问题（之前版本可以直接重启）\nconfig set activedefrag yes #启用了自动清理功能\n\n###具体清理\n# 内存碎片占用空间达到 500mb 的时候开始清理\nconfig set active-defrag-ignore-bytes 500mb\n# 内存碎片率大于 1.5 的时候开始清理\nconfig set active-defrag-threshold-lower 50\n\n###减少对Redis性能的影响\n# 内存碎片清理所占用 CPU 时间的比例不低于 20%\nconfig set active-defrag-cycle-min 20\n# 内存碎片清理所占用 CPU 时间的比例不高于 50%\nconfig set active-defrag-cycle-max 50\n\n\n缓冲区\n\n用一块内存空间来暂时存放命令数据，以免出现因为数据和命令的处理速度慢于发送速度而导致的数据丢失和性能问题，当缓冲区占用的内存超出了设定的上限阈值时，就会出现缓冲区溢出（bigkey/大RDB、处理慢、缓冲区小）\n\n客户端输入和输出缓冲区：输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端\n\n输入缓冲区溢出时\n\n发生场景：写入bigkey、服务端处理请求慢，通过CLIENT LIST命令可以查看客户端的输入缓冲区（qbuf）使用情况\n解决办法：避免客户端写入bigkey、避免Redis主线程阻塞\n\n\n输出缓冲区溢出时\n\n发生场景：返回bigkey、执行了MONITOR命令、缓冲区大小设置不合理\n\nMONITOR 命令是用来监测 Redis 执行的，执行后会持续输出监测到的各个命令操作，持续占用输出缓冲区\n\n缓冲区大小：与输入缓冲区不同输出缓冲区可以设置大小（client-output-buffer-limit）\n# normal 表示当前设置的是普通客户端，第 1 个 0 设置的是缓冲区大小限制\n# 第 2 个 0 和第 3 个 0 分别表示缓冲区持续写入量限制和持续写入时间限制\nclient-output-buffer-limit normal 0 0 0\n\n\n\n\n主从集群中的缓冲区\n\n全量复制：主节点向从节点传输RDB文件时，持续接受客户端发送的写命令请求，并保存在复制缓冲区中，主节点为每一个客户端维护一个复制缓冲区，RDB传输的慢就会导致复制缓冲区溢出，主节点会结束该连接导致全量复制失败\n建议把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令\n使用 client-output-buffer-limit 配置项，来设置合理的复制缓冲区大小\n控制从节点数量：主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和\n\n\n增量复制：主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区（主从同步中的repl_backlog_buffer）中，读取断连期间主节点接收到的写命令，进而进行增量同步\n环形缓冲区：一旦发生溢出，新写入的命令数据就会覆盖旧的命令数据，导致旧命令数据的丢失，进而导致主从节点重新进行全量复制\n\n\n\n\n\n\n\n\n三种常用的缓存读写策略：旁路缓存、读写穿透、异步缓存\n\nCache Aside Pattern（旁路缓存）：同时维护db和cache，并且以db的结果为准\n\n读取数据流程\n\n\n写数据中的问题\n\n正确方式：先更新db，在直接删除cache\n\n问题一：在写数据的过程中，可以先删除 cache ，后更新 db 么？\n\n回答：会有数据不一致的问题，在删除cache和更新db的过程中，如果有请求从db读取，会读到旧数据\n\n\n问题二：在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？\n\n在请求读取数据后，将新数据写入到缓存这个过程中，如果有请求更新db，那么读取数据的请求插入到cache中的就是旧数据\n\n\n\n\n\n旁路缓存模式的缺陷：\n\n首次请求数据一定不在cache中：提前缓存热点数据\n写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率\n数据库和缓存数据强一致场景 ：更新 db 的时候同样更新 cache，不过需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题\n可以短暂地允许数据库和缓存数据不一致的场景 ：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小\n\n\n保证缓存和数据库数据的一致性（更新数据库成功，但删除缓存这一步失败的情况）\n增加 cache 更新重试机制： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可\n\n\n\n\n\n\nRead/Write Through Pattern（同步直写）：将cache视为主要数据存储，cache服务负责将数据读取和写入db（很少用），对于首次请求不在cache中的问题，可以提前缓存热点数据\n\n写\n\n\n读\n\n\n\n\nWrite Behind Pattern（异步缓存写入）\n\n与读写穿透类似，都是cache负责db的读写，但是读写穿透是同步更新cache和db，而异步缓存写入更新缓存后，不直接更新db，改为异步批量的方式更新db\n开发中很少见，因为会有数据一致性的问题（没写入db就丢失），应用场景主要是消息队列中消息的异步写入磁盘、MySQL的Innodb Buffer Pool机制\n异步缓存写入下db的写性能非常高，非常适合一些数据经常变化又对数据一致性要求不高的场景，比如浏览量、点赞量\n\n\n\n\n缓存相关问题\n\n缓存雪崩\n\n缓存在同一时间大面积的失效，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力\n预防机制：构建Redis缓存高可靠集群，如果Redis缓存的主节点故障宕机了，可以进行主从切换\n\n\n原因一：缓存中有大量数据同时过期，导致大量请求无法得到处理\n避免给大量的数据设置相同的过期时间，在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟）\n通过服务降级，来应对缓存雪崩，即针对不同的数据采取不同的处理方式\n针对非核心数据请求，停止从缓存中查询这些数据，直接返回预定义的信息、空值或错误信息\n针对核心数据请求，仍然允许查询缓存，缓存缺失时继续通过数据库读取\n\n\n\n\n原因二：Redis缓存实例发生故障宕机，无法处理请求\n服务熔断：为了防止连锁的数据库雪崩，暂停应用对缓存系统的接口访问，会影响整个业务应用的运行\n请求限流：只允许通过一小部分请求，避免大量并发请求压力传递到数据库层\n\n\n\n\n缓存击穿\n\n请求的 key 对应的是热点数据，该数据存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期），这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了\n如：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n解决办法\n设置热点数据永不过期或者过期时间比较长\n针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期\n请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力\n\n\n\n\n缓存穿透\n\n大量请求的 key 是不合理的，根本不存在于缓存中，也不存在于数据库中。这就导致这些请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了\n\n业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据\n恶意攻击：专门访问数据库中没有的数据\n\n\n解决一：做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等\n\n解决二：布隆过滤器，非常方便的判断一个给定的数据是否存在于海量数据中\n\n布隆过滤器原理    \n\n首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值\n然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置\n最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作\n\n\n把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程\n\n误判：布隆过滤器说存在，则可能不存；但是说不存在则一定不存在\n\n计算哈希值，将位数组中对应下标设置为1；判断时检查位数组对应值是否是1（不同字符串可能哈希出来的位置相同）\n\n\n使用**docker redis bloomfilter**\n➜  ~ docker run -p 6379:6379 --name redis-redisbloom redislabs&#x2F;rebloom:latest\n➜  ~ docker exec -it redis-redisbloom bash\nroot@21396d02c252:&#x2F;data# redis-cli\n127.0.0.1:6379&gt;\n\n\n解决三：在请求入口的前端进行请求检测，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库\n\n\n\n\n\n\n2.高可靠1.数据持久化\nAOF\n写后日志：首先执行命令写入内存，然后再将命令记录到日志中。与传统数据库的写前（WAL）日志相比，避免了额外的检查开销，并且不会阻塞当前的命令（但会阻塞后一条命令）。但是在写入日之前宕机会丢失日志\n写回策略：控制一个写命令执行完后AOF日志写回磁盘的时机，即appendfsync配置项的三个可选值\nAlways，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；可以保证不丢失数据，但是回影响主线程性能\nEverysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘\nNo，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘，会有数据丢失的风险\n\n\nAOF重写机制：防止AOF文件过大，故障恢复时恢复过程缓慢\nRedis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入（将多个命令合并成一个命令）\nAOF使用后台子线程bgrewriteaof来完成，避免阻塞主线程：重写时fork出后台bgrewriteaof子线程，通过拷贝父进程的页表的方式共享父进程的内存数据的方式来共享父进程的数据\n写时复制：避免一次性大量拷贝给子进程造成的长时间阻塞问题，在父进程写入操作是一个已经存在的key时，父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间（否则子进程读完，父进程修改，就会丢失这一次修改的数据）\n\n\n两次日志写入：在重写过程中，新请求会先写入到原AOF文件的缓冲区中，然后写入到重写日志的缓冲区，在重写机制结束后再合并到AOF重写日志中（但是需要上面的写时复制来保证数据不会丢失修改）\n\n\n\n\nRDB\nAOF在故障恢复的时候需要逐一执行命令，恢复时间长，所以提出了RDB内存快照的方式来高效的恢复，提供了两个命令\nsave：在主线程中执行，会导致阻塞\nbgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置\n\n\n写时复制：在执行快照的同时，正常处理写操作\n由父进程fork出bgsave子进程，然后开始读取主线程的内存数据，并写入到RDB文件中，在主线程有写入请求时，这块数据会被复制一份，然后主线程在数据副本上进行修改，bgsave子进程继续将原来的数据写入RDB文件\n\n\n优化\n增量快照：一直做全量快照，虽然bgsave执行时不阻塞主线程，但是会对磁盘造成压力，而且fork操作本身也会阻塞主线程。通过记录修改的元数据信息来做增量快照，但是又会产生大量的额外空间开销\nRedis 4.0 中提出了一个混合使用 AOF 日志和内存快照：内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作\n\n\n\n\n如何选择 RDB 和 AOF？\nRDB 比 AOF 优秀的地方\nRDB 文件存储的内容是经过压缩的二进制数据， 保存着某个时间点的数据集，文件很小，适合做数据的备份，灾难恢复。AOF 文件存储的是每一次写命令，类似于 MySQL 的 binlog 日志，通常会必 RDB 文件大很多。当 AOF 变得太大时，Redis 能够在后台自动重写 AOF。新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。不过， Redis 7.0 版本之前，如果在重写期间有写入命令，AOF 可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。\n使用 RDB 文件恢复数据，直接解析还原数据即可，不需要一条一条地执行命令，速度非常快。而 AOF 则需要依次执行每个写命令，速度非常慢。也就是说，与 AOF 相比，恢复大数据集的时候，RDB 速度更快。\n\n\nAOF 比 RDB 优秀的地方\nRDB 的数据安全性不如 AOF，没有办法实时或者秒级持久化数据。生成 RDB 文件的过程是比较繁重的， 虽然 BGSAVE 子进程写入 RDB 文件的工作不会阻塞主线程，但会对机器的 CPU 资源和内存资源产生影响，严重的情况下甚至会直接把 Redis 服务干宕机。AOF 支持秒级数据丢失（取决 fsync 策略，如果是 everysec，最多丢失 1 秒的数据），仅仅是追加命令到 AOF 文件，操作轻量。\nRDB 文件是以特定的二进制格式保存的，并且在 Redis 版本演进中有多个版本的 RDB，所以存在老版本的 Redis 服务不兼容新版本的 RDB 格式的问题。\nAOF 以一种易于理解和解析的格式包含所有操作的日志。你可以轻松地导出 AOF 文件进行分析，你也可以直接操作 AOF 文件来解决一些问题。比如，如果执行FLUSHALL命令意外地刷新了所有内容后，只要 AOF 文件没有被重写，删除最新命令并重启即可恢复之前的状态\n\n\n由于 RDB 和 AOF 各有优势，于是，Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差\n\n\n\n2.主从复制\n使用主从库模式，通过增加副本冗余量，将一份数据同时保存在多个实例上。主从库之间采用读写分离的方式，主库和从库同时支持读操作，写操作通过主库执行然后同步到从库\n不采用主从库读写分离：需要加锁或实例间协商的方式完成修改，带来更大的开销\n\n\n主从库模式的建立\n启动多个Redis实例时，他们相互之间通过replicaof命令形成主库和从库的关系，按照三个阶段完成第一次同步\n第一阶段：从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了\n具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID（实例的随机ID，第一次设置为？）和复制进度offset（-1表示第一次复制）两个参数\n主库收到 psync 命令后，会用 FULLRESYNC 响应命令（全量复制）带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。\n\n\n第二阶段：主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的RDB文件\n主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空\n在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作\n\n\n第三阶段：主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了\n\n\n\n\n优化\n主从级联模式分担全量复制时的主库压力：主库需要进行两个耗时操作，生成RDB文件和传输RDB文件，如果从库数量过多就会忙于fork子进程生成RDB文件，通过主-从-从模式将主库的压力分担下去，让一些从库不再和主库交互，只和级联的从库进行写操作同步，减轻主库上的压力\n基于长连接的命令传播：主从库完成全量复制后，会一直维护一个网络连接，主库通过这个连接将后续命令同步给从库\n增量复制：主从库间网络断了，2.8之前进行全量复制，2.8之后采用部分增量复制仍需全量同步，4.0版本后进行增量同步\nrepl_backlog_buffer缓冲区：repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置\n当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区\n主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距\n\n\n注意事项\n通过reolid和replid2来判断主从切换的时候，新的master和slave是否曾经属于同一个主库，如果属于可进行增量同步的尝试\nmaster同步速度必须比slave快，且不能超过环形缓冲区大小，否则还是要进行全量同步操作\nrepl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致，可以通过调整 repl_backlog_size 这个参数来避免\n一个从库如果和主库断连时间过长，造成它在主库repl_backlog_buffer的slave_repl_offset位置上的数据已经被覆盖掉了，此时从库和主库间将进行全量复制\n每个从库会记录自己的slave_repl_offset，每个从库的复制进度也不一定相同。在和主库重连进行恢复时，从库会通过psync命令把自己记录的slave_repl_offset发给主库，主库会根据从库各自的复制进度，来决定这个从库可以进行增量复制，还是全量复制\n\n\n\n\n\n\n\n3.哨兵机制\n哨兵机制：在Redis主从集群中，实现主从库自动切换的机制，有效地解决了三个问题（主库判活、从库升级为主库、新主库同步消息到从库和客户端），即监控、选主和通知三个任务\n\n监控：哨兵在运行时周期性的给所有的主从库发送PING命令，检测他们是否仍在运行，如果从库规定时间内没响应，则标记为下线状态（主观下线）；如果主库规定时间内也没有响应，则开始自动切换主库的流程（主观下线）\n客观下线：主观下线如果是误判（网络压力大、主库压力大），会产生额外的通信和计算开销，所以选择多哨兵实例的哨兵集群的方式来减少误判率\n客观下线的标准：当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”，但是这个数量标准可以通过设置来指定\n哨兵领导者：哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换\n\n\n选主：在已有从库中，通过一定的规则（筛选+打分）选择一个从库实例，将其升级为主库\n筛选\n判断从库的当前在线状态：从库仍在运行\n判断之前的网络状态：使用配置项down-after-milliseconds * 10，down-after-milliseconds是从库断连的最大连接超时时间，如果down-after-milliseconds内从库都没有连接上则认为主从节点断连，如果从库从运行到现在一共断连次数超过10次，则认为从库网络状况不好（在sentinel.conf中配置）\n\n\n打分：按照三个规则依次打分（从库优先级、从库复制进度以及从库 ID 号），只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。\n第一轮：优先级最高的从库得分高，通过 slave-priority 配置项配置\n第二轮：和旧主库同步程度最接近的从库得分高，repl_backlog_buffer缓冲区的位置，主为master_repl_offset，副为slave_repl_offset，选择复制最快的（选slave_repl_offset最大的）\n第三轮：ID 号小的从库得分高，每个实例都会有一个 ID，类似于从库的编号\n\n\n\n\n通知：在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让他们执行repliicaof命令，和新主库建立连接并进行数据复制。同时，哨兵还会把新主库的连接信息发送给客户端，让它们把请求操作发到新主库上\n通知客户端：哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端\n如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。 所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址\n\n\n\n\n哨兵集群\n\n基于 pub/sub 机制的哨兵集群组成：不同哨兵通过___sentine__:hello频道来相互发现、实现互相通信\n\n主库和哨兵：哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口\n从库和哨兵：哨兵向主库发送 INFO 命令后可以知道从库的IP地址和端口\n\n\n基于 pub/sub 机制的客户端事件通知\n\n从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件\n\n重要频道使用：客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，可以在客户端执行订阅命令（SUBSCRIBE [下面的频道]），来获取不同的事件消息\n\n\n\n\n由哪个哨兵执行主从切换\n\n任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票\n要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds\n\n\n一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的\n需要同时满足：拿到半数以上的赞成票（选举Leader），并且票数需要大于quorum值（判读客观下线）\n\n\n此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader\n如果一轮投票没选出来Leader，哨兵集群就等待一段时间（哨兵故障转移超时时间的2倍），再重新选举\n\n\n\n\n\n\n相关问题：\n\n哨兵机制能防止脑裂吗\nmaster和两个slave节点因网络问题被隔离时，所有写入到master的数据都会丢失（网络恢复后master节点会变为新master的slave）\n解决办法\nmin-replicas-to-write 1：配置写master至少写入的slave数量，0表示关闭此功能，3个节点的情况下，可以配置为1\nmin-replicas-max-lag 10：配制master多长时间无法得到从节点的响应，就认为这个节点失联，失联则停止新的写入命令请求\n\n\n\n\nRaft协议\n\n\n\n3.高可扩展1.数据分片\n单实例：使用RDB进行持久化时，fork子进程的用时与Redis数据量是正相关的，所以采用切片/分片集群，同时启动多个Redis实例组成一个集群，然后按照一定的规则，把收到的数据划分为多份，每一份用一份实例来保存\n\nRedis在单机情况下支持多个数据库（同一个访问密码，FLUSHALL可以同时清空所有数据），每个数据库对外都是一个从0开始的递增数字命名，Redis默认支持16个数据库。并且可以随时使用SELECT命令更换数据库\n\n\nRedis Cluster：用于实现切片集群的方案，方案中规定了数据分片和实例的对应关系\n\n哈希槽：一个切片集群共有16384个哈希槽，根据键值对的key，按照CEC16算法计算一个16bit的值，然后与16384取模确定对应的哈希槽。哈希槽默认被均分到Redis实例上，也可以通过命令来配置（cluster meet、cluster addslots）\n\n为什么Redis Cluster的哈希槽是16384个：CRC16算法可以产生16位（65536），但是只用了14位（16384）\n通过bitmap来维护哈希槽信息，如果该位为1，则表示这个哈希槽属于这个节点，哈希槽长度为2048（16384/8）。哈希槽总数越少，bitmap填充率越小，压缩效果越好\n正常的心跳包会携带一个节点的完整配置，也就是说会包含当前节点负责的哈希槽的信息，如果是65536则需要8k的空间，内存占用过高\n\n\n\n\n客户端如何定位数据所在实例\n\nRedis会把自己的哈希槽发给和他相连接的其他实例，来完成哈希槽分配信息的扩散，客户端会把哈希槽信息缓存在本地\n127.0.0.1:6379&gt; cluster slots\n1) 1) (integer) 0\n   2) (integer) 4095\n   3) 1) &quot;192.168.10.3&quot;\n      2) (integer) 6379\n2) 1) (integer) 12288\n   2) (integer) 16383\n   3) 1) &quot;192.168.10.5&quot;\n      2) (integer) 6379\n变化：集群中增减Redis实例、为了负载均衡重新划分，重新划分完后实例间使用上面的方式扩散信息\n\nCLUSTER SETSLOT：使用不同的选项进行三种设置，分别是设置 Slot 要迁入的目标实例，Slot 要迁出的源实例，以及 Slot 所属的实例\n\nCLUSTER GETKEYSINSLOT：获取某个 Slot 中一定数量的 key\n\nMIGRATE：把一个 key 从源实例实际迁移到目标实例\n#假设要把 Slot 300 从源实例（ID 为 3）迁移到目标实例（ID 为 5）\n#第 1 步，我们先在目标实例 5 上执行下面的命令，将 Slot 300 的源实例设置为实例 3，表示要从实例 3 上迁入 Slot 300\nCLUSTER SETSLOT 300 IMPORTING 3\n#第 2 步，在源实例 3 上，我们把 Slot 300 的目标实例设置为 5，这表示，Slot 300 要迁出到实例 5 上，如下所示：\nCLUSTER SETSLOT 300 MIGRATING 5\n#第 3 步，从 Slot 300 中获取 100 个 key。因为 Slot 中的 key 数量可能很多，所以我们需要在客户端上多次执行下面的这条命令，分批次获得并迁移 key。\nCLUSTER GETKEYSINSLOT 300 100\n#第 4 步，我们把刚才获取的 100 个 key 中的 key1 迁移到目标实例 5 上（IP 为 192.168.10.5），同时把要迁入的数据库设置为 0 号数据库，把迁移的超时时间设置为 timeout。我们重复执行 MIGRATE 命令，把 100 个 key 都迁移完。\nMIGRATE 192.168.10.5 6379 key1 0 timeout\n#最后，我们重复执行第 3 和第 4 步，直到 Slot 中的所有 key 都迁移完成。\n\n#从 Redis 3.0.6 开始，你也可以使用 KEYS 选项，一次迁移多个 key（key1、2、3），这样可以提升迁移效率。\nMIGRATE 192.168.10.5 6379 &quot;&quot; 0 timeout KEYS key1 key2 key3\n\n\n\n\n重定向\n\n当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回MOVED命令响应结果，这个结果中就包含了新实例的访问地址\n数据迁移过程中的请求：如果不在本地，则返回ASK报错信息返回新地址，客户端给新地址发送ASKING命令在发送数据请求命令（如果不发ASKING直接请求则会报错，因为新实例上还没有管理这个槽位）\n\n\n\n\nGossip协议：Redis Cluster中的节点的通信方式，cluster.h定义了所有消息类型和消息结构\n\nGossip 协议的工作原理可以概括成两点\n\n每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表\n一个实例在接收到 PING 消息后，会给发送 PING 消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样\n\n\nGossip 消息大小：clusterMsgDataGossip * 集群中实例个数 + 16384bit的Bitmap（slot信息）\ntypedef struct &#123;\n    char nodename[CLUSTER_NAMELEN];  &#x2F;&#x2F;40字节\n    uint32_t ping_sent; &#x2F;&#x2F;4字节\n    uint32_t pong_received; &#x2F;&#x2F;4字节\n    char ip[NET_IP_STR_LEN]; &#x2F;&#x2F;46字节\n    uint16_t port;  &#x2F;&#x2F;2字节\n    uint16_t cport;  &#x2F;&#x2F;2字节\n    uint16_t flags;  &#x2F;&#x2F;2字节\n    uint32_t notused1; &#x2F;&#x2F;4字节\n&#125; clusterMsgDataGossip; &#x2F;&#x2F;104字节 一个实例状态信息大小\n实例间通信频率\n\nRedis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法\n为了避免有实例一直没有被发送PING信息：Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现有实例最近一次接收 PONG 消息的时间，已经大于配置项 cluster-node-timeout 的一半了（cluster-node-timeout/2），就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息\n\n\n\n\n\n2.负载均衡附录\n如何使用慢查询日志和 latency monitor 排查执行慢的操作（也可以使用监控工具latency monitor）\n\n设置参数\n\nslowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录\nslowlog-max-len：慢查询日志最多能记录多少条命令记录（队列）\n\n\n使用SLOWLOG GET命令查看慢查询日志中记录的命令操作\nSLOWLOG GET 1\n1) 1) (integer) 33           &#x2F;&#x2F;每条日志的唯一ID编号\n   2) (integer) 1600990583   &#x2F;&#x2F;命令执行时的时间戳\n   3) (integer) 20906        &#x2F;&#x2F;命令执行的时长，单位是微秒\n   4) 1) &quot;keys&quot;               &#x2F;&#x2F;具体的执行命令和参数\n      2) &quot;abc*&quot;\n   5) &quot;127.0.0.1:54793&quot;      &#x2F;&#x2F;客户端的IP和端口号\n   6) &quot;&quot;                     &#x2F;&#x2F;客户端的名称，此处为空\n\n\n如何排查Redis的bigkey：./redis-cli  --bigkeys\n\n在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行\n可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能\n缺点：只能返回最大的那个bigkey，只统计个数不统计实际占用的内存量\n\n$ .&#x2F;redis-cli  --bigkeys\n\n-------- summary -------\nSampled 32 keys in the keyspace!\nTotal key length in bytes is 184 (avg len 5.75)\n\n&#x2F;&#x2F;统计每种数据类型中元素个数最多的bigkey\nBiggest   list found &#39;product1&#39; has 8 items\nBiggest   hash found &#39;dtemp&#39; has 5 fields\nBiggest string found &#39;page2&#39; has 28 bytes\nBiggest stream found &#39;mqstream&#39; has 4 entries\nBiggest    set found &#39;userid&#39; has 5 members\nBiggest   zset found &#39;device:temperature&#39; has 6 members\n\n&#x2F;&#x2F;统计每种数据类型的总键值个数，占所有键值个数的比例，以及平均大小\n4 lists with 15 items (12.50% of keys, avg size 3.75)\n5 hashs with 14 fields (15.62% of keys, avg size 2.80)\n10 strings with 68 bytes (31.25% of keys, avg size 6.80)\n1 streams with 4 entries (03.12% of keys, avg size 4.00)\n7 sets with 19 members (21.88% of keys, avg size 2.71)\n5 zsets with 17 members (15.62% of keys, avg size 3.40)\nbigkey如何解决\n\n对大Key进行拆分：例如将含有数万成员的一个HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围\n对大Key进行清理：将不适用Redis能力的数据存至其它存储，并在Redis中删除此类数据。注意，要使用异步删除。\n优化1：从Redis4.0开始，当集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，建议使用 UNLINK 命令\n优化2：4.0之前，先使用集合类型提供的 SCAN 命令读取数据，然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避免一次性删除大量 key 给主线程带来的阻塞\n\n\n监控Redis的内存水位：可以通过监控系统设置合理的Redis内存报警阈值进行提醒，例如Redis内存使用率超过70%、Redis的内存在1小时内增长率超过20%等\n对过期数据进行定期清：堆积大量过期数据会造成大Key的产生，例如在HASH数据类型中以增量的形式不断写入大量数据而忽略了数据的时效性。可以通过定时任务的方式对失效数据进行清理\n\n\nhotkey：\n\n定义\nQPS集中在特定的Key：Redis实例的总QPS（每秒查询率）为10,000，而其中一个Key的每秒访问量达到了7,000。\n带宽使用率集中在特定的Key：对一个拥有上千个成员且总大小为1 MB的HASH Key每秒发送大量的HGETALL操作请求。\nCPU使用时间占比集中在特定的Key：对一个拥有数万个成员的Key（ZSET类型）每秒发送大量的ZRANGE操作请求。\n\n\n如何解决\n在Redis集群架构中对热Key进行复制。在Redis集群架构中，由于热Key的迁移粒度问题，无法将请求分散至其他数据分片，导致单个数据分片的压力无法下降。此时，可以将对应热Key进行复制并迁移至其他数据分片，例如将热Key foo复制出3个内容完全一样的Key并名为foo2、foo3、foo4，将这三个Key迁移到其他数据分片来解决单个数据分片的热Key压力\n使用读写分离架构。如果热Key的产生来自于读请求，您可以将实例改造成读写分离架构来降低每个数据分片的读请求压力，甚至可以不断地增加从节点。但是读写分离架构在增加业务代码复杂度的同时，也会增加Redis集群架构复杂度。不仅要为多个从节点提供转发层（如Proxy，LVS等）来实现负载均衡，还要考虑从节点数量显著增加后带来故障率增加的问题。Redis集群架构变更会为监控、运维、故障处理带来了更大的挑战。\n\n\n\n\n整合Spring\n\n添加依赖、新增Spring下Redis的配置文件\n# Spring节点下\nredis:\n    host: localhost # Redis服务器地址\n    database: 0 # Redis数据库索引（默认为0）\n    port: 6379 # Redis服务器连接端口\n    password: # Redis服务器连接密码（默认为空）\n    jedis: #Redis的Java客户端\n      pool:\n        max-active: 8 # 连接池最大连接数（使用负值表示没有限制）\n        max-wait: -1ms # 连接池最大阻塞等待时间（使用负值表示没有限制）\n        max-idle: 8 # 连接池中的最大空闲连接\n        min-idle: 0 # 连接池中的最小空闲连接\n    timeout: 3000ms # 连接超时时间（毫秒）\n添加RedisService和RedisServiceImpl（注入StringRedisTemplate（继承自RedisTemplate））\npublic interface RedisService &#123;\n    &#x2F;**\n     * 存储数据\n     *&#x2F;\n    void set(String key, String value);\n\n    &#x2F;**\n     * 获取数据\n     *&#x2F;\n    String get(String key);\n\n    &#x2F;**\n     * 设置超期时间\n     *&#x2F;\n    boolean expire(String key, long expire);\n\n    &#x2F;**\n     * 删除数据\n     *&#x2F;\n    void remove(String key);\n\n    &#x2F;**\n     * 自增操作\n     * @param delta 自增步长\n     *&#x2F;\n    Long increment(String key, long delta);\n\n&#125;\n\n@Service\npublic class RedisServiceImpl implements RedisService &#123;\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    @Override\n    public void set(String key, String value) &#123;\n        stringRedisTemplate.opsForValue().set(key, value);\n    &#125;\n\n    @Override\n    public String get(String key) &#123;\n        return stringRedisTemplate.opsForValue().get(key);\n    &#125;\n\n    @Override\n    public boolean expire(String key, long expire) &#123;\n        return stringRedisTemplate.expire(key, expire, TimeUnit.SECONDS);\n    &#125;\n\n    @Override\n    public void remove(String key) &#123;\n        stringRedisTemplate.delete(key);\n    &#125;\n\n    @Override\n    public Long increment(String key, long delta) &#123;\n        return stringRedisTemplate.opsForValue().increment(key,delta);\n    &#125;\n&#125;\n在CRUD代码中，通过注入redisService，来实现缓存功能\n@Service\npublic class UmsMemberServiceImpl implements UmsMemberService &#123;\n    @Autowired\n    private RedisService redisService;\n    @Value(&quot;$&#123;redis.key.prefix.authCode&#125;&quot;)\n    private String REDIS_KEY_PREFIX_AUTH_CODE;\n    @Value(&quot;$&#123;redis.key.expire.authCode&#125;&quot;)\n    private Long AUTH_CODE_EXPIRE_SECONDS;\n\n    @Override\n    public CommonResult generateAuthCode(String telephone) &#123;\n        StringBuilder sb &#x3D; new StringBuilder();\n        Random random &#x3D; new Random();\n        for (int i &#x3D; 0; i &lt; 6; i++) &#123;\n            sb.append(random.nextInt(10));\n        &#125;\n        &#x2F;&#x2F;验证码绑定手机号并存储到redis\n        redisService.set(REDIS_KEY_PREFIX_AUTH_CODE + telephone, sb.toString());\n        redisService.expire(REDIS_KEY_PREFIX_AUTH_CODE + telephone, AUTH_CODE_EXPIRE_SECONDS);\n        return CommonResult.success(sb.toString(), &quot;获取验证码成功&quot;);\n    &#125;\n\n\n    &#x2F;&#x2F;对输入的验证码进行校验\n    @Override\n    public CommonResult verifyAuthCode(String telephone, String authCode) &#123;\n        if (StringUtils.isEmpty(authCode)) &#123;\n            return CommonResult.failed(&quot;请输入验证码&quot;);\n        &#125;\n        String realAuthCode &#x3D; redisService.get(REDIS_KEY_PREFIX_AUTH_CODE + telephone);\n        boolean result &#x3D; authCode.equals(realAuthCode);\n        if (result) &#123;\n            return CommonResult.success(null, &quot;验证码校验成功&quot;);\n        &#125; else &#123;\n            return CommonResult.failed(&quot;验证码不正确&quot;);\n        &#125;\n    &#125;\n\n&#125;\n\n\n分布式锁\n\n四个特性\n\n互斥性：锁的目的是获取资源的使用权，所以只让一个竞争者持有锁，这一点要尽可能保证；\n安全性：避免死锁情况发生。当一个竞争者在持有锁期间内，由于意外崩溃而导致未能主动解锁，其持有的锁也能够被正常释放，并保证后续其它竞争者也能加锁；\n对称性：同一个锁，加锁和解锁必须是同一个竞争者。不能把其他竞争者持有的锁给释放了，这又称为锁的可重入性。\n可靠性：需要有一定程度的异常处理能力、容灾能力。\n\n\n实现\n\nRedis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：\n\n如果 key 不存在，则显示插入成功，可以用来表示加锁成功；\n如果 key 存在，则会显示插入失败，可以用来表示加锁失败。\n\n\n基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。\n\n加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；\n锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；\n锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；\n\n\n满足这三个条件的分布式命令如下：\nSET lock_key unique_value NX PX 10000 \n\n\nlock_key 就是 key 键；\nunique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；\nNX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；\nPX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。\n\n\n解锁操作\n\n将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除\n\n因为有两个操作，所以需要保证原子性，可以通过Lua脚本实现\n&#x2F;&#x2F; 释放锁时，先比较 unique_value 是否相等，避免锁的误释放\nif redis.call(&quot;get&quot;,KEYS[1]) &#x3D;&#x3D; ARGV[1] then\n    return redis.call(&quot;del&quot;,KEYS[1])\nelse\n    return 0\nend\n\n\n\n\n守护线程\n\n用途：防止锁在业务没有执行完成后就释放掉了,开启一个线程来定期对这把锁进行延期操作\n业务线程挂掉了，然后守护线程一直还在更新这把锁的延期时间，会怎么样\n业务线程挂了分情况。一种是整个程序被干掉了，比如掉线了。一个情况是程序出现 bug 了，导致业务线程在执行解锁逻辑之前被干掉了，或者说业务线程死循环了、忘记解锁了这类情况，都归属于 bug\n第一种情况，程序被干掉了，那么守护线程也没了，所以不会续期了，时间到了就释放锁，这个没问题\n第二种情况，根据看门狗机制，它就是会无线续期。相当于变成了一个死锁。这是由它的工作原理决定的，无解。但是可以自己魔改一下看门狗机制，比如设定为续期 1000 次后还要续期，就有可能出问题了，那就释放锁。但是这个方案，聊胜于无。还不如设置一个较长的过期时间。\n\n\n我觉得这种情况不应该考虑怎么改进看门狗机制，而是应该考虑怎么监控它是否在正常运行。比如续期了 1000 次还在续期，就发个预警出来，人工看看啥情况，然后具体情况具体分析，是 bug 就修 bug，是正常运行就先不管。人工一介入，就没有啥不能解决的。\n\n\n\n\n\n\n\n\n","slug":"Redis","date":"2023-05-04T10:00:34.000Z","categories_index":"","tags_index":"database","author_index":"Dajunnnnnn"},{"id":"8aa9bb0438939ce2b4a00f1a6ea1e9e5","title":"Go","content":"GO1.运行前准备\n源码结构\n\nGOROOT：Go 语言安装根目录的路径，也就是 GO 语言的安装路径\nGOPATH：若干工作区目录的路径。是我们自己定义的工作空间（workspace），go源码文件（.go）、归档文件（.a）、可执行文件都存在此处\ngo源码文件需要保存在GOPATH包含的某个工作区（目录）中的src目录下的某个代码包（目录）中，可执行文件放在该工作区的bin子目录，归档文件（.a）放在pkg子目录\n安装某个代码包而产生的归档文件是与这个代码包同名的\n构建和安装Go程序的过程：构建使用命令go build，安装使用go install。构建和安装代码的时候都会执行编译、打包等操作，并且这些操作生成的任何文件都会先保存到某个临时的目录中\n\n\nGOBIN：GO 程序生成的可执行文件（executable file）的路径\n添加export PATH=$PATH:/usr/local/go/bin到~/.bash_profile 或 /etc/profile，然后执行source ~/.bash_profile或source /etc/profile\n\n\n\n\n源码文件\n\n命令源码文件：可以使用 go run 命令启动，是程序的运行入口，每个可独立运行的程序必须拥有的。可以通过构建或安装，生成与其对应的可执行入口，后者一般会与该命令源码文件的直接父目录同名。例如存在demo.go文件中的hello world示例程序，可以通过go run demo.go命令来执行，在标准输出（屏幕）上看到hello world。（模块化编程时，也只有一个命令源码文件）\npackage main\n\nimport (\n\t&quot;flag&quot;&#x2F;&#x2F;专门用于接收和解析命令参数\n  &quot;fmt&quot;\n)\nvar name string\nfunc init() &#123;\n  &#x2F;&#x2F;四个参数：存命令参数值的地址、命令参数的名称、默认值、该命令参数的简短说明\n  flag.StringVar(&amp;name,&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n  &#x2F;&#x2F;var name &#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#125;\nfunc main() &#123;\n  &#x2F;&#x2F;对该函数的调用必须在所有命令参数存储载体的声明（这里是对变量name的声明）和\n  &#x2F;&#x2F;设置（这里是在[2]处对flag.StringVar函数的调用）之后\n  flag.Parse()&#x2F;&#x2F;用于真正解析命令参数，并把它们的值赋给相应的变量\n  fmt.Printf(&quot;Hello, %s!\\\\n&quot;, name)\n&#125;\n\n&#x2F;&#x2F;查看参数的使用说明\n$ go run demo.go -name-&quot;Robert&quot;\n#输出\nHello,Robert!\n#查看命令源码文件的参数说明\n$ go build demo.go\n$ .&#x2F;demo --help\n#go run命令构建上述命令源码文件时临时生成的可执行文件的完整路径,即go build demo.go\nUsage of .&#x2F;demo:\n -name string\n    The greeting object. (default &quot;everyone&quot;)\n库源码文件：库源码文件是不能被直接运行的源码文件，仅用于存放程序实体，这些程序实体可以被其他代码使用。（程序实体是变量、常量、函数、结构体和接口的统称，总是先声明再使用，程序实体的名字统称为标识符）这里的“其他代码”可以与被使用的程序实体在同一源码文件内，也可以在其他源码文件，甚至其他代码包中\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;demo.go&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npackage main\n\nimport (\n  &quot;flag&quot;\n)\n\nvar name string\n\nfunc init() &#123;\n  flag.StringVar(&amp;name, &quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#125;\n\nfunc main() &#123;\n  flag.Parse()\n  hello(name)&#x2F;&#x2F;变化位置\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;demo_lib.go&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npackage main\n\nimport &quot;fmt&quot;\n\nfunc hello(name string) &#123;\n  fmt.Printf(&quot;Hello, %s!\\\\n&quot;, name)\n&#125;\n什么样的程序实体才可以被当前包外的代码引用\n\n包级私有：首字母小写的程序实体（如函数名）\n公开：首字母大写的程序实体（如函数名）\n模块级私有：通过创建internal代码包（目录名为internal）让一些程序实体仅仅能被当前模块中的其他代码引用。具体规则是，internal代码包中声明的公开程序实体仅能被该代码包的直接父包及其子包中的代码引用。当然，引用前需要先导入这个internal包。对于其他代码包，导入该internal包都是非法的，无法通过编译\n\n\n\n\n补充\n\n\n2.基础语法\n程序实体\n\n关键字\n\n\n\n结构\n函数\n特殊\n\n\n\nif\nimport\ngo\n\n\nelse\npackage\nselect\n\n\nfor\nfunc\nchan\n\n\nswitch\nreturn\nrange\n\n\ncase\nstruct\nmap\n\n\nbreak\ninterface\nconst（标量）\n\n\ncontinue\ntype（别名）\ndefer\n\n\ndefault\nvar\ngoto\n\n\nfallthrough\n\n\n\n\n&#x2F;&#x2F;在声明name时，还为它赋了值，而没有显示指定name的类型，利用Go语言自身的类型推断，省去了声明\nvar name &#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\n&#x2F;&#x2F;短变量声明，类型推断加上语法糖，只能在函数体内部使用，用来声明一些临时的变量\nname :&#x3D; flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)\nconst LENGTH int &#x3D; 10\n\nfor i :&#x3D; range numbers1 &#123;\n\tif i &#x3D;&#x3D; 3 &#123;\n\t\tnumbers1[i] |&#x3D; i\n\t&#125;\n&#125;\n\ntype struct_variable_type struct &#123;\n   member definition\n   member definition\n   ...\n   member definition\n&#125;\n预定义标识符\n\n\n\n整数\n无符号整数\n小数\n其它数\n函数\n特性\n\n\n\nint\nuint\nfloat32\nuintptr\nappend\nnew\n\n\nint8\nuint8\nfloat64\nbool\ncopy\nmake\n\n\nint16\nuint16\ncomplex64\ntrue\ncap\nclose\n\n\nint32\nuint32\ncomplex128\nfalse\nlen\niota\n\n\nint64\nuint64\ncomplex\nstring\nprint\npanic\n\n\n\nbyte\nreal / imag\nnil\nprintln\nrecover\n\n\n\n内置函数\n\nfunc append(slice []Type, elems ...Type) []type：在切片变量的后面追加新的数据，然后返回新的切片变量，可用使用slice接收\n func copy(dst, src [] Type) int：把slice源的数据复制到目的切片中，目的slice填满后舍弃超出的部分\nfunc delete(m map[Type]Type1, key Type)：delete函数用于删除map中对应key的键值对，如果map中不存在该key，则什么也不做\nfunc new(Type) *Type：用来创建某一个类型的指针型对象，返回值是一个指向新分配的type类型的零值的指针\n使用new创建chan类型的指针对象，在使用之前仍然需要使用make函数来初始化chan的容量\nnew函数创建对象与直接使用struct{}创建的对象的区别是，前者为指向对象的一个指针，后者创建的是对象引用本身\n\n\nfunc make(t Type, size ...IntegerType) Type：只能用于chan，map和切片三种类型的创建，返回值就是对象本身，这三类数据结构本身就是引用类型，必须要初始化\n\n\n重声明：允许在使用短变量声明时不用理会被赋值的多个变量中是否包含旧变量\n\n由于变量的类型在其初始化时就已经确定了，所以对它再次声明时赋予的类型必须与其原本的类型相同，否则会产生编译错误\n\n变量的重声明只可能发生在某一个代码块中。如果与当前的变量重名的是外层代码块中的变量，那么就是另外一种含义了\n\n变量的重声明只有在使用短变量声明时才会发生，否则也无法通过编译。如果要在此处声明全新的变量，那么就应该使用包含关键字var的声明语句，但是这时就不能与同一个代码块中的任何变量有重名了\n\n被“声明并赋值”的变量必须是多个，并且其中至少有一个是新的变量。这时我们才可以说对其中的旧变量进行了重声明\n\n示例\nvar err error\n&#x2F;&#x2F; 这里对err进行了重声明\nn, err :&#x3D; io.WriteString(os.Stdout, &quot;Hello, everyone!\\\\n&quot;)\n\n\n类型\n\n类型断言：表达式x.(T)，x代表要判断类型的值，必须是接口类型\nvar container &#x3D; []string&#123;&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;&#125;\ncontainer :&#x3D; map[int]string&#123;0: &quot;zero&quot;, 1: &quot;one&quot;, 2: &quot;two&quot;&#125;\n\n&#x2F;&#x2F;interface&#123;&#125;(container)用来把container变量的值转换为空接口值，\n&#x2F;&#x2F;interface&#123;&#125;代表空接口，任何类型都是它的实现类型，&#123;&#125;要么是空代码块，要么表示不包含任何内容的数据结构，即空的接口类型\n&#x2F;&#x2F;用于判断前者的类型是否为切片类型[]string的.([]string)\n&#x2F;&#x2F;ok是布尔类型，代表类型判断的结果，true或false。如果是true，那么被判断的值将会被自动转换为[]string类型的值，\n&#x2F;&#x2F;并赋给变量value，否则value将被赋予nil（即“空”）。\nvalue, ok :&#x3D; interface&#123;&#125;(container).([]string)\n类型转换：语法形式是T(x)，注意事项如下：可表示范围相同就可以，可表示范围变窄时则截断高位\n\n整数值转换成string类型时，如果是无效的Unicode代码点，则结果是”�”（Unicode标准中定义的Replacement Character，用于替换未知的字符）组成的字符串\n一个值在从string类型向[]byte类型转换时代表着以 UTF-8 编码的字符串会被拆分成零散、独立的字节\n一个值在从string类型向[]rune类型转换时代表着字符串会被拆分成一个个 Unicode 字符\n\n\n别名类型：type MyString = string，byte是uint8的别名类型，而rune是int32的别名类型\n\n类型再定义：type MyString2 string，MyString2和string就是两个不同的类型了。这里的MyString2是一个新的类型，不同于其他任何类型。把string类型再定义成了另外一个类型MyString2\n\n潜在类型：某个类型在本质上是哪个类型，潜在类型相同的不同类型的值之间是可以相互进行类型转换的。但这种说法对集合类的类型却不合法\n\n\n\n\n\n容器\n\n数组&amp;切片：切片是引用类型，而数组是值类型\n\narray：长度是固定的，长度是类型的一部分，不同长度是两个不同的数组类型。数组是切片的底层结构，属于值类型，同属值类型的游基础数据结构以及结构体类型。\nslice：是可变长度的，切片的长度可以自动地随元素数量的增长而增长，但不会减小。切片是对数组的封装，可以看作是对数组的某个连续片段的引用（其他引用类型：字典、通道、函数）\n\npackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n  &#x2F;&#x2F; 用make声明两个不同切片\n  s1 :&#x3D; make([]int, 5)&#x2F;&#x2F;如果不指明其容量，那么它就会和长度一致\n  fmt.Printf(&quot;The length of s1: %d\\\\n&quot;, len(s1))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The capacity of s1: %d\\\\n&quot;, cap(s1))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The value of s1: %d\\\\n&quot;, s1)&#x2F;&#x2F;[0 0 0 0 0]\n  s2 :&#x3D; make([]int, 5, 8)&#x2F;&#x2F;切片的容量实际上代表了它的底层数组的长度，这里是8，但是只能看到5\n  fmt.Printf(&quot;The length of s2: %d\\\\n&quot;, len(s2))&#x2F;&#x2F;5\n  fmt.Printf(&quot;The capacity of s2: %d\\\\n&quot;, cap(s2))&#x2F;&#x2F;8\n  fmt.Printf(&quot;The value of s2: %d\\\\n&quot;, s2)&#x2F;&#x2F;[0 0 0 0 0]\n\t&#x2F;&#x2F;切片的容量可以看作是透过这个窗口最多可以看到的底层数组（即s3）中元素的个数，而且底层数组\n\t&#x2F;&#x2F;不变的情况下，可以向右扩展，直至底层数组的末尾\n\ts3 :&#x3D; []int&#123;1, 2, 3, 4, 5, 6, 7, 8&#125;\n\ts4 :&#x3D; s3[3:6]&#x2F;&#x2F;[3:6)\n\tfmt.Printf(&quot;The length of s4: %d\\\\n&quot;, len(s4))&#x2F;&#x2F;3\n\tfmt.Printf(&quot;The capacity of s4: %d\\\\n&quot;, cap(s4))&#x2F;&#x2F;5\n\tfmt.Printf(&quot;The value of s4: %d\\\\n&quot;, s4)&#x2F;&#x2F;[4 5 6]\n\tfmt.Println()\n\t&#x2F;&#x2F;把切片的窗口向右扩展到最大的方法\n\ts5 :&#x3D; s4[:cap(s4)]&#x2F;&#x2F;[4 5 6 7 8]\n&#125;\n链表\n\nlist包含的方法中，用于插入新元素的方法都只接受interface{ }类型的值，这些方法在内部会使用Element值，包装接受到的新元素。可以避免直接使用我们自己生成的元素，主要原因是避免链表的内部关联找到破坏\n\n开箱即用：经过语句var l list.List声明的变量l可以直接使用，因为有延迟初始化的机制（把初始化操作延后，分散初始化操作带来的计算量和存储空间的消耗），常用方法有以下几种：\n&#x2F;&#x2F;求长度\nfunc (l *List) Len() int &#123; return l.len &#125;\n&#x2F;&#x2F;移除元素\nfunc (l *List) Remove(e *Element) any &#123;&#125;\n&#x2F;&#x2F; 在最前&#x2F;后端插入新元素\nfunc (l *List) PushFront(v any) *Element &#123;&#125;\nfunc (l *List) PushBack(v any) *Element &#123;&#125;\n&#x2F;&#x2F;在元素之前&#x2F;后插入新元素\nfunc (l *List) InsertBefore(v any, mark *Element) *Element &#123;&#125;\nfunc (l *List) InsertAfter(v any, mark *Element) *Element &#123;&#125;\n&#x2F;&#x2F;将元素移到链表的最前&#x2F;后面\nfunc (l *List) MoveToFront(e *Element) &#123;&#125;\nfunc (l *List) MoveToBack(e *Element) &#123;&#125;\n&#x2F;&#x2F;将元素移到另一元素的前&#x2F;后面\nfunc (l *List) MoveBefore(e, mark *Element) &#123;&#125;\nfunc (l *List) MoveAfter(e, mark *Element) &#123;&#125;\n&#x2F;&#x2F;在链表后&#x2F;前插入另一链表的副本\nfunc (l *List) PushBackList(other *List) &#123;&#125;\nfunc (l *List) PushFrontList(other *List) &#123;&#125;\nRing和List的区别：其实List在内部就是一个循环链表。它的根元素永远不会持有任何实际的元素值，而该元素的存在就是为了连接这个循环链表的首尾两端，主要区别如下\n\n在创建并初始化一个Ring值的时候，可以指定它包含的元素的数量，但是对于一个List值来说却不能这样做（也没有必要这样做）。循环链表一旦被创建，其长度是不可变的\n仅通过var r ring.Ring语句声明的r将会是一个长度为1的循环链表，而List类型的零值则是一个长度为0的链表\nRing值的Len方法的算法复杂度是 O(N) 的，而List值的Len方法的算法复杂度则是 O(1) 的\n\n\n\n\n字典（map）\n\n键不可以是函数、字典、切片类型（需要能使用操作符==和!=），值可以是任何类型。map中不存储键的值，而是使用其hash值代表\n如果键的类型是接口类型的，那么键值的实际类型也不能是上述三种类型，否则在程序运行过程中会引发 panic\n如果键的类型是数组类型，那么还要确保该类型的元素类型不是函数类型、字典类型或切片类型\n如果键的类型是结构体类型，那么还要保证其中字段的类型的合法性\n\n\n注意事项\n优先考虑哪些类型作为字典的键类型：求哈希和判等操作的速度越快，越适合，优先使用数值和指针类型（因为求hash值的速度与类型的宽度成正比）\n在值为nil的字典上执行读/写操作：由于字典是引用类型，所以当仅声明而不初始化一个字典类型的变量的时候，它的值会是nil，除了对nil的字典添加键值对操作外，其它操作都不会跑出panic\n\n\n\n\n\n\n程序结构\n\n函数\n\n定义方式：func function_name( [parameter list] ) [return_types] &#123;...&#125;\n\n参数是数组并且在函数内更改了参数：原数组不会改变，所有传给函数的参数值都会被复制，函数在其内部使用的并不是参数值的原值，而是他的副本（数值）\n\n参数是切片并且在函数内更改了参数：原切片会改变，对于切片、字典、通道，只会拷贝他们本身，并不会拷贝他们引用的底层数据（引用）\n\n\nfunc modifyComplexArray(a [3][]string) [3][]string &#123;\n\ta[1][1] &#x3D; &quot;s&quot;&#x2F;&#x2F;a和s都改变，因为是引用类型的内部\n\ta[2] &#x3D; []string&#123;&quot;o&quot;, &quot;p&quot;, &quot;q&quot;&#125;&#x2F;&#x2F;只会改变a，s不变，因为改变的是数组\n\treturn a\n&#125;\n高阶函数：传入参数或返回参数\ntype operate func(x,y int) int\nfunc calculate(x int ,y int, op operate)(int ,error)&#123;\n  if op &#x3D;&#x3D; nil&#123;&#x2F;&#x2F;卫述语句：检查先决条件的合法性，如果未通过立即终止当前代码执行的语句\n    return 0,errors.New(&quot;invalid operation&quot;)\n  &#125;\n  return op(x, y), nil\n&#125;\n\nfunc main() &#123;\n  x, y &#x3D; 56, 78\n  op :&#x3D; func(x, y int) int&#123;\n    return x + y\n  &#125;\n  result, err &#x3D; calculate(x, y, op)\n&#125;\n闭包：内部逻辑并不完整，有一部分逻辑需要外来标识符参与完成，而此标识在函数定义时是未知的。表面上是延迟实现部分逻辑，实际上是在动态地生成那部分程序逻辑，类似于模版方法\n&#x2F;&#x2F;genCalculator就是高阶函数\nfunc genCalculator(op operate) calculateFunc &#123;\n  &#x2F;&#x2F;匿名的calculateFunc类型的闭包函数，内部需要op来实现，在调用时才知道是什么\n  return func(x int, y int) (int, error) &#123;\n    if op &#x3D;&#x3D; nil &#123;\n      return 0, errors.New(&quot;invalid operation&quot;)\n    &#125;\n    return op(x, y), nil\n  &#125;\n&#125;\n\n\n结构体\n&#x2F;&#x2F; AnimalCategory 代表动物分类学中的基本分类法。\ntype AnimalCategory struct &#123;\n  kingdom string &#x2F;&#x2F; 界。\n  phylum string &#x2F;&#x2F; 门。\n  class  string &#x2F;&#x2F; 纲。\n  order  string &#x2F;&#x2F; 目。\n  family string &#x2F;&#x2F; 科。\n  genus  string &#x2F;&#x2F; 属。\n  species string &#x2F;&#x2F; 种。\n&#125;\n&#x2F;&#x2F;嵌入字段\ntype Animal struct &#123;\n  scientificName string &#x2F;&#x2F; 学名。\n  AnimalCategory    &#x2F;&#x2F; 动物基本分类。\n&#125;\n&#x2F;&#x2F;方法名为String，接受者声明为AnimalCategory类型的ac，以在其中引用到当前值的任何一个字段，或者调用\n&#x2F;&#x2F;到当前值的任何一个方法（也包括String方法自己）,相当于java的toString\nfunc (ac AnimalCategory) String() string &#123;\n  return fmt.Sprintf(&quot;%s%s%s%s%s%s%s&quot;,\n    ac.kingdom, ac.phylum, ac.class, ac.order,\n    ac.family, ac.genus, ac.species)\n&#125;\nfunc (a Animal) Category() string &#123;\n  return a.AnimalCategory.String()\n&#125;\n接口\n\n无侵入式的接口实现方法：只要实现接口中的所有方法（方法签名一致+方法名一样）就一定是这个接口的实现类型（Duck typing）\n为接口变量赋值：当接口被赋值时，接口会获得动态值和动态类型（原来只具有静态类型），一起被存储在一个专用的数据结构中，叫做iface。iface实例会包含两个指针，一个是指向类型信息的指针，另一个是指向动态值的指针。这里的类型信息是由另一个专用数据结构的实例承载的，其中包含了动态值的类型，以及使它实现了接口的方法和调用它们的途径\n接口之间的组合：同名的方法会产生冲突，无法通过编译，推荐使用体量较小的接口\n\npackage main\n\nimport (\n    &quot;fmt&quot;\n)\n\ntype Phone interface &#123;\n    call()\n&#125;\n\ntype NokiaPhone struct &#123;\n&#125;\n\nfunc (nokiaPhone NokiaPhone) call() &#123;\n    fmt.Println(&quot;I am Nokia, I can call you!&quot;)\n&#125;\n\ntype IPhone struct &#123;\n&#125;\n\nfunc (iPhone IPhone) call() &#123;\n    fmt.Println(&quot;I am iPhone, I can call you!&quot;)\n&#125;\n\nfunc main() &#123;\n    var phone Phone\n\n    phone &#x3D; new(NokiaPhone)\n    phone.call()\n\n    phone &#x3D; new(IPhone)\n    phone.call()\n\n&#125;\n\n\n\n3.进阶语法\n多线程（go、chan、select）\n\nchan\n\nGo语言自带的唯一可以满足并发安全性的类型，示例如下\nfunc main() &#123;\n  &#x2F;&#x2F;chan 代表通道类型的关键字，int代表该通道类型的元素类型，3为通道容量，即通道可以缓存多少了元素值\n  &#x2F;&#x2F;通道长度为0时，称为非缓冲通道，也就是不带缓冲的通道。通道相当于先进先出的队列\n  ch1 :&#x3D; make(chan int, 3)\n  ch1 &lt;- 2&#x2F;&#x2F;接受和发送都需要&lt;-，形象的表示了元素值的传输方向\n  ch1 &lt;- 1\n  ch1 &lt;- 3\n  elem1 :&#x3D; &lt;-ch1\n  fmt.Printf(&quot;The first element received from channel ch1: %v\\\\n&quot;,elem1)&#x2F;&#x2F;2\n&#125;\n通道的特点\n\n对于同一个通道，发送操作之间是互斥的，接收操作之间也是互斥的\n发送操作和接收操作中对元素值的处理都是不可分割的，并且进通道和出通道的都是副本\n发送操作在完全完成之前会被阻塞。接收操作也是如此\n\n\n注意事项\n\n针对非缓冲通道，无论是发送还是接受操作，一开始执行就会被阻塞，直到配对的操作也开始执行，才会继续传递。由此可见，非缓冲通道是在用同步的方式传递数据。也就是说，只有收发双方对接上了，数据才会被传递\n对于值为nil的通道，不论它的具体类型是什么，对它的发送操作和接收操作都会永久地处于阻塞状态。由于通道类型是引用类型，所以它的零值就是nil，所以不要忘记初始化通道\n对于一个已初始化，但并未关闭的通道来说，收发操作一定不会引发 panic。但是通道一旦关闭，再对它进行发送操作，就会引发 panic。另外，如果我们试图关闭一个已经关闭了的通道，也会引发 panic\n当我们把接收表达式的结果同时赋给两个变量时，第二个变量的类型就是一定bool类型。它的值如果为false就说明通道已经关闭，并且再没有元素值可取了。如果通道关闭时，里面还有元素值未被取出，那么接收表达式的第一个结果，仍会是通道中的某一个元素值，而第二个结果值一定会是true。因此，通过接收表达式的第二个结果值，来判断通道是否关闭是可能有延时的\n\n\n单向通道：只能发不能收，或者只能收不能发的通道，var uselessChan = make(chan&lt;- int, 1)，紧挨在关键字chan右边的那个&lt;-，这表示了这个通道是单向的，并且只能发而不能收；类似的，如果这个操作符紧挨在chan的左边，那么就说明该通道只能收不能发，应用主要是约束代码行为（一般用在接口类型声明中的方法定义上，可以传入双向通道，会自动转换为单向）\n\n\n\nselect：只能与通道联用，一般由若干个分支组成，每次执行只有一个分支的代码会被执行。分支分为两种，一种叫做候选分支，另一种叫做默认分支。候选分支总是以关键字case开头，后跟一个case表达式和一个冒号，然后我们可以从下一行开始写入当分支被选中时需要执行的语句；默认分支其实就是 default case，因为，当且仅当没有候选分支被选中时它才会被执行，所以它以关键字default开头并直接后跟一个冒号\n&#x2F;&#x2F; 准备好几个通道\nintChannels :&#x3D; [3]chan int&#123;\n  make(chan int, 1),\n  make(chan int, 1),\n  make(chan int, 1),\n&#125;\n&#x2F;&#x2F; 随机选择一个通道，并向它发送元素值\nindex :&#x3D; rand.Intn(3)\nfmt.Printf(&quot;The index: %d\\\\n&quot;, index)\nintChannels[index] &lt;- index\n&#x2F;&#x2F; 哪一个通道中有可取的元素值，哪个对应的分支就会被执行。仅当select语句中的所有case表达式都被求值完毕(从上到下)后，它才会开始选择候选分支\nselect &#123;\ncase &lt;-intChannels[0]:\n  fmt.Println(&quot;The first candidate case is selected.&quot;)\ncase &lt;-intChannels[1]:\n  fmt.Println(&quot;The second candidate case is selected.&quot;)\ncase elem :&#x3D; &lt;-intChannels[2]:\n  fmt.Printf(&quot;The third candidate case is selected, the element is %d.\\\\n&quot;, elem)\ndefault:\n  fmt.Println(&quot;No candidate case is selected!&quot;)\n&#125;\n\n&#x2F;&#x2F;通过接收表达式的第二个结果值来判断通道是否已经关闭\nintChan :&#x3D; make(chan int, 1)\n&#x2F;&#x2F; 一秒后关闭通道。\ntime.AfterFunc(time.Second, func() &#123;\n  close(intChan)\n&#125;)\nselect &#123;\ncase _, ok :&#x3D; &lt;-intChan:\n  if !ok &#123;\n    fmt.Println(&quot;The candidate case is closed.&quot;)\n    break\n  &#125;\n  fmt.Println(&quot;The candidate case is selected.&quot;)\n&#125;\ngo\n\n并发模型：（G（goroutine 的缩写）、P（processor 的缩写）和 M（machine 的缩写））\n\n\nGo语句执行时，Go语言运行时系统会先试图从存放空闲G的队列中获取一个G（即goroutine），如果没有才会创建一个新的，然后包装将要执行的代码，并且加入到待执行的队列中，虽然很快就能执行，但是依旧会耗时。所以在go语言本身执行完毕后，Go程序完全不会等待go函数的执行，而是立刻执行后边的语句，即异步并发地执行。在执行完主goroutine的所有go语句后，主goroutine就会结束运行\npackage main\n\nimport &quot;fmt&quot;\n\nfunc main() &#123;\n  for i :&#x3D; 0; i &lt; 10; i++ &#123;\n    go func() &#123;\n      fmt.Println(i)&#x2F;&#x2F;不会打印任何东西\n    &#125;()\n  &#125;\n&#125;\n&#x2F;&#x2F;因为主goroutine先结束，而其他goroutine内部的语句还没开始执行\n让主goroutine等待其他goroutine：让主goroutine执行time.Sleep(time.Millisecond * 500) 或 利用通道\nfunc main() &#123;\n\tnum :&#x3D; 10\n  &#x2F;&#x2F;struct&#123;&#125;类似于空接口类型interface&#123;&#125;，代表既不包含任何字段也不拥有任何方法的空结构体类型\n  &#x2F;&#x2F;struct&#123;&#125;类型值的表示法只有一个，即struct&#123;&#125;&#123;&#125;，占用0字节内存空间，全局只有一份\n\tsign :&#x3D; make(chan struct&#123;&#125;, num)\n\n\tfor i :&#x3D; 0; i &lt; num; i++ &#123;\n\t\tgo func() &#123;\n\t\t\tfmt.Println(i)\n\t\t\tsign &lt;- struct&#123;&#125;&#123;&#125;\n\t\t&#125;()\n\t&#125;\n\tfor j :&#x3D; 0; j &lt; num; j++ &#123;\n\t\t&lt;-sign &#x2F;&#x2F;go语句全执行完，主goroutine才会全部不阻塞，顺利执行完\n\t&#125;\n&#125;\n让多个goroutine按既定顺序执行\nfunc main() &#123;\n  var count uint32\n  &#x2F;&#x2F;trigger函数会不断地获取一个名叫count的变量的值，并判断该值是否与参数i的值相同。如果相同，那么就\n  &#x2F;&#x2F;立即调用fn代表的函数，然后把count变量的值加1，最后显式地退出当前的循环。否则，我们就先让当前的\n  &#x2F;&#x2F;goroutine“睡眠”一个纳秒再进入下一个迭代。\n  trigger :&#x3D; func(i uint32, fn func()) &#123;\n    for &#123;\n      &#x2F;&#x2F;原子操作，count是一个信号，值总是下一个可以调用打印函数的go函数的序号\n      if n :&#x3D; atomic.LoadUint32(&amp;count); n &#x3D;&#x3D; i &#123;\n        fn()\n        atomic.AddUint32(&amp;count, 1)\n        break\n      &#125;\n      time.Sleep(time.Nanosecond)\n    &#125;\n  &#125;\n  for i :&#x3D; uint32(0); i &lt; 10; i++ &#123;\n    go func(i uint32) &#123;\n      fn :&#x3D; func() &#123;\n        fmt.Println(i)\n      &#125;\n      trigger(i, fn)\n    &#125;(i)\n  &#125;\n  &#x2F;&#x2F;让主 goroutine 最后一个运行完毕。当手动启用的 goroutine 都运行完毕之后，count的值一定会是10\n  &#x2F;&#x2F;所以把10作为了第一个参数值。又由于并不想打印这个10，所以把一个什么都不做的函数作为了第二个参数值\n  trigger(10, func() &#123;&#125;)\n&#125;\n\n\n\n\n错误处理（defer）\n\nerror类型是一个接口类型，也是一个Go语言的内建类型。在这个接口类型的声明中只包含一个方法Error，不接受任何参数，但会返回一个string类型的结果，作用是返回错误信息的字符串表示形式，相当于其他类型值的String方法。示例如下\npackage main\n\nimport (\n  &quot;errors&quot;\n  &quot;fmt&quot;\n)\n&#x2F;&#x2F;用在结果列表的最后，声明一个error类型的结果\nfunc echo(request string) (response string, err error) &#123;\n  if request &#x3D;&#x3D; &quot;&quot; &#123;\n    &#x2F;&#x2F;为err赋值，返回错误信息。err的静态类型是error，动态类型是errors包中的*errorString\n    err &#x3D; errors.New(&quot;empty request&quot;)\n    return\n  &#125;\n  response &#x3D; fmt.Sprintf(&quot;echo: %s&quot;, request)\n  return\n&#125;\n\nfunc main() &#123;\n  for _, req :&#x3D; range []string&#123;&quot;&quot;, &quot;hello!&quot;&#125; &#123;\n    fmt.Printf(&quot;request: %s\\\\n&quot;, req)\n    resp, err :&#x3D; echo(req)\n    if err !&#x3D; nil &#123;\n      &#x2F;&#x2F;fmt.Printf函数如果发现被打印的值是一个error类型的值，那么就会去调用它的Error方法\n      fmt.Printf(&quot;error: %s\\\\n&quot;, err)\n      continue\n    &#125;\n    fmt.Printf(&quot;response: %s\\\\n&quot;, resp)\n  &#125;\n&#125;\n\n\n对于类型在已知范围内的一系列错误值，一般使用类型断言表达式或类型switch语句来判断；\n&#x2F;&#x2F;os包中的几个代表错误的类型os.PathError、os.LinkError、os.SyscallError和os&#x2F;exec.Error\nfunc underlyingError(err error) error &#123;\n  switch err :&#x3D; err.(type) &#123;\n  case *os.PathError:\n    return err.Err\n  case *os.LinkError:\n    return err.Err\n  case *os.SyscallError:\n    return err.Err\n  case *exec.Error:\n    return err.Err\n  &#125;\n  return err\n&#125;\n对于已有相应变量且类型相同的一系列错误值，一般直接使用判等操作来判断；\nprintError :&#x3D; func(i int, err error) &#123;\n  if err &#x3D;&#x3D; nil &#123;\n    fmt.Println(&quot;nil error&quot;)\n    return\n  &#125;\n  err &#x3D; underlyingError(err)&#x2F;&#x2F;得到潜在错误值\n  switch err &#123;\n  case os.ErrClosed:\n    fmt.Printf(&quot;error(closed)[%d]: %s\\\\n&quot;, i, err)\n  case os.ErrInvalid:\n    fmt.Printf(&quot;error(invalid)[%d]: %s\\\\n&quot;, i, err)\n  case os.ErrPermission:\n    fmt.Printf(&quot;error(permission)[%d]: %s\\\\n&quot;, i, err)\n  &#125;\n&#125;\n对于没有相应变量且类型未知的一系列错误值，只能使用其错误信息的字符串表示形式来做判断。\n\n\n\npanic（运行时恐慌）：是一种程序异常，抛出panic时如果程序没有保护措施，就会打印出panic的详细信息，然后终止运行\npanic: runtime error: index out of range#运行时异常，panic包含一个runtime.Error接口类型的值\n\ngoroutine 1 [running]:#表示有一个ID为1的goroutine在此panic被引发的时候正在运行\nmain.main()#表明了这个goroutine包装的go函数就是命令源码文件中的那个main函数，即此为主goroutine\n &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q0&#x2F;demo47.go:5 +0x3d\nexit status 2#表明这个程序是以退出状态码2结束运行的\n\n\n从panic被引发到程序终止运行的大致过程：某行代码引发panic，从那行代码开始根据调用层级，反向依次终止函数，一直到最外层函数，即go函数/main函数，然后控制权被运行时系统收回，程序崩溃并终止运行，承载程序这次运行的进程也会随之消亡。在这个传播过程中，panic详情会逐步完善，最终打印\n怎么让panic包含一个值：通过内建函数panic，可以在程序运行期间报告异常，直接通过参数传入即可。如果某个值有可能会被记到日志里，那么就应该为它关联String方法。\n\n\nrecover：施加应对panic的保护措施，Go 语言的内建函数recover专用于恢复 panic，或者说平息运行时恐慌。recover函数无需任何参数，并且会返回一个空接口类型的值。defer语句就是被用来延迟执行代码的，延迟到该语句所在的函数即将执行结束的那一刻，无论结束执行的原因是什么。\npackage main\n\nimport (\n &quot;fmt&quot;\n &quot;errors&quot;\n)\n\nfunc main() &#123;\n fmt.Println(&quot;Enter function main.&quot;)\n defer func()&#123;&#x2F;&#x2F;类似于go语句的写法\n  fmt.Println(&quot;Enter defer function.&quot;)\n  if p :&#x3D; recover(); p !&#x3D; nil &#123;&#x2F;&#x2F;recover会返回空接口类型的结果值，如果没有panic，则值是nil\n   fmt.Printf(&quot;panic: %s\\\\n&quot;, p)\n  &#125;\n  fmt.Println(&quot;Exit defer function.&quot;)\n &#125;()\n &#x2F;&#x2F; 引发panic。\n panic(errors.New(&quot;something wrong&quot;))\n fmt.Println(&quot;Exit function main.&quot;)\n&#125;\ndefer：defer用于资源的释放，会在函数返回之前进行调用。一般采用如下模式\nf,err :&#x3D; os.Open(filename)\nif err !&#x3D; nil &#123;\n    panic(err)\n&#125;\ndefer f.Close()\n\n\n多条defer语句的执行顺序：在同一个函数中，defer函数调用的执行顺序与它们分别所属的defer语句的出现顺序（更严谨地说，是执行顺序）完全相反。当一个函数即将结束执行时，其中的写在最下边的defer函数调用会最先执行，其次是写在它上边、与它的距离最近的那个defer函数调用，以此类推，最上边的defer函数调用会最后一个执行。因为defer语句每次执行的时候，Go 语言会把它携带的defer函数及其参数值另行存储到一个链表中，这个链表是先进后出的，相当于栈\n\n\n\n\n库函数\n\nstrings\n\nGo使用Unicode编码规范中的UTF-8编码格式，一个string类型的值是由一系列对应的UTF-8编码值来表达。一个string类型的值可以被拆分成一个包含多个字符的序列，也可以被拆分为一个包含多个字节的序列。前者使用以rune为元素类型的切片来表示，后者则可以用一个以byte为元素类型的切片来表示。rune是Go特有的一个基本数据类型，一个值就代表一个字符（Unicode字符），它是一个int32类型的一个别名类型\nstr :&#x3D; &quot;Go爱好者&quot;\nfmt.Printf(&quot;The string: %q\\\\n&quot;, str)\n&#x2F;&#x2F;&#x3D;&gt; runes(char): [&#39;G&#39; &#39;o&#39; &#39;爱&#39; &#39;好&#39; &#39;者&#39;]\nfmt.Printf(&quot;  &#x3D;&gt; runes(char): %q\\\\n&quot;, []rune(str))\n&#x2F;&#x2F;&#x3D;&gt; runes(hex): [47 6f 7231 597d 8005]\nfmt.Printf(&quot;  &#x3D;&gt; runes(hex): %x\\\\n&quot;, []rune(str))\n&#x2F;&#x2F;&#x3D;&gt; bytes(hex): [47 6f e7 88 b1 e5 a5 bd e8 80 85]\nfmt.Printf(&quot;  &#x3D;&gt; bytes(hex): [% x]\\\\n&quot;, []byte(str))\n使用带有range子句的for语句遍历字符串：带有range子句的for语句会先把被遍历的字符串值拆成一个字节序列，然后再试图找出这个字节序列中包含的每一个 UTF-8 编码值，或者说每一个 Unicode 字符\nstr :&#x3D; &quot;Go爱好者&quot;\nfor i, c :&#x3D; range str &#123;\n fmt.Printf(&quot;%d: %q [% x]\\\\n&quot;, i, c, []byte(string(c)))\n&#125;\n&#x2F;&#x2F;0: &#39;G&#39; [47]\n&#x2F;&#x2F;1: &#39;o&#39; [6f]\n&#x2F;&#x2F;2: &#39;爱&#39; [e7 88 b1]\n&#x2F;&#x2F;5: &#39;好&#39; [e5 a5 bd]\n&#x2F;&#x2F;8: &#39;者&#39; [e8 80 85]\nstrings包中有strings.Builder类型的WriteRune方法、strings,Reader类型的ReadRune方法\n\n虽然string值能通过切片操作来裁剪或者通过操作符+来拼接，但是都需要拷贝到新的内存里，但是strings.Builder类型的值有以下优势：\n已存在的内容不可变，但可以拼接更多的内容：通过一个byte为元素的类型的切片来存储内容，通过一个unsafe.Pointer类型的字段来指向持有那个指向了底层字节数组的指针值。虽然可以进行任何操作，但是要求只能被拼接或完全覆盖\n减少了内存分配和内容拷贝的次数：容量不够或者调用Grow方法的时候才会扩容并拷贝数据\n可将内容重置，可重用值：通过Reset方法\n\n\nstrings.Builder类型在已被真正使用后就不可再被复制，否则会引发panic，但是可以复制指针值；并且由于其内容不是完全不可变的，所以需要使用方自行解决操作冲突和并发安全问题\nstrings.Reader类型的值可以高效地读取字符串，因为在读取过程中，Reader值会保存已读取的字节的计数，代表着下一次读取的起始位置，所以很容易计算出下一次读取的起始索引位置\n\n\n\n\nbytes\n\nstrings包和bytes包很多API是相似的，提供的函数的数量和功能也差别不大。主要区别是strings包主要面向Unicode字符和经过UTF-8编码的字符串，而bytes包面对的则主要是字节和字节切片，主要用途是作为字节序列的缓冲区\nbytes.Buffer的扩容策略：\n对于处在零值状态的Buffer值来说，如果第一次扩容时的另需字节数不大于64，那么该值就会基于一个预先定义好的、长度为64的字节数组来创建内容容器。在这种情况下，这个内容容器的容量就是64。这样做的目的是为了让Buffer值在刚被真正使用的时候就可以快速地做好准备\n如果可以（内容容器容量与其长度之差大于或等于需要的字节数）则会在当前的内容容器之上，进行长度扩容，即通过切片操作对原有的内容容器的长度进行扩容\n如果内容容器剩余容量不够，那么就会用新的内容容器去替代原有的内容容器，进行扩容。如果当前内容容器的容量的一半，仍然大于或等于其现有长度（即未读字节数）再加上另需的字节数的和，那么，扩容代码就会复用现有的内容容器，并把容器中的未读内容拷贝到它的头部位置。否则就创建一个新的内容容器，新容器的容量等于原有容量的二倍再加上另需字节数的和\n\n\n在bytes.Buffer中，Bytes方法和Next方法都可能会造成内容的泄露。原因在于，它们都把基于内容容器的切片直接返回给了方法的调用方，而且通过切片可以直接访问和操纵它的底层数组\n\n\nio\n\nstrings.Builder、strings.Reader和bytes.Buffer都分别实现了很多io包中的接口，io包中接口的优势是可以提供不同程序实体之间的互操作性\n\nio包中的接口及其关系\n\n核心接口：io.Reader、io.Writer、io.Closer\n\nio包中的简单接口共有 11 个。其中，读取操作相关的接口有 5 个，写入操作相关的接口有 4 个，而与关闭操作有关的接口只有 1 个，另外还有一个读写位置设定相关的接口。此外，io包还包含了 9 个基于这些简单接口的扩展接口\n\n\n\n\n\n\nbufio\n\nbufio是buffed I/O的缩写，即实现的I/O操作都内置了缓冲区，主要的数据类型有Reader、Scanner、Writer、ReadWriter\nbufio.Reader类型值中的缓冲区的作用：是一个数据存储中介，介于底层读取器（初始化时传入的io.Reader）与读取方法及其调用方之间。Reader值的读取方法一般都会先从其所属值的缓冲区中读取数据。同时，在必要的时候，它们还会预先从底层读取器那里读出一部分数据，并暂存于缓冲区之中以备后用。可以降低读取方法的执行时间。（fill函数）\nbufio.Reader类型读取方法有哪些不同\nPeek：读取并返回其缓冲区中的n个未读字节，并且它会从已读计数代表的索引位置开始读。即使它读取了缓冲区中的数据，也不会更改已读计数的值。\nRead：有时会把缓冲区中的未读字节，依次拷贝到其参数p代表的字节切片中，并立即根据实际拷贝的字节数增加已读计数的值\n在缓冲区中还有未读字节的情况下，该方法的做法就是如此。不过，在另一些时候，其所属值的已读计数会等于已写计数，这表明：此时的缓冲区中已经没有任何未读的字节了。\n当缓冲区中已无未读字节时，Read方法会先检查参数p的长度是否大于或等于缓冲区的长度。如果是，那么Read方法会索性放弃向缓冲区中填充数据，转而直接从其底层读取器中读出数据并拷贝到p中。这意味着它完全跨过了缓冲区，并直连了数据供需的双方。\n\n\nReadSlice：先在其缓冲区的未读部分中寻找分隔符。如果未能找到，并且缓冲区未满，那么该方法会先通过调用fill方法对缓冲区进行填充，然后再次寻找，如此往复\nReadBytes：会通过调用ReadSlice方法一次又一次地从缓冲区中读取数据，直至找到分隔符为止。在这个过程中，ReadSlice方法可能会因缓冲区已满而返回所有已读到的字节和相应的错误值，但ReadBytes方法总是会忽略掉这样的错误，并再次调用ReadSlice方法，这使得后者会继续填充缓冲区并在其中寻找分隔符。\n\n\n\n\nos\n\nos包中的API可以帮助我们使用操作系统中的文件系统、权限系统、环境变量、系统进程、系统信号\n\n\n\n\nnet\n\n网络编程底层以来socket系统调用，是一种IPC （Inter-Process Communication）方法。在syscall代码包中有一个与这个socket系统调用对应的函数，两者的函数签名基本一致，都会接受三个int类型的参数（通信域、类型、使用的协议），并返回一个可以代表文件描述符的结果\n\n\n在调用net.Dial函数的时候，会为它的两个参数设定值。其中的第一个参数名为network，它决定 Go 程序在底层会创建什么样的 socket 实例，并使用什么样的协议与其他程序通信，第二个参数是address。参数network有以下可选值：tcp, tcp4, tcp6, udp, udp4, udp6, unix, unixgram, unixpacket\n\nnet/http代码包\n\n使用：只需要传给它一个URL就可以，http.Get函数会返回两个结果值，第一个结果值的类型是*http.Response，它是网络服务给我们传回来的响应内容的结构化表示。第二个结果值是error类型的，它代表了在创建和发送HTTP 请求，以及接收和解析 HTTP 响应的过程中可能发生的错误。http.Get函数会在内部使用缺省的 HTTP 客户端，并且调用它的Get方法以完成功能。这个缺省的 HTTP 客户端是由net/http包中的公开变量DefaultClient代表的，其类型是*http.Client\nurl1 :&#x3D; &quot;&lt;http:&#x2F;&#x2F;google.cn&gt;&quot;\nfmt.Printf(&quot;Send request to %q with method GET ...\\\\n&quot;, url1)\nresp1, err :&#x3D; http.Get(url1)\nif err !&#x3D; nil &#123;\n  fmt.Printf(&quot;request sending error: %v\\\\n&quot;, err)\n&#125;\ndefer resp1.Body.Close()\nline1 :&#x3D; resp1.Proto + &quot; &quot; + resp1.Status\nfmt.Printf(&quot;The first line of response:\\\\n%s\\\\n&quot;, line1)\nhttp.Client类型中的Transport字段：\n\n向网络服务发送 HTTP 请求，并从网络服务接收 HTTP 响应的操作过程。也就是说，该字段的方法RoundTrip应该实现单次 HTTP 事务（或者说基于 HTTP 协议的单次交互）需要的所有步骤\n这个字段是http.RoundTripper接口类型的，它有一个由http.DefaultTransport变量代表的缺省值（以下简称DefaultTransport）。当我们在初始化一个http.Client类型的值（以下简称Client值）的时候，如果没有显式地为该字段赋值，那么这个Client值就会直接使用DefaultTransport\nhttp.Client类型的Timeout字段，代表的正是前面所说的单次 HTTP 事务的超时时间，它是time.Duration类型的。它的零值是可用的，用于表示没有设置超时时间。\n\n\nhttp.Server类型的ListenAndServe方法：http.Server类型与http.Client是相对应的。http.Server代表的是基于 HTTP 协议的服务端，或者说网络服务。http.Server类型的ListenAndServe方法的功能是：监听一个基于 TCP 协议的网络地址，并对接收到的 HTTP 请求进行处理。这个方法会默认开启针对网络连接的存活探测机制，以保证连接是持久的。同时，该方法会一直执行，直到有严重的错误发生或者被外界关掉。当被外界关掉时，它会返回一个由http.ErrServerClosed变量代表的错误值。\n\nnet.Listen函数都做了哪些事情：解析参数值中包含的网络地址隐含的 IP 地址和端口号；根据给定的网络协议，确定监听的方法，并开始进行监听。\nhttp.Server类型的Serve方法是怎样接受和处理 HTTP 请求的：在一个for循环中，网络监听器的Accept方法会被不断地调用，该方法会返回两个结果值；第一个结果值是net.Conn类型的，它会代表包含了新到来的 HTTP 请求的网络连接；第二个结果值是代表了可能发生的错误的error类型值。\n\n\n\n\n\n\n\n\n\n","slug":"Go","date":"2023-05-04T04:42:41.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"838ae74e3a76757d637de803a615bfd9","title":"MySQL","content":"MySQL1.使用1.SQL语法\n数据库概念：数据库（DB）、数据库管理系统（DBMS）、数据库系统（软件+数据库+DBA）、数据库管理员（DBA）、元祖（tuple 一行）、码（列）、候选码（唯一标识元祖）、主码（主键）、外码（另一表的主键）、主属性（候选码中的属性）、非主属性、注释（##，–，/* */）、SQL语句不区分大小写（MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写）\n\n表设计\n\nE-R图（Entity Relationship Diagram 实体+属性+联系「1:1, 1:N, M:N」）\n范式\n1NF：强调列的原子性，列不可再分\n2NF：1NF基础上，表必须有一个主键+非主键列不能部分依赖主键\n3NF：2NF基础上，非主键列必须不能传递依赖主键\nBCNF：关系模式中每一个决定因素都包含候选键，只要A能决定B，A内部就必须有主键列\n\n\n\n\n常见数据类型\n\n整数类型：（TINYINT(1)、SMALLINT(2)、MEDIUMINT(3)、INT(4)、BIGINT(8)）\n小数类型：浮点数（FLOAT、DOUBLE）、定点数（DECIMAL、NUMERIC）\n字符串类型：CHAR、VARCHAR、BLOB、TEXT\nVARCHAR：可变长度最大为65535、存储附加元信息、超出长度返回警告（CHAR直接截断）\n大部分情况都使用VARCHAR，但是其修改会导致页分裂、页空隙等问题\n\n\nInnoDB会将长度超过768字节的定长字段存储为变长字段，可以跨页存储。例如：CHAR(255)在utf8mb4字符集（字符编码可能超过3字节）下可能会被存储成变长字段\nCHAR会截断尾空格，VARCHAR不会，插入没有尾空格的数据时，使用=查找时有没有尾空格都可以查出数据（自动补空格），但是用like查找时查不出没有空格的\n\n\n日期类型：DATE（YYYY-MM-DD）、TIME（hh:mm:ss[.fraction]）、DATETIME（YYYY-MM-DD hh:mm:ss[.fraction]）、TIMESTAMP（从1970年开始的秒数）、YEAR（YYYY）\n不要用字符串存储日期：占用空间大、查询慢（逐个字符进行比对）、无法用日期相关的函数\n数值型时间戳：这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便；缺点是可读性太差，无法直观的看到具体时间\nDatetime和 Timestamp是 MySQL 提供的两种比较相似的保存时间的数据类型，通常会首选Timestamp\nDateTime 类型没有时区信息，导致服务器更换地址的时候，数据库读出的时间有错误\nTimestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，在不同时区，查询同一条记录值会不一样\nTimestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小（1970年到2037年）\n\n\n\n\n\n\n基础SQL语句\n\n数据定义语言（DDL）：CREATE、ALTER、DROP、USE、ADD\n操作对象：视图（VIEW）、表（TABLE）、索引（INDEX）\n修饰约束：NOT NULL、UNIQUE、PRIMARY KEY、FOREIGN KEY、CHECK、DEFULT、KEY\n\n\n数据操纵语言（DML）：INSERT、UPDATE、DELETE、SELECT\n约束：DISTINCT返回不同值、LIMIT限制返回行数、ORDER BY排序（ASC升序、DESC降序）\n子查询：子查询可以嵌入 SELECT、INSERT、UPDATE和 DELETE语句中（需要放入()中），也可以和 =、&lt;、&gt;、&lt;&gt;、&gt;=、&lt;=、IN、BETWEEN、EXISTS、LIKE（%或_）、AND、OR、NOT等运算符一起使用\n分组：group by、聚合（count，max，sum，avg忽律null行）、having用于对汇总的 group by结果进行过滤\n连接：join…on…、join…using…（列名相同）、join默认是inner join（还有left join、right join、full join、self join需命名一个表、cross join笛卡尔积）\nstraight_join 让 MySQL 使用固定的连接方式执行查询\n\n\n组合：UNION运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自UNION中参与查询的提取行\n\n\n事务控制语言：COMMIT、ROLLBACK\n不能回退 SELECT语句，回退 SELECT语句也没意义；也不能回退 CREATE和 DROP语句；默认每一条语句都当成一个事务进行提交\n当出现 START TRANSACTION语句时，会关闭隐式提交；当 COMMIT或 ROLLBACK语句执行后，事务会自动关闭，重新恢复隐式提交\n通过 set autocommit=0可以取消自动提交，直到 set autocommit=1才会提交；autocommit标记是针对每个连接而不是针对服务器\n\n\n\n\n进阶SQL语句\n\nshow processlist\n\ncommand（查看连接状态）：sleep（空闲连接）、\nstate：\n\nmysql&gt; show processlist;\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\n| Id | User            | Host      | db   | Command | Time   | State                  | Info        |\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\n|  5 | event_scheduler | localhost | NULL | Daemon  | 610663 | Waiting on empty queue | NULL         |\n+----+-----------------+-----------+------+---------+--------+------------------------+--------------+\ndelimiter\n\nexplain：并不会真的执行语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息\n\ntype表的访问方法\n\nsystem：这种类型要求数据库表中只有一条数据，是const类型的一个特例，一般情况下是不会出现的\nconst：通过一次索引就能找到数据，一般用于主键或唯一索引作为条件，这类扫描效率极高，速度非常快\neq_ref：常用于主键或唯一索引扫描，一般指使用主键的关联查询\nref : 常用于非主键和唯一索引扫描\nref_or_null：这种连接类型类似于ref，区别在于MySQL会额外搜索包含NULL值的行\nindex_merge：使用了索引合并优化方法，查询使用了两个以上的索引\nunique_subquery：类似于eq_ref，条件用了in子查询\nindex_subquery：区别于unique_subquery，用于非唯一索引，可以返回重复值\nrange：常用于范围查询，比如：between … and 或 In 等操作\nindex：全索引扫描\nALL：全表扫描\n\n\npossible_keys可能用到的索引，一般配合possible_keys列一起看\n\nkey实际用到的索引\n\nrowsMySQL预计要读取的行数，对InnoDB表来说是个估计值\n\nfiltered按表条件过滤后，留存的记录数的百分比，即存储引擎返回的数据在经过过滤后，剩下满足条件的记录数的比例\n\nExtra \n\nUsing filesort：表示按文件排序，一般是在指定的排序和索引排序不一致的情况才会出现。一般见于order by语句\nUsing index ：表示是否用了覆盖索引\nUsing temporary: 表示是否使用了临时表，性能特别差，需要重点优化。一般多见于group by语句，或者union语句\nUsing where : 表示使用了where条件过滤\nUsing index condition：MySQL5.6之后新增的索引下推。在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据\n\n\n\n\n\n\n\n mysql&gt; explain select * from t where a between 10000 and 20000;\n+--+-----------+-----+----------+----+-------------+---+-------+---+----+--------+------------------+\n|id|select_type|table|partitions|type|possible_keys|key|key_len|ref|rows|filtered|Extra    |\n+----+---------+-----+----------+----+-------------+------+---------+------+-------+----------+-----------------------+\n| 1| SIMPLE    | t   | NULL     |range| a          | a | 5     | NULL | 10001 | 100.00 | Using index condition |\n\n\n通过查询 sys库的 schema_unused_indexes视图来查询哪些索引从未被使用\n\nkill\n\nkill query + 线程 id：终止这个线程中正在执行的语句\n把session的运行状态改成THD::KILL_QUERY（将变量 killed 赋值为THD::KILL_QUERY），给session的执行线程发信号\nsession语句中执行到预埋点后才可以终止语句逻辑，处于等待状态的必须是可唤醒的等待\n\n\nkill connection + 线程 id：断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的\nkill query + 线程 id失效的情况：show processlist的时候，看到Command列显示为 killed\n线程没有执行到判断线程状态的逻辑\n等行锁时使用pthread_cond_timedwait函数，虽然可被唤醒但是唤醒后的执行逻辑并没有判断线程状态\n由于 IO 压力过大，读写 IO 的函数一直无法返回，导致不能及时判断线程的状态\n\n\n终止逻辑耗时较长\n超大事务执行期间被 kill：这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长\n大查询回滚：如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长\nDDL 命令执行到最后阶段，如果被 kill，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久\n\n\n直接在客户端通过 Ctrl+C 命令也无法终止：由于 MySQL 是停等协议，所以这个线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，执行 Ctrl+C 的时候，是 MySQL 客户端另外启动一个连接，然后发送一个 kill query 命令\n\n\n\n\n\n2.数据库设计\n命名规范\n采用26个英文字母（区分大小写）和0-9的自然数（基本不需要）加上下划线 _ 组成，使用名词或动宾短语\n表命名：全部用小写，多个单词用下划线 _ 分隔，禁止出现数据库的关键字，一般使用复数\n列命名：在命名表的列时，不要重复表的名称\n字段命名：使用完整名称，禁止缩写，字段一般取单数\n常用缩写：sn（编号）、时间（_at，如created_at）\n\n\n外键：外键约束的作用是维护表与表之间的关系，确保数据的完整性和一致性，举例来说，某一个字段是表b的主键，但是它也是表a中的字段，表a中该字段的使用范围取决于表b\n大表如何添加索引\n不可以随便添加索引的原因：给表添加索引的时候，会对表加锁，会使得对表的增删改查失效\n先创建一张跟原表A数据结构相同的表B，在新表上添加新索引，将原表A数据导入到新表B并将更新表B名字\n\n\n如果 SQL 和索引都没问题，查询还是很慢怎么办？\n大查询改造为分批查询\n数据库分表，降低数据库表的数量\n引入redis缓存 ，减少 mysql 的访问\n\n\n分页\n如果查询出来的结果集，存在连续且递增的字段，可以基于有序字段来进行查询，例如 select xxx from book where 有序字段 &gt;= 1 limit 100\n舍弃limit关键字，如果查询出来的结果集存在连续且递增的字段，使用between and来进行范围结果集查询，例如 select xxx from book where 有序字段 between 10000000 and 1000100\n采用MongoDB、ES搜索引擎优化深分页\n\n\n生成主键的策略\n自增主键\n雪花算法：\nUUID 虽然也可以保证唯一性，但是 UUID 的值是随机的，无序的，不太适合作为主键，因为随机插入，可能会引起页分裂的问题，从而影响查询性能。\n\n\n慢sql\n如果是在项目中，可以通过SpringAOP去查询这个接口运行的时间\n如果是一个sql，可以通过explain的指令去查这个sql的执行计划\n可通过开启mysql的慢日志查询，设置好时间阈值，进行捕获\n\n\n\n2.基础知识1.基础架构\n\n\n\n\n\n\n\n\n客户端+server层+存储引擎，其中server层包括五部分，连接器（身份权限验证）、查询缓存（键值对，易失效，8.0移除）、分析器（词法分析+语法分析，返回出错位置）、优化器（选择索引，按照最优方案执行）、执行器（检验表权限，操作引擎，返回结果）\n\n查询语句执行流程；客户端验证登陆并通过TCP三次握手==连接==服务端，提交的执行语句经过分析器进行词法分析和语法分析通过后，提交给优化器来生成最优的执行方案，最后交给引擎来具体执行\n\n查询缓存不命中的情况    \n任何两个查询在任何字符上的不同都会导致缓存不命中\n如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存\n表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效\n\n\n客户端长时间（wait_timeout）没有命令时，连接器会自动断开\n数据传输（net_buffer）：服务端不保存一个完整的结果集，而是将取到的每一行写入net_buffer中，写满就发送然后清空；如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送\nMySQL 客户端发送请求后，接收服务端返回结果的方式有两种，默认使用第一种，加上-quick后使用第二种\n一种是本地缓存，也就是在本地开一片内存，先把结果存起来。如果用 API 开发，对应的就是 mysql_store_result 方法\n另一种是不缓存，读一个处理一个。如果用 API 开发，对应的就是 mysql_use_result 方法\n\n\n对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，建议使用mysql_store_result这个接口，直接把查询结果保存到本地内存，否则使用mysql_use_result接口\n\n\n\n\n更新语句执行流程：查询缓存，调用引擎API写入数据，InnoDB通过==两阶段提交==记录日志，流程为redo log（prepare）-&gt;binlog-&gt;redo log（commit）\n\n异常时：有prepare，但没有binlog，则回滚事务；由prepare、binlog，但没有commit，则提交事务恢复数据\n非两阶段提交：先写redo log然后宕机，虽然可以通过redo log恢复数据，但是通过binlog备份的时候会丢失数据；先写binlog然后宕机，本地无法通过redo log恢复数据，通过binlog备份时会多出一条事务\nchange buffer：当有更新操作，如果数据页在内存中，则直接更新数据页；如果数据页不在内存中，则会将更新操作先缓存在change buffer中。在后续数据页读入到内存中时执行merger操作，即将change buffer内的更改同步到数据页\n使用场景：适用于普通索引，但唯一索引需要每次都取数据确定唯一性；适用于写多读少的情况（merge操作少）\nchange buffer和redo log：redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗\n\n\n\n\n存储引擎对比\n\nMyISAM 不支持==事务==（MVCC+Next-Key Lock区间锁）和==行级锁==，而且最大的缺陷就是崩溃后无法安全恢复（只有binlog无==redo log==）\n\nMemory引擎：主要用于内存临时表的场景（没有并发问题、不许持久化数据、主备库之间不冲突）\n\n索引组织形式\n\nInnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，称为索引组织表（Index Organizied Table），整体结构为B+树，数据有序存放，数据位置变化时只需要改主键索引，查找需要回表\nMemory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，称为堆组织表（Heap Organizied Table），整体结构为hash表，数据按写入顺序存放，数据位置变化时需要改所有索引\n使用b树索引：alter table t1 add index a_btree_index using btree (id);\n\n\nInnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同\n\n生产环境不使用内存表的原因：内存表不支持行锁，只支持表锁；数据库重启后，所有内存表都会被清空\n\n\n\n存储引擎所支持的索引概述\n\n\n\n\n文件目录结构\n\n每创建一个database，都会在/var/lib/mysql目录下创建一个以数据库名字为名的目录，然后保存表结构和表数据的文件都放在这个目录下，名为my_test的数据库下有一个名为t_order的表\n\n\n数据库名（my_test）目录下的具体文件（以t_order表为例）\n\n\ndb.out：用来存储当前数据库的默认字符集和字符校验规则\nt_order.frm：t_order的表结构会保存在这个文件，MySQL中建立的每一张表都会生成一个.frm文件，用来保存表的元数据信息，如表结构定义\nt_order.ibd：t_order的表数据会保存在这个文件中，表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）。由参数 innodb_file_per_table 控制的，5.6.6之后默认是1，即存放在独占表空间文件（文件名：表名字.ibd）中\n\n\n\n\nInnoDB 它如果是存储一张表的话它是怎么去存储的（应为双向链表）\n\n\n\n2.日志\nredo log\n\n定义：InnoDB特有的，组织成大小为4*1GB（文件组）的一个环形缓冲区，使用两个指针记录位置，write pos（下一次写入位置）+ check point（等待擦除的位置），满了之后就阻塞等待，主要用于MySQL崩溃（实例挂了/宕机）后的恢复\nredo log buffer：查询和删除都是直接操作Buffer Pool中的数据页，更新时记录到redo log buffer中，然后刷盘到redo log\n记录条目：“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”\n刷盘时机：innodb_flush_log_at_trx_commit 参数确定何时刷新、一个后台定时线程每秒刷新、redo log buffer占用的空间即将达到 innodb_log_buffer_size一半时刷新\n\n\n写入流程：\n\n\nbinlog\n\n定义：server层的通用模块，与redo log记录物理日志（在哪个数据页上做了什么）不同，binlog记录逻辑日志，即语句的原始逻辑（在哪个表上做了什么），并且不会覆盖已有日志，直接写入新文件。主要用于数据备份和主从数据同步\n格式；statement（SQL语句原文）、row（SQL语句+数据）、mixed（MySQL选择用哪一个）\n\n\nbinlog cache：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。通过binlog_cache_size确定空间大小，一个事务的binlog不能被拆开，空间不够时需要暂存到磁盘上\n数据先write到文件系统的page cache，再fsync到磁盘，由参数sync_binlog控制write和fsync的时机\n\n\n为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的：MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中。redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中\n\n\nundo log\n\n用途：用于事务执行异常时进行回滚、提供MVCC机制需要的历史版本数据\n回滚日志先于数据持久化到磁盘上，在事务执行中宕机也可以回滚已执行的一半事务\n\n\n确保事务的原子性，用于回滚事务，同时提供mvcc下的非锁定读\n\n\n慢查询日志\n\n记录了执行时间超过long_query_time（默认10s，通常设置为1s）的所有查询语句，在解决SQL慢查询的时候经常用到\n命令：开启（SET GLOBAL slow slow_query_log=ON）、查看状态（show variables like “slow_query_log”; ）\n\n\n中转日志（relay log）：用于主从复制场景下，slave通过io线程拷贝master的bin log后本地生成的日志\n\n提交事务的一整个过程，每个日志都是怎么工作的？具体更新一条记录 UPDATE t_user SET name = &#39;xiaolin&#39; WHERE id = 1; 的流程如下:\n\n执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：\n\n\n如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；\n如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。\n\n\n执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：\n\n\n如果一样的话就不进行后续更新流程；\n如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；\n\n\n开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。\n\nInnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。\n\n至此，一条记录更新完了。\n\n在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。\n\n事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）：\n\n\nprepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；\ncommit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；\n\n\n至此，一条更新语句执行完成\n\n\n\n\n3.锁\n全局锁：Flush tables with read lock;，主要用于做主库逻辑备份，其它全库备份方法如下\nmysqldump+–single-transaction：在一致性读隔离级别开启一个事务，来确保拿到一致性视图，通过MVCC来保证数据可正常更新，需要引擎支持一致性读级别（InnoDB支持MyISAM不支持）\nset global readonly=true：可以让全库进入只读状态，但一方面readonly会有其他用处这样改有副作用，另一方面数据库异常后不自动改此值，导致数据库一直不可写（全局锁自动释放）\n\n\n表级锁（MyISAM、InnoDB）：针对非索引字段加锁，对当前操作的整张表加锁，实现简单，资源消耗少，不会出现死锁，但是高并发下效率低\n表锁：lock tables t1 read, t2 write;\n可以使用unlock tables主动释放锁，也可以在客户端断开的时候自动释放，所以建议把可能影响并发度的锁尽量往后放\nlock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象，比如线程A执行上面的示例语句，线程B写t1、读写t2都会被阻塞；线程A解锁前也只能读t1、读写t2\n意向锁：用表锁的时候快速判断表中的记录是否有行锁，意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁（意向共享锁、意向排他锁）\n\n\n元数据锁（MDL）：MDL 不需要显式使用，在访问一个表的时候会被自动加上，语句执行开始时申请，但是在整个事务提交后才释放（可以通过加超时机制防止阻塞太多后续命令）\n在MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁\n读锁之间不互斥，因此可以有多个线程同时对一张表增删改查；读写锁之间、写锁之间是互斥的，因此两个线程同时给一个表加字段，其中一个要等另一个执行完才能开始执行\n\n\n\n\n行级锁（InnoDB）：针对索引字段进行加锁，只针对当前操作的行记录进行加锁，锁粒度小、并发度高、锁开销大，会出现死锁\n两阶段锁：行锁是在需要的时候加上去的，但是要等事务结束时才释放\n行锁是针对索引字段加的锁，如果where语句中字段没有命中唯一索引或者索引失效时，会导致扫描全表，对表中的所有行记录加锁，但有的时候即使用了索引，也会全表扫描（优化器的原因）\nInnoDB有哪几类行锁：REPEATABLE-READ隔离级别下，默认使用Next-Key Lock，操作的索引是唯一索引或主键时，优化降级为Record Lock\n记录锁（Record Lock） ：也被称为记录锁，属于单个行记录上的锁\n间隙锁（Gap Lock） ：锁定一个范围，不包括记录本身。跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作，两个间隙锁之间不存在冲突关系\n两个线程都拿到了同一间隙锁，然后在执行插入时等待对方的间隙锁，这就导致了同样的语句因为间隙锁的存在会锁住更大的范围而产生死锁\n\n\n临键锁（Next-Key Lock） ：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁\n每个 next-key lock 是前开后闭区间，对于正无穷使用了一个不存在的最大值 supremum 代替（保证闭区间）\n\n\n\n\n\n\n\n4.事务\n基础概念\n\nACID属性：Atomic、Consistency、Isolation、Durability（AID是手段，C是目的）\n原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样\n事务的原子性是通过 undo log 实现的，undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。\n每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作。\n\n\n一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如转账的情况\n隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的\n持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失\n通过 redo log 保证持久化。buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。\n通过两次写来实现，当缓冲池的脏页刷新时，并不直接写磁盘，而是会通过memcpy函数将脏页先拷贝到内存中的doublewrite buffer，之后通过doublewrite buffer再分两次，每次写入1MB到共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘，进行数据持久化。\nWAL （Write-Ahead Logging）技术：InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里\n\n\n\n\n并发带来的问题：脏读（读后被回滚）、丢失修改（写后被覆盖）、不可重复读（两次读结果不同）、幻读（第二次读到的行数多了）\n脏读：如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。\n不可重复读：在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。\n幻读：在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。\n解决幻读的方法：提升事务隔离级别到可序列化、可重复读级别下添加表锁或添加Next-key Lock（记录锁+间隙锁）、隔离级别降到读提交并将binlog改成row格式（记录更改前后数据）\n\n\n\n\n\n\nMVCC\n\n原理：实现依赖==隐藏字段==、==Read View==、==undo log==。在内部实现中，InnoDB 通过数据行的DB_TRX_ID（事务id）和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\nMySQL 中每条记录在更新的时候都会同时记录一条回滚操作（在回滚日志中，在没有比该条日志更旧的read-view后自动删除），记录上的最新值，通过回滚操作，都可以得到前一个状态的值\nMVCC解决部分幻读：MVCC只能解决读取数据是的幻读（当前事务读取时，不受其他事务修改的影响），但是不能解决写入时的幻读（需要MVCC+锁、或者可串行化事务隔离级别）\n与间隙锁的对比：间隙锁锁定索引范围而非实际数据的锁，MVCC与间隙锁的目的都是保证数据库的并发访问安全性，但是MVCC的优势是没有用到锁，性能比间隙锁更好\n\n\n相关字段\nInnoDB为每一行添加了三个隐藏字段\n\n\nDB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务 id\nDB_ROLL_PTR（7字节）回滚指针，指向该行的 undo log\nDB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引\n\n\nRead View：用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”\n\n\nm_low_limit_id：目前出现过的最大的事务 ID+1，即下一个将被分配的事务 ID。大于等于这个 ID 的数据版本均不可见\nm_up_limit_id：活跃事务列表 m_ids 中最小的事务 ID，如果 m_ids 为空，则 m_up_limit_id 为 m_low_limit_id。小于这个 ID 的数据版本均可见\nm_ids：Read View 创建时其他未提交的活跃事务 ID 列表。创建 Read View时，将当前未提交事务 ID 记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids 不包括当前事务自己和已提交的事务（正在内存中）\nm_creator_trx_id：创建该 Read View 的事务 ID\n\n\nundo-log：事务回滚时恢复数据，分为两类\n\ninsert undo log：指在 insert操作中产生的 undo log。因为 insert操作的记录只对事务本身可见，对其他事务不可见，故该 undo log可以在事务提交后直接删除。不需要进行 purge操作\nupdate undo log：update或 delete操作中产生的 undo log。该 undo log可能需要提供 MVCC机制，因此不能在事务提交时就进行删除。提交时放入 undo log链表，等待 purge线程进行最后的删除\n\n\n\n\n底层实现（未完待续）：数据可达性算法\n\n\n事务隔离机制：读未提交、读已提交、可重复读（默认级别）、可序列化\nSET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE]\n\n\n避免长事务的方式：通过information_schema.innodb_trx表监控事务的持续时间、增加undo表空间、通过配置参数max_execution_time指定事务执行的最长时间、利用pt工具监控长事务\n\n可重复读\n\n定义：指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的\n快照生成\n生成时机：在 MySQL 有两种开启事务的命令，这两种开启事务的命令，创建 read view 的时机是不同的\nbegin/start transaction 命令：执行了 begin/start transaction 命令后，并不会创建 read view，只有在第一次执行 select 语句后， 才会创建 read view\nstart transaction with consistent snapshot 命令：执行了 start transaction with consistent snapshot 命令，就会马上创建 read view\n\n\n执行两个select语句，会生成几个快照：1个\n\n\n同一个事务的所有更新操作，都是可见的。事务隔离性，隔离的是其他事务，不隔离自己人\n\n\n可重复读与幻读\n\n标准的SQL隔离级别定义里，可重复读是不可以防止幻读的，但是InnoDB实现的可重复读隔离级别可以解决幻读问题\n快照读（一致性非锁定读）：由MVCC机制保证不出现幻读\nRR/RC级别select默认是快照读（RC级别读锁定行最新快照数据，RR级别读事务开始的数据）、读取到的行正在执行update或delete则不等待锁释放直接读取快照\n\n\n当前读（锁定读）：由Next-Key Lock加锁来防止幻读\nselect加锁（lock in share mode共享锁、for update排他锁）是当前读、update、insert、delete\nRR级别：扫描到的数据都会加行锁和间隙锁，并在commit时释放\nRC级别：扫描到的数据都会加行锁，但不满足条件的数据，不需等到commit，扫描完就释放\n\n\n\n\n可重复读和读提交有什么区别\n\n读提交，指一个事务提交之后，它做的变更才能被其他事务看到\n可重复读，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别\n对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 MVCC 来实现的，它们的区别在于创建 Read View 的时机不同，「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View\n\n\nMVCC解决幻读问题\n\nMySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：\n\n针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。\n针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select … for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。\n\n\n这两个解决方案是很大程度上解决了幻读现象，但是还是有个别的情况造成的幻读现象是无法解决的。比如这个场景：在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。\n\n\n\n\n\n\n\n5.索引\n\n\n\n\n\n\n\n\n\n按「数据结构」分类：B+tree索引、Hash索引、Full-text索引\n按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）\n按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引\n按「字段个数」分类：单列索引、联合索引\n\n\nB+树：索引的底层数据结构，InnoDB中每个节点使用一个页（page），页的大小为16KB，元数据占128字节，一条记录大约16字节，对于非叶节点，可以存1000条记录，对于叶节点，假设可以存100条数据，综上，对于一颗3层B+树，可以存储1亿条记录，充分利用局部性原理减少IO次数\n其它索引结构：Hash索引不支持顺序和范围查询、二叉查找树容易不平衡、平衡二叉树由于旋转耗时，删树数据时效率很低、红黑树效率高但是高度太高增加IO次数、B树节点过大增加IO次数\nB树和B+树的区别：B+树非叶子不存数据、叶子节点有一条引用链指向其它相邻叶子节点所以可直接对链表进行遍历、B+树查到叶子才返回数据可在非叶子节点中重复出现\nHash索引和B+树索引的区别：B+树可以进行范围查询、B+树支持联合索引的最左匹配原则、B+树支持order by排序、B+树支持like进行模糊查询；但Hash索引在等值查询上比B+树高效\n范围查询：比如要查主键在[1,17]之间的记录。二次查询，先查找1所在的叶子节点的记录位置，再查找17所在的叶子节点记录的位置（就是16所处的位置），然后顺序地从1遍历链表直到16所在的位置\n前缀匹配模糊查询。假设主键是一个字符串类型，要查询where Key like abc%，其实可以转化成一个范围查询Key in [abc,abcz]。当然，如果是后缀匹配模糊查询，或者诸如where Key like %abc%这样的中间匹配，则没有办法转化成范围查询，只能挨个遍历\nHash索引缺点：容易导致全表扫描，因为可能存在不同的key经过hash运算后值相同；索引列上的值相同的话，易造成hash冲突，效率低下\n\n\nMyISAM和InnoDB引擎对B+树的不同实现\nMyISAM中，叶子节点的data域存放的是数据记录的地址，需要通过改地址读取对应的数据记录\nInnoDB中，索引文件和数据文件是分离的，数据文件是以主键为索引的key形成的树，叶子节点保存了完整的数据，其他的索引的叶子节点存储的是主键的值。所以通过主键查找直接能找到数据，通过其他索引只能找到对应主键，然后再根据主键去数据文件找。所以建议使用单调的字段作为主键，防止造成主索引频繁分裂（B+树的插入机制）\n索引结构和数据一起存放的索引称为聚簇索引，如InnoDB的主键索引；索引结构和数据分开存放的索引称为非聚簇索引，如InnoDB的辅助索引\n\n\n\n\n主键索引（聚簇索引，clustered index）和非主键索引（二级索引，secondary index）\n主键索引：加速查询 + 列值唯一（不可以有 NULL）+ 表中只有一个，查询速度快但更新代价大，所以一般都是不可修改的，每个表只能有一个主键\n聚簇索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里\n当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引且不允许存在 null 值的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键\n建表语句里一定要有自增主键，只有在类似于哈希表的数据表中才会使用业务字段直接锁主键\n重建主键索引的方法：直接删除重建会使得所有非主键索引都失效，推荐方法为用空的alter操作，比如ALTER TABLE t1 ENGINE = InnoDB;这样子就会原地重建表结构\n\n\n非主键索引（二级索引）：叶子节点存储的数据是主键，需要根据主键去主键索引在搜索一次（回表），更新代价小但需要回表\n唯一索引：加速查询 + 列值唯一（可以有 NULL），主要为了保证属性列的数据的唯一性\n普通索引：仅加速查询，允许重复、允许为NULL、允许创建多个\n前缀索引(Prefix)：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符（alter table SUser add index index2(email(6));）\n倒序索引：select field_list from t where id_card = reverse(&#39;input_id_card_string&#39;);\n前缀索引对覆盖索引的影响：使用前缀索引就用不上覆盖索引对查询性能的优化了，因为无法确定前缀索引是否截断了完整信息\n\n\n全文索引：对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替\n\n\n为什么不推荐使用外键和级联（主键改外键需要跟着改）：不适用高并发、分库分表不友好、增加复杂性（外键约束、业务变化）\n外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度\n增加了复杂性：\n每次做 DELETE 或者 UPDATE 都必须考虑外键约束，会导致开发的时候很痛苦, 测试数据极为不方便;\n外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦\n\n\n对分库分表不友好：因为分库分表下外键是无法生效的\n\n\n\n\n联合索引及相关优化：覆盖索引、最左前缀匹配原则、索引下推\n联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并，实现为ALTER TABLE cus_order ADD INDEX id_score_name(score, name);\n覆盖索引：一个索引叶子节点数据包含（或者说覆盖）所有需要查询的字段的值，可以不用二次查询，比如在非主键索引查记录的主键可以不用回表\n最左前缀匹配原则：在使用联合索引时，MySQL会根据联合索引中的字段顺序，从左到右依次到查询条件中去匹配，如果查询条件中存在与联合索引中最左侧字段相匹配的字段，则就会使用该字段过滤一批数据，直至联合索引中全部字段匹配完成，或者在执行过程中遇到范围查询（如**&gt;、&lt;**）才会停止匹配。所以在使用联合索引时，可以将区分度高的字段放在最左边，这样可以过滤掉更多数据\n索引下推：MySQL 5.6 版本中提供的一项索引优化功能，可以在索引遍历过程中，对索引中包含的字段（联合索引）先做判断，过滤掉不符合条件的记录，减少回表次数\n\n\n非主键索引默认与主键建立联合索引，可以减少需要的联合索引个数\n\n\n\n\n3.进阶知识1.索引选择\n\n\n\n\n\n\n\n\n选择合适的字段创建索引（不为NULL、被频繁查询、被作为条件查询、频繁需要排序的、频繁用于连接的）；频繁用于更新的字段不适合建立索引，维护索引的成本很高；索引数量不能过多，避免冗余索引；不适合建立索引（数据量少、更新频繁、区分度低、已经有联合索引、用不到的字段）\n\n索引选择：\n\n指标\n\n预估扫描行数：show index的cardinality列反应的是索引的基数（索引上不同值个数），通过使用采样统计选择M个数据页，统计每个页面上不同值个数，然后求求平均再乘索引的页面数得到索引的基数（变更的数据行超过1/M时重新统计）\n\nanalyze table tableName;：当索引的统计信息不对时，可以用来重新统计索引信息\n\nMySQL有两种存储索引统计的方式通过设置参数 innodb_stats_persistent 的值来选择：\n\n设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10\n设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16\n\n\n\n\n是否需要回表：选择不需要回表的作为索引\n\n是否需要再次排序：选择已排序列为索引\n\n\n\n引导优化器选择索引的方法\n\n采用 force index 强行选择一个索引：select * from t force index(a) where a between 10000 and 20000;\n修改语句，引导 MySQL 使用我们期望的索引：在保证业务正确的前提下，进行一些优化：如order by b limit 1 改为 order by b,a limit 1，可以使其使用a为索引\n新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引\n\n\n通过查询sys库的schema_unused_indexes视图来查询哪些索引从未被使用\n\n\n示例\n\n问题：index(abc)\nselect * from T where a=x and b=y and c=z\nselect * from T where a=x and b&gt;y and c=z\nselect * from T where c=z and a=x and b=y\nselect (a,b) from T where a=x and b&gt;y\nselect count(*) from T where a=x\nselect count(*) from T where b=y\nselect count(*) form T\n\n\n索引选择：\na、b、c三个字段都可以走联合索引\na和b都会走联合索引，但是由于最左匹配原则， 范围查找后面的字段是无法走联合索引的，但是在 mysql 5.6 版本后，c 字段虽然无法走联合索引，但是因为有索引下推的特性，c 字段在 inndob 层过滤完满足查询条件的记录后，才返回给server 层进行回表，相比没有索引下推，减少了回表的次数。\n查询条件的顺序不影响，优化器会优化，所以a、b、c三个字段都可以走联合索引\na和b都会走联合索引，查询是覆盖索引，不需要回表\na 可以走联合索引\n只有b，无法使用联合索引，由于表存在联合索引，所以 count(*) 选择的扫描方式是扫描联合索引来统计个数，扫描的方式是type=index\n由于表存在联合索引，所以 count(*) 选择的扫描方式是扫描联合索引来统计个数，扫描的方式是type=index\n\n\n\n\n\n\n对索引字段进行函数操作，优化器会放弃走树搜索功能\n\n条件字段函数操作：如果对字段做了函数计算，就用不上索引了，这是 MySQL 的规定\n\n问题SQL：select count(*) from tradelog where month(t_modified)=7;\n\n原因：对索引字段做函数操作（包括+1操作），可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能，但是不代表放弃这个索引，而是继续根据之前指标来确定索引\n\n改进：把 SQL 语句改成基于字段本身的范围查询，这样优化器就能用上 t_modified 索引的快速定位能力了，否则需要进行全表扫描\nmysql&gt; select count(*) from tradelog where\n    -&gt; (t_modified &gt;&#x3D; &#39;2016-7-1&#39; and t_modified&lt;&#39;2016-8-1&#39;) or\n    -&gt; (t_modified &gt;&#x3D; &#39;2017-7-1&#39; and t_modified&lt;&#39;2017-8-1&#39;) or \n    -&gt; (t_modified &gt;&#x3D; &#39;2018-7-1&#39; and t_modified&lt;&#39;2018-8-1&#39;);\n\n\n隐式类型转换\n\n问题SQL：select * from tradelog where tradeid=110717;\n原因：tradeid字段是varchar(32)，输入的参数确实整型，所以需要做类型转换，这里的类型转换规则是字符串和数组做比较，将字符串转换成数字\n对于优化器来说，上面的语句相当于：select * from tradelog where CAST(tradid AS signed int) = 110717;，即对索引字段使用了函数，优化器放弃走树搜索功能\n\n\n\n\n隐式字符编码转换\n\n问题SQL：select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;，其中tradelog\n字符集为utf8mb4，trade_detail字符集为utf8\n\n底层：select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value;\n改进：select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2;\n\n\n原因：两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引，导致tradelog查处一行后去trade_detail查时使用的全表扫描\n\n字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较\n因此， 在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，在跟另一表中的字段进行比较\n\n\n不会出现问题的SQL：select operator from tradelog where traideid =$R4.tradeid.value;\n\n底层：select operator from tradelog where traideid =CONVERT($R4.tradeid.value USING utf8mb4);\n这里的 CONVERT 函数是加在输入参数上的，这样就可以用上被驱动表的 traideid 索引\n\n\n\n\n\n\n常见优化索引的方法：\n\n前缀索引优化：使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小\n覆盖索引优化：覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作\n主键索引最好是自增的：\n如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高\n如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率\n\n\n防止索引失效\n\n\n索引失效的情况\n\n使用 SELECT * 进行查询;\n\n创建了组合索引，但查询条件未遵守最左匹配原则;\n\n在索引列上进行计算（如，+、-、*、/）、函数、类型转换等操作;\n\n以 % 开头的 LIKE 查询比如 like &#39;%abc&#39;;（左模糊）、like %a%;（全模糊）\n\n查询条件中使用 or，且 or 的前后条件中有一个列没有索引，涉及的索引都不会被使用到;\n\n如果 or 左右两个字段都是索引，就能走索引\n\n如果 a 和b 是联合索引，会发生索引失效，对于联合索引（比如 bc），如果使用了 b =xxx or c=xxx，会走不了索引\n\n\n\n发生隐式转换\n\n问题：下面四条语句中，第3条比1、2、4慢很多\n1: SELECT * FROM &#96;test1&#96; WHERE num1 &#x3D; 10000;\n2: SELECT * FROM &#96;test1&#96; WHERE num1 &#x3D; &#39;10000&#39;;\n3: SELECT * FROM &#96;test1&#96; WHERE num2 &#x3D; 10000;\n4: SELECT * FROM &#96;test1&#96; WHERE num2 &#x3D; &#39;10000&#39;;\n定义：当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。某些转换是隐式发生的。例如，MySQL 会根据需要自动将字符串转换为数字，反之亦然\n\n根据文档：语句2和语句3的两边被转换成了浮点数来比较\n其中语句2都转换成了浮点数进行比较，转换结果是唯一确定的（都是10000），不影响索引使用\n语句3虽然都转换成了10000，但是除了‘10000’可以转换成10000，‘01000’也可以，所以不是唯一的，不可用索引\n转换规则\n不以数字开头的字符串都将转换为0。如&#39;abc&#39;、&#39;a123bc&#39;、&#39;abc123&#39;都会转化为0；\n以数字开头的字符串转换时会进行截取，从第一个字符截取到第一个非数字内容为止。比如&#39;123abc&#39;会转换为123，&#39;012abc&#39;会转换为012也就是12，&#39;5.3a66b78c&#39;会转换为5.3，其他同理\n\n\n\n\n\n\n\n\n两列数据做比较，即使两列都创建了索引，索引也会失效\n\n查询条件是is null时正常走索引，使用is not null时，不走索引\n\n当查询条件为大于等于、in等范围查询时，根据查询结果占全表数据比例的不同，优化器有可能会放弃索引，进行全表扫描\n\nmysql 估计使用全表扫描要比使用索引快，则不使用索引\n\n\n\n\n\n2.缓存\n刷脏页：InnoDB使用buffer pool管理内存，当内存数据页与磁盘不一样时就称为脏页，需要合适的时机刷新到磁盘上同步数据\n\n刷脏页的时机\n\nInnoDB 的 redo log 写满了。系统会停止所有更新操作，把checkpoint往前推进，将扫到的redo log字段对应的数据页flush到磁盘上，redo log留出空间可以继续写\n系统内存不足需要淘汰掉内存中的页时，如果该页是脏页则需要将数据同步到磁盘上，保证每个数据页不论在内存中还是磁盘上，都是正确的数据（在内存的数据页，其磁盘的就是旧值）\nMySQL 认为系统“空闲”的时候（见缝插针刷新脏页）、MySQL 正常关闭的时候（刷新所有脏页，再次启动时直接读磁盘）\n\n\nInnoDB刷脏页的控制策略\n\n影响性能的情况：一个查询要更新的脏页个数太多；日志写满更新全部堵住，写性能跌为0\n\n刷盘速度：X * max(F1(M), F2(N))\n\ninnodb_io_capacity：告诉 InnoDB 现在的磁盘能力，可以设置成磁盘的IOPS，假设当前为（X）\n\ninnodb_max_dirty_pages_pct：==脏页比例==上限，默认是75%，InnoDB会根据当前脏页比例（假设为M，计算方式 如下），算出一个0到100之间的数字（F1(M)）\nmysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME &#x3D; &#39;Innodb_buffer_pool_pages_dirty&#39;;\nselect VARIABLE_VALUE into @b from global_status where VARIABLE_NAME &#x3D; &#39;Innodb_buffer_pool_pages_total&#39;;\nselect @a&#x2F;@b; #即Innodb_buffer_pool_pages_dirty&#x2F;Innodb_buffer_pool_pages_total\n==redo log写盘速度==：根据写入日志的序号和checkpoint序号之间的差值（假设为N），计算出另一个0到100之间的数字（F2(N)）\n\n\n\n脏页选择算法：改进的LRU算法\n\n按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域，前面是young区域，LRU_old指向old区域第一块\nyoung区域：访问后放到young头部，新数据插入到LRU-old处\nold区域：在LRU中存在超过1s，移到链表头部；否则保持不变（很快失效的不会被保存很久）。所以短时间多次访问一个表不会让其它缓存失效\n\n\n\n\n邻居刷新机制：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷（innodb_flush_neighbors = 1时启用，0时关闭）\n\n\n\n标记删除：从5.6.6开始每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中，删除的记录不会直接在B+树中删除，而是标记删除并等待被复用，所以经过大量删除的表可能存在空洞，通过重建表来收缩空洞减少内存消耗\n\n原理：新建一个表B，表A的数据按顺序插入到B中，这个过程需要全程拿MDL写锁（需要移动数据，下面的Online DDL不需要移动数据，数据存放在tmp_file临时文件中）\nOnline DDL：MySQL5.6引入，可以在重建表的过程中，保证表A上的更新操作不被阻塞：使用日志文件（row log）记录所有A的操作（alter table t engine=innodb,ALGORITHM=inplace;）\nanalyze table t 不是重建表，只是通过那MDL读锁并重新统计；而 optimize table t 等于 recreate+analyze\n\n\n临时表\n\n特点\n可以使用各种引擎类型，使用InnoDB引擎/MyISAM引擎就写到磁盘上，否则使用Memory引擎写到内存上，支持自动回收\n一个临时表只能被创建它的session访问，对其他线程不可见，不同session的临时表可重名\n创建一个名为\\#sql&#123;进程 id&#125;_&#123;线程 id&#125;_ 序列号.frm的文件，所以可重名\n\n\n可以与普通表同名，同名时除了show tables外，都显示临时表，如show create、增删改查等语句\n内存中每个表都对应一个 table_def_key，普通表的值为库名 + 表名，临时表的值外加了server_id+thread_id\n\n\n\n\n用途：因为不用担心重名冲突，所以常被用在复杂查询的优化过程（sort buffer、join buffer）\n分库分表的跨库查询：把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作\n\n\n日志记录\n临时表 redolog：不记录，因为崩溃之后，临时表全没了，也不需要恢复\nundolog：需要记录，5.6之前是和普通表放一块的；5.7之后放在临时表空间的\nbinlog：row格式不用记，statement/mix需要记录，但不记录\n主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，这时候我们就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行\n线程是session级别的且binlog_fotmat=row时，drop table 临时表不会传过去，因为row模式从库没有临时表\n\n\n\n\n内部临时表\n如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果；\njoin_buffer 是无序数组，sort_buffer 是有序数组，临时表是二维表结构；\n如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union 需要用到唯一索引约束， group by 还需要用到另外一个字段来存累积计数\n\n\n\n\n\n3.加锁规则\n查询长时间不返回\n阻塞\n可以使用 show processlist命令查看当前执行的语句是否在等待锁\n通过查询 sys.schema_table_lock_waits 这张表（select blocking_pid from sys.schema_table_lock_waits;），就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可\n\n\n等flush\n通过select * from information_schema.processlist where id=1;语句查看是否在等flush\nMySQL 里面对表做 flush 操作的用法，一般有以下两个flush tables t with read lock; 和flush tables with read lock;，但是这两条语句一般都执行很快，waiting for table flush状态可能是有一个flush tables命令被别的语句堵住\n\n\n等行锁\n通过select * from t sys.innodb_lock_waits where locked_table=&#39;test.t&#39;\\\\G来查询谁占着这个写锁\n\n\n不断回滚\n使用带lock in share mode的SQL语句，是当前读，而不带这个的SQL语句会使用undolog，不断回滚找到自己的视图，这样速度会很慢\n\n\n\n\n加锁规则（5.x 系列 &lt;=5.7.24，8.0 系列 &lt;=8.0.13）\n原则 1：加锁的基本单位是 next-key lock（前开后闭区间）\n原则 2：查找过程中访问到的对象才会加锁\n优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁\n优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁\n一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止\n\n\n死锁解除：方法一是设定超时时间（innodb_lock_wait_timeout）、方法二是发起死锁检测（innodb_deadlock_detect=on），主动回滚死锁链条中的某一事务、方法三是控制并发度、方法四是确保业务一定不死锁，产生了就回滚、方法五是将一个总账户分成多个小账户来提高并发度（需要业务控制逻辑正确）\n\n4.日志配置\n双1配置\n定义：sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog\n日志逻辑序列号（log sequence number）：单调递增，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。\n组提交：一个事务提交的时候，使用组里的现有事务作为LSN，并将现有事务一起写入磁盘中。所以在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好\n\n\n如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？\n设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险\n将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志\n将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据\n\n\n主备同步（未完待续）\n\n5.SQL语句\ncount(*)\n\nMyISAM每个表缓存此值，但是InnoDB每次都需要重新计算，因为MVCC机制，每个版本的表不同，一个表记录一个值没有意义\n优化：将此值保存在数据库的一张表里，通过事务机制来保证数据更改的并发问题（使用Redis缓存不是原子操作有并发问题）\n不同的count用法：server层要什么就给什么、InnoDB只给必要的值、优化器之优化了count(*)的语义为“取行数”\n对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加\n对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加\n对于 count(字段) 来说，\n如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，字段肯定不能为 null 可以直接按行累加\n如果这个“字段”定义允许为 null，那么执行的时候，字段有可能是 null，需要把值取出来再判断一下，不是 null 才累加\n\n\n count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加（效率最高）\n\n\n\n\norder by：通过max_length_for_sort_data参数来决定排序方法，需要的一行数据小于此值时使用全字段排序，大于则使用rowid 排序\n\n全字段排序：通过索引取出满足条件的记录的所需字段，放入名为sort_buffer的内存中，然后进行快速排序或外部归并排序（取决于sort_buffer_size的大小，不够则使用磁盘里的临时文件来辅助）\nrowid排序：放入sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id，排序完后按照顺序返回原表中取出所需的其它字段\n优化：建立联合索引（另一字段有序），覆盖索引（不用回表）来使得查询不用每次都排序\n使用SELECT * FROM information_schema.OPTIMIZER_TRACE\\\\G来查看相关数据\nnumber_of_tmp_files：看到使用的临时文件数量\nsort_mode：packed_additional_fields（使用实际大小申请内存）、rowid（使用rowid排序）\n\n\n\n\n\n\n显示随机消息：select word from words order by rand() limit 3;随机拿出的值是需要放到临时表中存储的，大小超过tmp_table_size参数使用order by的InnoDB表的排序方式，小于tmp_table_size参数则使用内存临时表和rowid方法来排序（不用回表）\n\n当limit限制的行数所占用的内存小于sort_buffer_size时，会选择优先级队列排序算法（堆排序），大于sort_buffer_size时，使用外部归并排序算法\n优化方法\n方法一：取得这个表的主键 id 的最大值 M 和最小值 N；用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N；取不小于 X 的第一个 ID 的行（结果不是严格随机的，但是效率高）\n方法二：取得整个表的行数，并记为 C；取得 Y = floor(C * rand())（ floor 函数在这里的作用，就是取整数部分）；再用 limit Y,1 取得一行（结果是严格随机的，但是效果低于方法一）\n\n\n\n\ndrop（删除表）、delete（清除记录）、truncate（清空表中数据）的区别\n\n用法不同\ndrop：丢弃数据，如drop table 表名，直接将表删除掉，不但数据会删除，表的结构也会删除\ntruncate：清空数据，如truncate table 表名，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用\ndelete：删除数据，如delete from 表名 where 列名=值，删除某一行的数据，如果不加 where子句和truncate table 表名作用类似\n\n\ndrop和truncate属于DDL（数据定义）语句，操作立即生效，不能回滚，而delete是DML（数据操作语言）语句，如果放到rollback片段中，事务提交之后才会生效\n执行速度不同：一般来说：drop&gt;truncate&gt;delete\ndelete命令执行的时候会产生数据库的binlog日志，而日志记录是需要消耗时间的，但是也有个好处方便数据回滚恢复\ntruncate命令执行的时候不会产生数据库日志，因此比delete要快。除此之外，还会把表的自增值重置和索引恢复到初始大小等\ndrop命令会把表占用的空间全部释放掉\n\n\n\n\njoin\n\nIndex Nested-Loop Join：先从表1取1行数据，然后取出join字段去表2中查找，取出满足条件的行，并且表2该join字段有索引，可以走树搜索过程\n驱动表是走全表扫描，而被驱动表是走树搜索，所以让小表做驱动表更快\n小表确定：在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表\nBatched Key Access算法：set optimizer_switch=&#39;mrr=on,mrr_cost_based=off,batched_key_access=on&#39;;\nMulti-Range Read 优化原理：join得到多个返回值时，先放入read_rnd_buffer进行排序，然后批量返回进行顺序查找\n使用join_buffer来暂存数据用于排序\nBNL算法转成BKA算法：直接在被驱动表上建索引（数据存到临时表再加索引），这时就可以使用NLJ算法，然后使用 BKA 算法了\n\n\n\n\nSimple Nested-Loop Join：先从表1取1行数据，然后取出join字段去表2中查找，取出满足条件的行，但是表2该join字段没有索引，需要走全表扫描，效率低，所以被驱动表没有可用索引时使用下面的join方法\nBlock Nested-Loop Join：把表 t1 的数据读入线程内存 join_buffer 中，扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回\njoin_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，就分段放\n内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数还是应该选择小表来作为驱动表\n缺点：多次扫描一个表，虽然有优化后的LRU算法，但是如果是冷表就会有问题\n冷表的数据量小于整个 Buffer Pool 的 3/8：多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部\n冷表很大：由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰，业务正常访问的数据页，没有机会进入 young 区域\n\n\n\n\n\n\ngroup by\n\n如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null\nselect id%10 as m, count(*) as c from t1 group by m order by null;\n尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort\n\n如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过调大 tmp_table_size 参数，避免用到磁盘临时表；\nset tmp_table_size&#x3D;1024;\nselect id%100 as m, count(*) as c from t1 group by m order by null limit 10;\n如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果\nselect SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;\n\n\n\n6.紧急操作\n短期临时提升性能\n\n短连接风暴：正常执行流程是创建短连接，执行少量的SQL，然后断开。但是在连接数暴涨（超过max_connections参数）时，系统就会拒绝接下来的连接请求，返回“Too many connections”\n\n方法一：先处理掉那些占着连接但是不工作的线程，通过kill connection + id;主动断开不需要的连接，类似于实现设置连接的wait_timeout参数，空闲过久则断开连接\n安全删除：通过show processlist;查找sleep的线程，通过查 information_schema 库的 innodb_trx 表看对应事务具体的状态\n断开的连接会返回“ERROR 2013 (HY000): Lost connection to MySQL server during query”，需要业务系统发起新的连接请求，否则业务认为MySQL一直没恢复\n\n\n方法二：减少连接过程的消耗\n跳过权限验证的方法：重启数据库，并使用–skip-grant-tables 参数启动。这样，整个 MySQL 会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内\n\n\n\n\n慢查询性能问题：在上线前使用慢查询日志记录所有语句的执行过程，看看Rows_examined字段是否与预期一致\n\n索引没有设计好：通过紧急创建索引，直接执行alter table语句，可以使用下面的方法，或者使用gh-ost这样的方案\n\n在备库 B 上执行 set sql_log_bin=off，也就是不写 binlog，然后执行 alter table 语句加上索引\n执行主备切换；这时候主库是 B，备库是 A\n在 A 上执行 set sql_log_bin=off，然后执行 alter table 语句加上索引\n\n\nSQL 语句没写好：5.7开始提供query_rewrite功能，可以把输入的一种语句改写成另一种模式，如下所示\n#改写 select * from t where id + 1 &#x3D; 10000\nmysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (&quot;select * from t where id + 1 &#x3D; ?&quot;, &quot;select * from t where id &#x3D; ? - 1&quot;, &quot;db1&quot;);\nmysql&gt; call query_rewrite.flush_rewrite_rules();\nMySQL 选错了索引：使用查询重写功能，给原来的语句加上 force index\n\n\n\nQPS（每秒查询数）突增：由于业务突然出现高峰，或应用程序bug所导致，解决方案如下\n\n一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉\n如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0\n如果这个新增的功能跟主体功能是部署在一起的，那么只能通过处理语句来限制，可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成”select 1”返回\n\n\n\n\n误删数据\n\n使用 delete 语句误删数据行：用 Flashback 工具通过闪回把数据恢复回来；原理是通过修改binlog的内容，拿回原库重放；前提是确保 binlog_format=row 和 binlog_row_image=FULL\n不建议直接在主库上执行这些操作，恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库\n事前预防：把 sql_safe_updates 参数设置为 on（没有where时会报错）；代码上线前，必须经过 SQL 审计\n使用 truncate /drop table 和 drop database 命令删除的数据，就无法通过 Flashback 来恢复了，因为binlog没有每一条记录\n\n\n使用 drop database 语句误删数据库：使用全量备份，加增量日志的方式了。这个方案要求线上有定期的全量备份，并且实时备份 binlog\n跳过误操作的语句\n先用–stop-position 参数执行到误操作之前的日志，然后再用–start-position从误操作之后的日志继续执行；\n实例使用了 GTID 模式，通过set gtid_next=gtid1;begin;commit;跳过改语句；\n\n\n一种加速方法：在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，跳过增量日志的读取过程\n在 start slave 之前，先通过执行﻿﻿change replication filter replicate_do_table = (tbl_name)命令，就可以让临时库只同步误操作的表，这样做也可以用上并行复制技术，来加速整个数据恢复过程\n在接入线上备库的从库时, 需要先将误删除的gtid先设置跳过, 然后利用主从同步的并行复制技术，来加速整个数据恢复过程\n\n\n预防方法\n搭建延迟复制的备库，通过CHANGE MASTER TO MASTER_DELAY = N命令，可以指定这个备库持续保持跟主库有 N 秒的延迟\n账号分离：只给业务开发 DML 权限，而不给 truncate/drop 权限\n制定操作规范：在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响后再删除这张表\n\n\n\n\n使用 rm 命令误删整个 MySQL 实例\n对于一个有高可用机制的 MySQL 集群来说，最不怕的就是 rm 删除数据了。只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作\n\n\n\n\n快速复制一张表\n\n使用 mysqldump 命令将数据导出成一组 INSERT 语句\n\n–single-transaction 的作用是，在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法；\n–add-locks 设置为 0，表示在输出的文件结果里，不增加” LOCK TABLES t WRITE;” ；\n–no-create-info 的意思是，不需要导出表结构；\n–set-gtid-purged=off 表示的是，不输出跟 GTID 相关的信息；\n–result-file 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的\n\nmysqldump -h$host -P$port -u$user --add-locks&#x3D;0 --no-create-info --single-transaction  --set-gtid-purged&#x3D;OFF db1 t --where&#x3D;&quot;a&gt;900&quot; --result-file&#x3D;&#x2F;client_tmp&#x2F;t.sql\n\n# 将这些 INSERT 语句放到 db2 库里去执行\nmysql -h127.0.0.1 -P13000  -uroot db2 -e &quot;source &#x2F;client_tmp&#x2F;t.sql&quot;\n导出和导入 CSV 文件:\nselect * from db1.t where a&gt;900 into outfile &#39;&#x2F;server_tmp&#x2F;t.csv&#39;;\nload data infile &#39;&#x2F;server_tmp&#x2F;t.csv&#39; into table db2.t;\n物理拷贝方法\n\n\n\n慢sql排查\n\n如果是在项目中，可以通过SpringAOP去查询这个接口运行的时间\n如果是一个sql，可以通过explain的指令去查这个sql的执行计划\n可通过开启mysql的慢日志查询，设置好时间阈值，进行捕获\n\n\n分库分表\n\n分表：当数据量过大造成事务执行缓慢时，就要考虑分表，因为减少每次查询数据总量是解决数据查询缓慢的主要原因\n分库：为了应对高并发，一个数据库实例撑不住，即单库的性能无法满足高并发的要求，就把并发请求分散到多个实例中去，这种就是分库\n\n\n\n","slug":"MySQL","date":"2023-04-27T10:54:53.000Z","categories_index":"","tags_index":"database","author_index":"Dajunnnnnn"},{"id":"75ca176d6b382373bec123f05862c849","title":"Java并发","content":"Java并发1.线程\n线程\n\n线程状态：NEW、RUNNABLE（READY、RUNNING）、WAITING、BLOCKED、TERMINATED、TIME_WAITING\n\n\n线程模型：内核线程（1:1）、用户线程（1:N）、混合线程（M:N）\n\nJava使用用户线程模型，上层JVM通过协作式调度来管理这些用户线程，可以在一个线程执行过程中暂停切换到另一线程执行，底层JVM将Java线程映射到操作系统的线程，由操作系统调度和管理\n启动main函数时启动了一个JVM进程，而main函数所在的线程就是这个进程中的一个（主）线程。多个线程共享进程的堆（新建的对象）和方法区资源（已加载的类信息、静态变量、常量、JIT代码），但每个线程有自己的程序计数器、虚拟机栈和本地方法栈\n多线程：减少了上下文的开销，提高了系统的并发能力，减弱IO与CPU的速度差；但会造成死锁、内存泄漏、线程不安全等问题\n\n\n线程安全\n\n线程安全：描述的对象可以是函数也可以是类，线程安全意味者不同线程并发执行相同的函数，或者不同线程执行一个类的不同函数，因为线程切换，函数内的指令都可以任意交叉执行，最终任意执行顺序得到的结果都是相同的，符合预期的\n\n临界区：可能会引起线程不安全的局部代码块，有两个特征，一是访问了共享资源、二是包含复合操作（先检查在执行、先读取再修改后写入）\n&#x2F;&#x2F;先检查再执行\npublic class Singleton &#123;\n    private static Singleton instance;\n    private Singleton()&#123;&#125;\n    public static Singleton getInstance()&#123;\n        if (instance &#x3D;&#x3D; null) &#123;\n            instance &#x3D; new Singleton();\n        &#125;\n        return instance;\n    &#125;\n&#125;\n&#x2F;&#x2F;先读取再修改后写入\npublic class Demo &#123;\n    private int count &#x3D; 0;\n    public void increment()&#123;\n        count++;\n    &#125;\n&#125;\n同步互斥：用于保证线程安全的访问临界区资源的方法\n\n\n\n\n\n线程创建\n\n实现Runnable接口的run()和start()；继承Thread类重写run方法和start方法，==可用Thread类的已有方法==\n&#x2F;&#x2F;class ThreadDemo extends Thread &#123; 内容同下 &#125; \nclass RunnableDemo implements Runnable &#123;\n   private Thread t;\n   private String threadName;\n   \n   RunnableDemo( String name) &#123; threadName &#x3D; name; &#125;\n   \n   public void run() &#123;\n      &#x2F;&#x2F;线程内需要做的操作\n   &#125;\n   \n   public void start () &#123;\n      if (t &#x3D;&#x3D; null) &#123;\n         t &#x3D; new Thread (this, threadName);\n         t.start ();\n      &#125;\n   &#125;\n&#125;\n通过Callable接口和FutureTask类创建线程，==可创建有返回值的线程（在call函数中实现）==\npublic class CallableThreadTest implements Callable&lt;Integer&gt; &#123;\n    public static void main(String[] args)  \n    &#123;  \n        CallableThreadTest ctt &#x3D; new CallableThreadTest();  \n      \t&#x2F;&#x2F;使用FutureTask包装Callable接口的实现类\n        FutureTask&lt;Integer&gt; ft &#x3D; new FutureTask&lt;&gt;(ctt);\n        for(int i &#x3D; 0;i &lt; 100;i++)  \n        &#123;  \n            System.out.println(Thread.currentThread().getName()+&quot; 的循环变量i的值&quot;+i);  \n            if(i&#x3D;&#x3D;20)  \n            &#123;  \n                new Thread(ft,&quot;有返回值的线程&quot;).start();&#x2F;&#x2F;call相当于run，但是有返回值  \n            &#125;  \n        &#125;  \n        try  \n        &#123;  \n            System.out.println(&quot;子线程的返回值：&quot;+ft.get());&#x2F;&#x2F;得到call函数的返回值   \n        &#125; catch (InterruptedException e)  \n        &#123;  \n            e.printStackTrace();  \n        &#125; catch (ExecutionException e)  \n        &#123;  \n            e.printStackTrace();  \n        &#125;  \n  \n    &#125;\n    @Override  \n    public Integer call() throws Exception  \n    &#123;  \n        int i &#x3D; 0;  \n        for(;i&lt;100;i++)  \n        &#123;  \n            System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);  \n        &#125;  \n        return i;  \n    &#125;  \n&#125;\n==注意事项==\n\n直接使用Thread类的run方法：new一个Thread类，线程进入NEW状态，调用start方法，启动一个线程并使线程进入READY状态，当分配到时间片后就可以开始运行了，start会执行线程的相应准备工作，然后自动执行run方法的内容，这是真正的多线程工作，但是直接执行run方法，会把run方法当作一个main线程下的普通方法来执行，并不会在某个线程中执行它，所以这并不是多线程工作\nsleep与wait的区别：sleep是Thread类的静态本地方法，wait则是Object类的本地方法\nsleep方法没有释放锁，wait释放了锁\nwait是让获得对象锁的进程实现等待，会自动释放当前线程占有的对象锁，每个对象（Object）都拥有对象锁，既然要释放当前线程占有的对象锁并让其进入WAITING状态，自然是要操作对应的对象（Object）而非当前的线程（Thread）\n因为Sleep是让当前线程暂停执行，不涉及到对象类，所以也不需要对象锁\n\n\nsleep常用于暂停执行，wait方法常用于线程间交互/通信\nwait方法被调用后，线程不会自动苏醒，需要notify方法或notifyAll方法，sleep执行完线程会自动苏醒，或者也可以使用wait(long timeout)超时后自动苏醒\n\n\n\n\n\n\n线程池创建\n\n线程池出现的原因：因为线程过多会增加创建、调度线程的开销，所以通过线程池提前创建若干线程，一方面避免了处理任务时频繁的，创建销毁线程的开销，另一方面避免了线程数量膨胀导致的过分调度问题，并且可以集中管理线程资源，提高系统稳定性\npublic class ThreadPoolExecutor extends AbstractExecutorService &#123;\n\n    &#x2F;**\n     * 核心线程数\n     * 当向线程池提交一个任务时，若线程池已创建的线程数小于corePoolSize，即便此时存在空闲线程，\n     * 也会通过创建一个新线程来执行该任务，直到已创建的线程数大于或等于corePoolSize\n     *&#x2F;\n    private volatile int corePoolSize;\n\n    &#x2F;**\n     * 最大线程数\n     * 当队列满了，且已创建的线程数小于maximumPoolSize，则线程池会创建新的线程来执行任务。\n     * 另外，对于无界队列，可忽略该参数\n     *&#x2F;\n    private volatile int maximumPoolSize;\n    &#x2F;**\n     * 线程存活保持时间\n     * 当线程池中线程数 超出核心线程数，且线程的空闲时间也超过 keepAliveTime时，\n     * 那么这个线程就会被销毁，直到线程池中的线程数小于等于核心线程数\n     *&#x2F;\n    private volatile long keepAliveTime;\n\n    &#x2F;**\n     * 任务队列\n     * 用于传输和保存等待执行任务的阻塞队列\n     *&#x2F;\n    private final BlockingQueue&lt;Runnable&gt; workQueue;\n\n    &#x2F;**\n     * 线程工厂\n     * 用于创建新线程。threadFactory 创建的线程也是采用 new Thread() 方式，threadFactory\n     * 创建的线程名都具有统一的风格：pool-m-thread-n（m为线程池的编号，n为线程池中线程的编号\n     *&#x2F;\n    private volatile ThreadFactory threadFactory;\n\n    &#x2F;**\n     * 线程饱和策略\n     * 当线程池和队列都满了，再加入的线程会执行此策略\n     *&#x2F;\n    private volatile RejectedExecutionHandler handler;\n\n    &#x2F;**\n     * 构造方法提供了多种重载，但实际上都使用了最后一个重载 完成了实例化\n     *&#x2F;\n    public ThreadPoolExecutor(int corePoolSize,\n                              int maximumPoolSize,\n                              long keepAliveTime,\n                              TimeUnit unit,\n                              BlockingQueue&lt;Runnable&gt; workQueue) &#123;\n        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n             Executors.defaultThreadFactory(), defaultHandler);\n    &#125;\n\n    public ThreadPoolExecutor(int corePoolSize,\n                              int maximumPoolSize,\n                              long keepAliveTime,\n                              TimeUnit unit,\n                              BlockingQueue&lt;Runnable&gt; workQueue,\n                              ThreadFactory threadFactory) &#123;\n        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n             threadFactory, defaultHandler);\n    &#125;\n\n    public ThreadPoolExecutor(int corePoolSize,\n                              int maximumPoolSize,\n                              long keepAliveTime,\n                              TimeUnit unit,\n                              BlockingQueue&lt;Runnable&gt; workQueue,\n                              RejectedExecutionHandler handler) &#123;\n        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n             Executors.defaultThreadFactory(), handler);\n    &#125;\n\n    public ThreadPoolExecutor(int corePoolSize,\n                              int maximumPoolSize,\n                              long keepAliveTime,\n                              TimeUnit unit,\n                              BlockingQueue&lt;Runnable&gt; workQueue,\n                              ThreadFactory threadFactory,\n                              RejectedExecutionHandler handler) &#123;\n        if (corePoolSize &lt; 0 ||\n            maximumPoolSize &lt;&#x3D; 0 ||\n            maximumPoolSize &lt; corePoolSize ||\n            keepAliveTime &lt; 0)\n            throw new IllegalArgumentException();\n        if (workQueue &#x3D;&#x3D; null || threadFactory &#x3D;&#x3D; null || handler &#x3D;&#x3D; null)\n            throw new NullPointerException();\n        this.corePoolSize &#x3D; corePoolSize;\n        this.maximumPoolSize &#x3D; maximumPoolSize;\n        this.workQueue &#x3D; workQueue;\n        this.keepAliveTime &#x3D; unit.toNanos(keepAliveTime);\n        this.threadFactory &#x3D; threadFactory;\n        this.handler &#x3D; handler;\n    &#125;\n\n    &#x2F;**\n     * 执行一个任务，但没有返回值\n     *&#x2F;\n    public void execute(Runnable command) &#123;\n        if (command &#x3D;&#x3D; null)\n            throw new NullPointerException();\n        int c &#x3D; ctl.get();\n        if (workerCountOf(c) &lt; corePoolSize) &#123;\n            if (addWorker(command, true))\n                return;\n            c &#x3D; ctl.get();\n        &#125;\n        if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;\n            int recheck &#x3D; ctl.get();\n            if (! isRunning(recheck) &amp;&amp; remove(command))\n                reject(command);\n            else if (workerCountOf(recheck) &#x3D;&#x3D; 0)\n                addWorker(null, false);\n        &#125;\n        else if (!addWorker(command, false))\n            reject(command);\n    &#125;\n\n    &#x2F;**\n     * 提交一个线程任务，有返回值。该方法继承自其父类 AbstractExecutorService，有多种重载，这是最常用的一个。\n     * 通过future.get()获取返回值（阻塞直到任务执行完）\n     *&#x2F;\n    public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123;\n        if (task &#x3D;&#x3D; null) throw new NullPointerException();\n        RunnableFuture&lt;T&gt; ftask &#x3D; newTaskFor(task);\n        execute(ftask);\n        return ftask;\n    &#125;\n\n    &#x2F;**\n     * 关闭线程池，不再接收新的任务，但会把已有的任务执行完\n     *&#x2F;\n    public void shutdown() &#123;\n        final ReentrantLock mainLock &#x3D; this.mainLock;\n        mainLock.lock();\n        try &#123;\n            checkShutdownAccess();\n            advanceRunState(SHUTDOWN);\n            interruptIdleWorkers();\n            onShutdown(); &#x2F;&#x2F; hook for ScheduledThreadPoolExecutor\n        &#125; finally &#123;\n            mainLock.unlock();\n        &#125;\n        tryTerminate();\n    &#125;\n\n    &#x2F;**\n     * 立即关闭线程池，已有的任务也会被抛弃\n     *&#x2F;\n    public List&lt;Runnable&gt; shutdownNow() &#123;\n        List&lt;Runnable&gt; tasks;\n        final ReentrantLock mainLock &#x3D; this.mainLock;\n        mainLock.lock();\n        try &#123;\n            checkShutdownAccess();\n            advanceRunState(STOP);\n            interruptWorkers();\n            tasks &#x3D; drainQueue();\n        &#125; finally &#123;\n            mainLock.unlock();\n        &#125;\n        tryTerminate();\n        return tasks;\n    &#125;\n\n    public boolean isShutdown() &#123;\n        return ! isRunning(ctl.get());\n    &#125;\n&#125;\nThreadPoolExecutor\n\n基础\n\n继承链\n\nExecutor接口：声明了execute方法，使得用户不需要关注如何创建线程， 只需要传入实现了Runnable接口的线程任务类\nExecutorService接口：声明了执行一批异步生成Future的方法；声明了管控线程池的方法（关闭等方法）\nAbstractExecutorService：将执行任务的流程串联起来，保证下层的实现只需关注一个执行任务的方法\nThreadPoolExecutor：实现复杂的运行部分（维护自身的生命周期、管理线程和任务）\n\n\n参数：corePoolSize、maximumPoolSize、keepAliveTime、unit、workQueue、threadFactory、handler\n\n运行状态\n\n\n\n\n池内线程创建过程：首先使用工厂函数针对新任务创建线程直到数量达到核心线程池数量，然后将新任务存储在工作队列中，待工作队列满了之后创建一个新线程来处理任务（没任务一段时间后会被销毁），直到总线程数量达到最大线程池数量后，后续的新任务根据拒绝策略来确定对应操作\n\nworker进程实现了Runnable接口继承自AQS，持有一个线程thread（通过TheradFactory来创建），一个初始化任务firstTask\n确定线程状态：线程池通过一张hash表来保存线程的引用，通过增删引用来控制线程的生命周期。因为使用了AQS锁来实现独占锁，根据独占锁的状态反应线程现在的执行状态\nworker线程增加（addWorker方法）：增加一个线程，有两个参数firstTask和core，根据core的值判断现有线程数在哪个区见\nworker线程的回收：线程池中的回收依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程倍JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可\nworker线程执行任务：worker类中的run方法调用了runWorker方法来执行任务，轮询获取任务，再获取锁，直到没有任务\n\n\n任务与线程的匹配：通过生产者消费者模型，缓存任务，供线程池针对任务进行线程的分配\n\n线程池使用AtomicInteger变量维护：运行状态（runState）和线程数量（workerCount）\n&#x2F;&#x2F;高三位保存runState，低29位保存workerCount\nprivate final AtomicInteger ctl &#x3D; new AtomicInteger(ctlOf(RUNNING, 0));\n\n\n\n\n示例\n\nExecutors 类 通过 ThreadPoolExecutor 封装了 4 种常用的线程池：CachedThreadPool，FixedThreadPool，ScheduledThreadPool 和 SingleThreadExecutor。其功能如下。\n\nScheduledThreadPool：适用于执行 延时 或者 周期性 任务。\n\nFixedThreadPool：它的核心线程数和最大线程数是一样的，所以可以把它看作是固定线程数的线程池，它的特点是线程池中的线程数除了初始阶段需要从 0 开始增加外，之后的线程数量就是固定的，就算任务数超过线程数，线程池也不会再创建更多的线程来处理任务，而是会把超出线程处理能力的任务放到任务队列中进行等待。而且就算任务队列满了，到了本该继续增加线程数的时候，由于它的最大线程数和核心线程数是一样的，所以也无法再增加新的线程了。\n\nCachedThreadPool：可以称作可缓存线程池，它的特点在于线程数是几乎可以无限增加的（实际最大可以达到 Integer.MAX_VALUE，为 2^31-1，这个数非常大，所以基本不可能达到），而当线程闲置时还可以对线程进行回收。也就是说该线程池的线程数量不是固定不变的，当然它也有一个用于存储提交任务的队列，但这个队列是 SynchronousQueue，队列的容量为0，实际不存储任何任务，它只负责对任务进行中转和传递，所以效率比较高。适用于执行大量短生命周期的异步任务。\n\nSingleThreadExecutor：它会使用唯一的线程去执行任务，原理和 FixedThreadPool 是一样的，只不过这里线程只有一个（单线程），如果线程在执行任务的过程中发生异常，线程池也会重新创建一个线程来执行后续的任务。这种线程池由于只有一个线程，所以非常适合用于所有任务都需要按被提交的顺序依次执行的场景，而前几种线程池不一定能够保障任务的执行顺序等于被提交的顺序，因为它们是多线程并行执行的。\n\nSingleThreadScheduledExecutor：它实际和 ScheduledThreadPool 线程池非常相似，它只是 ScheduledThreadPool 的一个特例，内部只有一个线程。\n\n\n\nThreadPoolExecutor\npublic class ThreadPoolExecutorDemo &#123;\n\n    public static void main(String[] args) &#123;\n        &#x2F;&#x2F; 创建一个线程池，包含5个线程\n        ThreadPoolExecutor executor &#x3D; (ThreadPoolExecutor) Executors.newFixedThreadPool(5);\n        &#x2F;&#x2F; 提交10个任务给线程池执行\n        for (int i &#x3D; 0; i &lt; 10; i++) &#123;\n            Runnable worker &#x3D; new WorkerThread(&quot;Task &quot; + i);\n            executor.execute(worker);\n        &#125;\n        &#x2F;&#x2F; 关闭线程池\n        executor.shutdown();\n        while (!executor.isTerminated()) &#123;\n            &#x2F;&#x2F; 等待线程池中的任务执行完毕\n        &#125;\n        System.out.println(&quot;All tasks have been completed.&quot;);\n    &#125;\n&#125;\n\nclass WorkerThread implements Runnable &#123;\n    private String taskName;\n\n    public WorkerThread(String taskName) &#123;\n        this.taskName &#x3D; taskName;\n    &#125;\n\n    @Override\n    public void run() &#123;\n        System.out.println(Thread.currentThread().getName() + &quot; executing &quot; + taskName);\n        try &#123;\n            &#x2F;&#x2F; 模拟执行任务需要的时间\n            Thread.sleep(1000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\nExecutor框架的Executors\n&#x2F;&#x2F;1.创建\nExecutorService service &#x3D; Executors.newFixedThreadPool(10);\n&#x2F;&#x2F;2.执行\nservice.execute(new MyThread());\nservice.execute(new MyThread());\nservice.execute(new MyThread());\nservice.execute(new MyThread());\n&#x2F;&#x2F;3.关闭连接\nservice.shutdown();\n\n\n\n\n\n2.互斥2.1synchronized\n粒度：对象锁（this、newObject）、局部代码锁、类锁（Demo.class）\n\n静态synchronized方法和非静态synchronized方法之间的调用不互斥（一个是类的锁一个是实例对象的锁）\n\n尽量不要使用synchronized(String a)，因为JVM中，字符串常量池具有缓存功能\n\n构造方法不能使用 synchronized 关键字修饰，因为构造方法本身就属于线程安全的，不存在同步的构造方法\npublic synchronized void add(int value) &#123;&#125; &#x2F;&#x2F;方法\nsynchronized (this)&#123;&#125; &#x2F;&#x2F;局部代码块\nsynchronized (obj1) &#123;&#125; &#x2F;&#x2F;内部的一个对象 Object obj1 &#x3D; new Object()\nsynchronized (Wallet.class) &#x2F;&#x2F;类锁\n\n\n锁类别：偏向锁（一个）、轻量级锁（不竞争）、重量级锁（竞争）\n\n通过MarkWork字段辨别锁的类别，新创建的对象处于无锁状态，随后自动变为偏向锁状态，线程可以通过CAS操作竞争偏向锁（单进程使用），竞争成功则执行完任务，执行完后锁会继续保持偏向锁状态，竞争失败则请求线程将锁升级为轻量级锁\n\n升级过程先暂停（JVM的STW）持有锁进程，如其在运行synchronized代码，则升级为轻量级锁（线程交叉使用不存在竞争），否则将MarkWork设置为无锁状态（偏向锁升级代价大，不如直接升级为轻量级锁）\n\n在轻量级锁状态，如果通过（自适应）自旋方式循环执行CAS操作请求锁达到一定数量仍未获得时，就申请升级为重量级锁，唤醒等待重量级锁的进程\n\n锁升级：通过CAS操作，持有锁的线程继续执行，请求锁的线程负责升级任务，包括创建Monitor锁，将自己放到Monitor锁的_cxq中，调用OS系统调用来阻塞自己\n\n解锁：先检查锁标志位，如果没有升级，只需要使用CAS操作解锁即可；如果已升级为重量级锁，那么持有轻量级锁的线程去唤醒等待重量级锁的进程\n\nMonitor锁（hotspot）：\nclass ObjectlMonitor &#123;\n    void * volatile _object;&#x2F;&#x2F;该Monitor锁所属的对象\n    void * volatile _owner;&#x2F;&#x2F;获取到该Monitor锁的线程\n    ObjectWaiter * volatile _cxq;&#x2F;&#x2F;没有获取到锁的线程暂时加入_cxq\n    ObjectWaiter * volatile _EntryList;&#x2F;&#x2F;存储等待被唤醒的线程\n    &#x2F;&#x2F;存储调用了wait()的线程，用来实现wait()、notify()线程同步功能\n\t\t&#x2F;&#x2F;wait、notify等方法也依赖于monitor对象\n    ObjectWaiter * volatile _waitSet;\n    &#x2F;&#x2F;...\n&#125;\n\n\n多个对象通过CAS操作（底层为cmpxchg指令）竞争_owner字段，没有获取到锁的线程加入_cxq队列中等待，待锁释放先通知_EntryList队列中的线程通过CAS操作竞争_owner字段，如果_EntryList队列为空，则将_cxq队列中移到_EntryList队列（一个负责存，一个负责取，减少并发冲突）\n内核线程执行上述步骤没得到锁时，会调用Linux的park函数自行阻塞；阻塞线程获取到锁之后，调用unpark函数来取消对应内核线程的阻塞状态\n\n\n\n\n\n\n锁优化\n\n锁消除：虚拟机在执行JIT编译时，有时会根据对代码的分析(逃逸分析)，去掉某些没有必要的锁（局部变量的锁）\n锁粗化：虚拟机在执行JIT编译时，有时会扩大加锁范围，将对多个小范围代码的加锁，合并一个对大范围代码的加锁（如for循环内的锁）\n\n\n\n2.2锁\n锁类别\n\n可重入锁：可以被同一个线程多次加锁的锁，即在锁没有解锁前，再次加锁，通过变量记录重入次数，JUC提供的锁都是可重入锁\n公平锁：线程会按照请求的先后顺序获得锁。synchronized是非公平锁（新请求可插队），ReentrantLock既支持公平锁也支持非公平锁，默认为非公平锁，通过在构造函数中添加true可声明为公平锁。非公平锁的性能比公平锁更好。ReentrantLock通过AQS（抽象队列同步器）来排队等待锁的线程\n可中断锁：对于synchronized来说，一个线程在阻塞等待锁时，是无法响应中断的，即不可被打断。JUC Lock接口提供了lockInterruptibly()函数，支持可响应中断的方式来请求锁（用于线程池，关闭正在执行的线程）\n非阻塞锁：JUC提供了tryLock()函数，支持非阻塞的方式获取锁，如果锁已经被其他线程获取，则不阻塞直接返回\n可超时锁：JUC提供了带参数的tryLock()函数，支持非阻塞获取锁的同时设置超时时间，tryLock()也可被中断，主要用于对响应时间敏感的系统，如Tomcat\n读写锁：为了提到并发度，可多次获得读锁，JUC提供了ReadWrite接口和其实现类ReetrantReadWriteLock。读锁是一种共享锁，可以被多个线程同时获取，写锁是排他锁，同时只能被一个线程获取，读写锁之间也是排他的（写优先）\n乐观读锁：StampedLock是对ReadWriteLock的进一步优化，提供了读锁、写锁和乐观读锁，其中的读锁和写锁与ReadWriteLock中的类似，乐观读锁是对读锁的进一步优化，在读多写少的时候，大部分读操作都不会被写操作干扰，因此连读锁都不需要加，只有验证真正有被写操作干扰的情况下，再加读锁即可\n\n\nAQS\n\n抽象队列同步器，与synchronized底层的ObjectMonitor类相似，都实现了排队线程、阻塞线程和唤醒线程等功能，但只有一个队列，且基于Java语言实现，是锁实现的原理，在ReentrantLock类有体现（Sync、NofairSync、FairSync都继承自AbstractQueuedSynchronizer）\n\nCLH(Craig,Landin,and Hagersten) 队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。在 CLH 同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）\n\n方法\n\nAQS定义了8个模板方法，可以分为两组：独占模式（Lock）和共享模式（Semaphore）\n&#x2F;&#x2F;独占模式\npublic final void acquire(int arg) &#123; ...&#125;\npublic final void acquirelnterruptibly(int arg)throws InterruptedException &#123; ...&#125;\npublic final boolean tryAcquireNanos(int arg, long nanosTimeout)throws InterruptedException &#123; ...&#125;\npublic final boolean release(int arg) &#123; ...&#125;\n&#x2F;&#x2F;共享模式\npublic final void acquireShared(int arg) &#123; ...&#125;\npublic final void acquireSharedInterruptibly(int arg)throws InterruptedException &#123; ...&#125;\npublic final boolean tryAcquireSharedNanos(int arg, long nanosTimeout)throws InterruptedException &#123; ...&#125;\npublic final boolean releaseShared(int arg) &#123; ...&#125;\nAQS提供了4个抽象方法：没有声明为abstract是为了减少代码量，更灵活编写代码\n&#x2F;&#x2F;独占模式\nprotected boolean tryAcquire(int arg)&#123;throw new UnsupportedOperationException();&#125;\nprotected boolean tryRelease(int arg)&#123;throw new UnsupportedOperationException();&#125;\n&#x2F;&#x2F;共享模式\nprotected int tryAcquireShared(int arg) &#123;throw new UnsupportedOperationException();&#125;\nprotected boolean tryReleaseShared(int arg) &#123;throw new UnsupportedOperationException();&#125;\n\n\n\n\nReetrantLock：定义了两个继承自AQS的子类：NofairSync和FairSync，分别用来实现非公平锁和公平锁，并且因为底层释放锁的逻辑相同，故又抽象出公共父类Sync\n\nSync，NofairSync和FairSync（根据构造函数的不同使用不同的Sync实现）\npublic class ReentrantLock implements Lock, java.io.Serializable &#123;\n\t  private final sync sync;\n\t  \n\t  abstract static class Sync extends AbstractQueuedSynchronizer &#123; ...&#125;\n\t  static final class NonfairSync extends Sync &#123; ...&#125;\n\t\tstatic final class FairSync extends Sync &#123; ...&#125;\n\t    \n\t\tpublic ReentrantLock()&#123;\n\t\t\t\tsync &#x3D; new NonfairSync();\n\t   &#125;\n\t\tpublic ReentrantLock(boolean fair) &#123;\n\t\t\t\tsync &#x3D; fair ? new FairSync() : new NonfairSync();\n\t   &#125;\n\t        \n\t\tpublic void lock()&#123;sync.acquire(1);&#125;\n\t\tpublic void unlock() &#123;sync.release(1);&#125;\n\t\t&#x2F;&#x2F;...省略其他方法...\n\t    &#125;\n&#125;\nacquire：改state值，是否查看等待队列（公平/不公平），addWaiter（自旋+CAS）、acquireQueued（唤醒后竞争锁）\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;Sync&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic final void acquire(int arg) &#123;\n    &#x2F;&#x2F;1.调用tryAcquire去竞争获取锁，如果成功，则直接返回\n    &#x2F;&#x2F;2.调用addWaiter，将线程包裹为Node节点放入等待队列的尾部\n    &#x2F;&#x2F;3.调用acquireQueued阻塞当前线程，\n    if ( !tryAcquire(arg) &amp;&amp; acquireQueued( addWaiter(Node.EXCLUSIVE), arg ) )\n        &#x2F;&#x2F;用来处理中断，如果在等待锁的过程中，被其它线程中断，\n        &#x2F;&#x2F;则在获取锁之后，将现成的中断标记设置为true\n        selfInterrupt();\n&#125;\n\nstatic final class NonfairSync extends Sync &#123;\n    &#x2F;&#x2F;尝试获取锁，成功返回true，失败返回false。AQS用于实现锁时，acquires&#x3D;1\n    protected final boolean tryAcquire(int acquires)&#123;\n        final Thread current &#x3D; Thread.currentThread();\n        int c &#x3D; getState(); &#x2F;&#x2F;获取state值\n        if (c &#x3D;&#x3D; 0)&#123;&#x2F;&#x2F;锁没有被其他线程占用\n            if (compareAndSetstate(0,acquires)) &#123; &#x2F;&#x2F; CAS设置state值为1\n                setExclusiveOwnerThread(current);&#x2F;&#x2F; 设置exclusiveownerThread\n                return true;&#x2F;&#x2F;获取锁成功\n            &#125;\n        &#125;else if (current &#x3D;&#x3D; getExclusiveOwnerThread())&#123;&#x2F;&#x2F; 锁已被自己占用，可重入\n            int nextc &#x3D; c + acquires; &#x2F;&#x2F; state+1\n            if (nextc &lt; 0)&#x2F;&#x2F;重入次数太多，超过了int最大值，溢出为负数，此情况罕见\n                throw new Error(&quot;Maximum lock count exceeded&quot;);\n            setState(nextc); &#x2F;&#x2F; state&#x3D;state+1,state记录重入的次数，解锁的时候用\n            return true;&#x2F;&#x2F;获取锁成功\n        &#125;\n        return false;&#x2F;&#x2F;获取锁失败\n    &#125;\n&#125;\nstatic final class FairSync extends Sync &#123;\n    protected final boolean tryAcquire(int acquires) &#123;\n        final Thread current &#x3D; Thread.currentThread();\n\t\t\t\tint c &#x3D; getState();\n        if (c &#x3D;&#x3D; 0)&#123;\n            if (!hasQueuedPredecessors() &amp;&amp;&#x2F;&#x2F;等待队列中没有线程时才获取锁\n                compareAndSetstate(0, acquires))&#123;\n                setExclusiveownerThread(current);\n                return true;\n            &#125;\n        &#125;else if (current &#x3D;&#x3D; getExclusiveOwnerThread())&#123;\n            int nextc &#x3D; C + acquires;\n            if (nextc &lt; 0)\n                throw new Error(&quot;Maximum lock count exceeded&quot;);setState(nextc);\n            return true;\n        &#125;\n        return false;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;通过自旋和CAS操作解决往链表尾部添加节点和特殊处理链表为空所存在的线程安全问题\nprivate Node addWaiter(Node mode)&#123;\n    Node node &#x3D; new Node(Thread.currentThread(), mode);\n    &#x2F;&#x2F;自旋执行CAS操作，直到成功为止\n    for (;;) &#123;\n        Node t &#x3D; tail;\n        if (t &#x3D;&#x3D; null) &#123;&#x2F;&#x2F;链表为空，添加虚拟头节点\n            &#x2F;&#x2F;CAS操作解决添加虚拟头节点的线程安全问题\n            if (compareAndSetHead(null, new Node()))\n                tail &#x3D; head;\n        &#125;else &#123;&#x2F;&#x2F;链表不为空\n            node.prev &#x3D; t;\n            &#x2F;&#x2F;CAS操作解决了同时往链表尾部添加节点时的线程安全问题\n            if (compareAndSetTail(t, node)) &#123;\n                t.next &#x3D; node;\n                return t;\n            &#125;\n        &#125;\n    &#125;\n    return node;\n&#125;\n\n&#x2F;&#x2F;主要有两部分逻辑，使用tryAcquire函数来竞争锁和使用park()函数来阻塞线程\n&#x2F;&#x2F;采用for循环来交替执行这两个逻辑，为了在线程被唤醒后，并不是直接获取锁，\n&#x2F;&#x2F;而是重新竞争锁，如果竞争失败，则需要再次被阻塞\nfinal boolean acquireQueued(final Node node, int arg) &#123;\n    boolean failed &#x3D; true;\n    try &#123;\n        boolean interrupted &#x3D; false;\n        for (;;)&#123;\n            &#x2F;&#x2F;使用tryAcquire()函数来竞争锁\n            final Node p &#x3D; node.predecessor();\n            if (p &#x3D;&#x3D; head &amp;&amp; tryAcquire(arg)) &#123;\n                setHead(node);\n                p.next &#x3D; null; &#x2F;&#x2F; help GC\n                failed &#x3D; false;\n                return interrupted;\n            &#125;\n            &#x2F;&#x2F;调用park()函数来阻塞线程，等待其他线程调用unpark()函数唤醒\n            if (parkAndCheckInterrupt()) interrupted &#x3D; true;\n        &#125;\n    &#125;finally &#123;\n        if (failed) cancelAcquire(node);\n    &#125;\n&#125;\nprivate final boolean parkAndChecklnterrupt() &#123;\n    LockSupport.park(this);&#x2F;&#x2F;底层也是调用JVM提供的native park()函数来实现\n    return Thread.interrupted();\n&#125;\nrelease：sync和nofairsync的实现相同，state-1→setExclusiveownerThread(state==0)-&gt;setState(state != 0有重入)\npublic final boolean release(int arg) &#123;\n    &#x2F;&#x2F;tryRelease释放锁\n    if (tryRelease(arg)) &#123;\n        Node h &#x3D; head;\n        if (h !&#x3D; null &amp;&amp; h.waitStatus !&#x3D; 0)\n            unparkSuccessor(h);&#x2F;&#x2F;内部调用unpark()函数，唤醒链表首节点对应的线程\n        return true;\n    &#125;\n    return false;\n&#125;\n&#x2F;&#x2F;公平锁和非公平锁的实现相同\nstatic final class Sync extends AbstractQueuedSynchronizer &#123;\n    &#x2F;&#x2F;释放锁，成功返回true，失败返回false。AQS用于实现锁时，releases&#x3D;1\n    protected final boolean tryRelease(int releases)&#123;\n        int c &#x3D; getState() - releases; &#x2F;&#x2F;state-1\n        &#x2F;&#x2F;不持有锁的线程去释放锁，抛出异常\n        if (Thread.currentThread() !&#x3D; getExclusiveownerThread())\n            throw new lllegalMonitorStateException();\n        boolean free &#x3D; false;\n        if (c &#x3D;&#x3D; 0) &#123; &#x2F;&#x2F;stat-1之后为0，解锁\n            free &#x3D; true;\n            setExclusiveownerThread(null);\n        &#125;\n        setState(c); &#x2F;&#x2F;state-1之后不为0，说明锁被重入多次，还不能解锁。\n        return free;\n    &#125;\n&#125;\n中断机制：lockInterruptibly→acquirelnterruptibly→doAcquireInterruptibly（类似于acquireQueued，但对中断的响应处理不同）\npublic void lockInterruptibly() throws InterruptedException &#123;\n    sync.acquirelnterruptibly(1);\n&#125;\n&#x2F;&#x2F;如果线程中断则抛出异常，否则。调用tryAcquire()竞争获取锁，\n&#x2F;&#x2F;获得失败后调用doAcquireInterruptibly\npublic final void acquirelnterruptibly(int arg) throws InterruptedException &#123;\n    if (Thread.interrupted()) throw new InterruptedException();\n    if (!tryAcquire(arg)) doAcquireInterruptibly(arg);\n&#125;\n&#x2F;&#x2F;与acquireQueued()函数的代码非常相似，唯一区别是对中断的响应处理不同\nprivate void doAcquireInterruptibly(int arg) throws InterruptedException &#123;\n    final Node node &#x3D; addWaiter(Node.EXCLUSIVE);\n    boolean failed &#x3D; true;\n    try &#123;\n        for(;;)&#123;\n            final Node p &#x3D; node.predecessor();\n            if (p &#x3D;&#x3D; head &amp;&amp; tryAcquire(arg)) &#123;\n                setHead(node);\n                p.next &#x3D; null; &#x2F;&#x2F; help GC\n                failed &#x3D; false;\n                return;\n            &#125;\n            if (parkAndChecklnterrupt())\n                throw new lnterruptedException(); &#x2F;&#x2F;区别:抛出异常! 阻止等待锁\n        &#125;\n    &#125;finally &#123;\n        if (failed) \n            cancelAcquire(node);&#125;\n&#125;\n超时机制：tryLock→tryAcquireNanos→doAcquireNanos（在acquireInterruptibly的基础上增加了超时机制）\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;ReentrantLock&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic boolean tryLock(long timeout,TimeUnit unit)\n    throws InterruptedException &#123;\n    return sync.tryAcquireNanos(1 , unit.toNanos(timeout));\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;AbstractQueueSynchronizer&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123;\n    &#x2F;&#x2F;如果线程被中断则抛出异常\n    if (Thread.interrupted()) throw new InterruptedException();\n    &#x2F;&#x2F;调用tryAcquire竞争获取锁，成功则返回，失败则调用doAcquireNanos\n    return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);\n&#125;\n&#x2F;&#x2F;在acquireInterruptibly函数基础上，添加了对超时的处理机制\nprivate boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123;\n    if (nanosTimeout &lt;&#x3D; 0L) return false;\n    final long deadline &#x3D; System.nanoTime() + nanosTimeout;\n    final Node node &#x3D; addWaiter(Node.EXCLUSIVE);\n    boolean failed &#x3D; true;\n    try &#123;\n        for (;;)&#123;\n            final Node p &#x3D; node.predecessor();\n            if (p &#x3D;&#x3D; head &amp;&amp; tryAcquire(arg))&#123;\n                setHead(node);\n                p.next &#x3D; null; &#x2F;&#x2F; help GC\n                failed &#x3D; false;\n                return true;\n            &#125;\n            nanosTimeout &#x3D; deadline - System.nanoTime();\n            if (nanosTimeout &lt;&#x3D; 0L) return false;\n            if(nanosTimeout &gt; spinForTimeoutThreshold)&#x2F;&#x2F;不着急阻塞，先自旋—下\n                LockSupport.parkNanos(this, nanosTimeout);&#x2F;&#x2F;超时阻塞\n            if (Thread.interrupted()) throw new InterruptedException();\n        &#125;\n    &#125;finally &#123;\n        if (failed) cancelAcquire(node);\n    &#125;\n&#125;\n&#x2F;&#x2F;为了支持超时阻塞，在阻塞线程时，doAcquireNanos调用parkNanos函数\n&#x2F;&#x2F;synchronized中park函数实现如下，parkNanos只将其中的pthread_cond_wait换成了\n&#x2F;&#x2F;pthread_cond_timewait，便可实现超时等待。\npthread_mutex_t mutex &#x3D; PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t cond &#x3D; PTHREAD_COND_INITIALIZER;\nvoid park() &#123;\n    pthread_mutex_lock(&amp;mutex);\n    pthread_cond_wait(&amp;cond,&amp;mutex);&#x2F;&#x2F;阻塞等待其他线程发送信号\n    pthread_mutex_unlock(&amp;mutex);\n&#125;\n\n\nReadWriteLock：读锁不可以转成写锁，但在写锁释放前加读锁，在写锁释放后线程持有的锁自动从写锁降级为读锁\n\nstate：低16位写锁、高16位读锁\n\n低16位表示，0表示没有加写锁，1表示已经加写锁，大于1表示写锁的可重入次数\n高16位表示，0表示没有加读锁，1表示已经加读锁，大于1表示读锁总共被获取了多少次（每个线程对读锁重入的次数相加），使用ThreadLocal变量存储重入次数\n\n\n写锁\nprotected final boolean tryAcquire(int acquires) &#123;\n    Thread current &#x3D; Thread.currentThread();\n    int c &#x3D; getState();\n    int w &#x3D; exclusiveCount(c);&#x2F;&#x2F;高16位的值，也就是写锁的加锁情况\n    &#x2F;&#x2F;1.已经加读锁或写锁（state!&#x3D;0）\n    if (c !&#x3D; 0) &#123;\n        &#x2F;&#x2F; 已加读锁(w&#x3D;&#x3D;0)或者当前加写锁的线程不是自己\n        if (w &#x3D;&#x3D; 0 || current !&#x3D; getExclusiveOwnerThread())\n            return false;&#x2F;&#x2F;去排队\n        if (w + exclusiveCount(acquires) &gt; MAX_COUNT)\n            throw new Error(&quot;Maximum lock count exceeded&quot;);\n        &#x2F;&#x2F; 获取到了写锁\n        setState(c + acquires);&#x2F;&#x2F;更新写锁的重入次数\n        return true;\n    &#125;\n    &#x2F;&#x2F;2.没有加锁（state&#x3D;0）\n    if (writerShouldBlock() || !compareAndSetState(c, c + acquires))\n        return false;&#x2F;&#x2F;去排队\n    setExclusiveOwnerThread(current);\n    return true;&#x2F;&#x2F;获取了锁\n&#125;\n&#x2F;&#x2F;writerShouldBlock函数控制锁是否为公平锁，在state&#x3D;0，也就是没有加读锁和\n&#x2F;&#x2F;写锁的情况下，如果writerShouldBlock返回值为true，那么线程不尝试竞争锁，而是直接去排队，\n&#x2F;&#x2F;如果writerShouldBlock返回值是false，那么线程尝试竞争锁，失败再去排队。\n&#x2F;&#x2F;对于非公平锁，总是返回false，对于公平锁如果等待队列中有线程，则返回true\n读锁\npublic final void acquireShared(int arg) &#123;\n    if (tryAcquireShared(arg) &lt; 0)&#x2F;&#x2F;竞争读锁\n        doAcquireShared(arg);&#x2F;&#x2F;竞争失败去排队\n&#125;\n&#x2F;&#x2F;返回-1表示竞争锁失败，返回1表示竞争锁成功\nprotected final int tryAcquireShared(int unused) &#123;\n    Thread current &#x3D; Thread.currentThread();\n    int c &#x3D; getState();\n    &#x2F;&#x2F;一些优化代码\n    return fullTryAcquireShared(current);\n&#125;\nfinal int fullTryAcquireShared(Thread current) &#123;\n    HoldCounter rh &#x3D; null;\n    &#x2F;&#x2F;如果state没加锁或者是加了读锁，那么线程会通过CAS操作改变state值来竞争锁;\n    &#x2F;&#x2F;如果其他线程也在竟争读锁，并且竞争成功，那么此线程就会竟争失败;\n    &#x2F;&#x2F;于是，此线程就要自旋(for循环)再次尝试去竞争读锁。\n    for (;;) &#123;\n        int c &#x3D; getState();\n        if (exclusiveCount(c) !&#x3D; 0) &#123;&#x2F;&#x2F;已加写锁\n            &#x2F;&#x2F;如果加写锁的线程不是此线程，那么读锁也加不成，直接返回-1\n            &#x2F;&#x2F;否则，读写锁支持锁降级，加了写锁的线程可以再加读锁\n            if (getExclusiveOwnerThread() !&#x3D; current)\n                return -1;\n        &#125; \n        &#x2F;&#x2F;理论上讲，如果没有加写锁，不管有没有加读锁，都可以去竞争读锁了，\n        &#x2F;&#x2F;毕竟读锁是共享锁。但是，存在两个特殊情况:\n        &#x2F;&#x2F;1.对于公平锁来说，如果等待队列不为空，并且当前线程没有持有读锁(重入加\n        &#x2F;&#x2F;锁)，那么，线程就要去排队。\n        &#x2F;&#x2F;2.对于非公平锁来说，如果等待队列中队首线程(接下来要被唤醒的）是写线\n        &#x2F;&#x2F;程，那么，线程就要去排队。这样做是为了避免请求写锁的线程迟迟获取不\n        &#x2F;&#x2F;到写锁。\n        else if (readerShouldBlock()) &#123;&#x2F;&#x2F;上述1和2情况在此时返回true      \n            if (readHolds.get().count &#x3D;&#x3D; 0)&#x2F;&#x2F;此线程没有持有读锁，不能重入\n                return -1;\n            &#x2F;&#x2F;以下是对上述代码中readHolds的解释:readHolds是ThreadLocal变量，保存\n            &#x2F;&#x2F;跟这个线程的读锁重入次数。如果重入次数为0，表示没有加读锁，返回-1去\n            &#x2F;&#x2F;排队。如果重入次数大于等于0，表示已加读锁，可以继续重入，不用排队。\n        &#125;\n        if (sharedCount(c) &#x3D;&#x3D; MAX_COUNT)\n            throw new Error(&quot;Maximum lock count exceeded&quot;);\n        &#x2F;&#x2F;CAS竞争读锁，此时有可能还有其他线程在竞争读锁或写锁\n        if (compareAndSetState(c, c + SHARED_UNIT)) &#123;&#x2F;&#x2F;SHARED_UNIT&#x3D;1&lt;&lt;16\n            &#x2F;&#x2F;竞争读锁成功\n            readHolds.get().count++;&#x2F;&#x2F;更新线程重入次数\n            return 1;&#x2F;&#x2F;成功获取读锁\n        &#125;\n    &#125;\n&#125;\n&#x2F;&#x2F;负责排队和等待唤醒，与之前的acquireQueued有两个不同\nprivate void doAcquireShared(int arg) &#123;\n    final Node node &#x3D; addWaiter(Node.SHARED);&#x2F;&#x2F;一：标记此线程等待的是共享锁\n    boolean failed &#x3D; true;\n    try &#123;\n        boolean interrupted &#x3D; false;\n        for (;;) &#123;\n            final Node p &#x3D; node.predecessor();\n            if (p &#x3D;&#x3D; head) &#123;\n                int r &#x3D; tryAcquireShared(arg);\n                if (r &gt;&#x3D; 0) &#123;\n                    &#x2F;&#x2F;区别二：如果下一个节点对应的线程也在等待读锁，那么顺道唤醒它\n                    &#x2F;&#x2F;线程获取到读锁之后，如果下一个节点对应的线程也在等待读锁，\n                    &#x2F;&#x2F;那么也会被唤醒。下一个节点对应的线程获取到读锁之后，又会去唤醒\n                    &#x2F;&#x2F;下下个节点对应的线程(如果下下个节点对应的线程也在等待读锁的\n                    &#x2F;&#x2F;话)。唤醒操作一直传播下去，直到遇到等待写锁的线程为止。\n                    setHeadAndPropagate(node, r);\n                    p.next &#x3D; null; &#x2F;&#x2F; help GC\n                    if (interrupted)\n                        selfInterrupt();\n                    failed &#x3D; false;\n                    return;\n                &#125;\n            &#125;\n            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;\n                parkAndCheckInterrupt())\n                interrupted &#x3D; true;\n        &#125;\n    &#125; finally &#123;\n        if (failed)\n            cancelAcquire(node);\n    &#125;\n&#125;\n\n\nReentrantReadWriteLock：readerLock、writerLock、Sync、FairSync、NonfairSync\npublic class ReentrantReadWriteLock\n    implements ReadWriteLock, java.io.Serializable &#123;\n    &#x2F;&#x2F;内部类提供实现，使用NonfairSync和FairSync来编程实现读锁（ReadLock）和\n    &#x2F;&#x2F;写锁（WriteLock），均实现了Lock接口、相同的AQS、Lock接口中的所有加解锁函数\n    private final ReentrantReadWriteLock.ReadLock readerLock;\n    private final ReentrantReadWriteLock.WriteLock writerLock;\n    final Sync sync;&#x2F;&#x2F;执行所有同步机制\n    public ReentrantReadWriteLock() &#123;\n        this(false);\n    &#125;\n    public ReentrantReadWriteLock(boolean fair) &#123;\n        sync &#x3D; fair ? new FairSync() : new NonfairSync();\n        readerLock &#x3D; new ReadLock(this);\n        writerLock &#x3D; new WriteLock(this);\n    &#125;\n    public ReentrantReadWriteLock.WriteLock writeLock() &#123; return writerLock; &#125;\n    public ReentrantReadWriteLock.ReadLock  readLock()  &#123; return readerLock; &#125;\n    &#x2F;&#x2F;AQS的子类NonfairSync和FairSync的公共父类\n    abstract static class Sync extends AbstractQueuedSynchronizer &#123;\n        abstract boolean readerShouldBlock();&#x2F;&#x2F;区分公平锁和非公平锁\n        abstract boolean writerShouldBlock();&#x2F;&#x2F;区分公平锁和非公平锁\n        &#x2F;&#x2F;以下为AQS模板方法的抽象方法的实现\n        protected final boolean tryRelease(int releases) &#123;&#125;\n        protected final boolean tryAcquire(int acquires) &#123;&#125;\n        protected final boolean tryReleaseShared(int unused) &#123;&#125;\n        protected final int tryAcquireShared(int unused) &#123;&#125;\n\n        final boolean tryWriteLock() &#123;&#125;\n        final boolean tryReadLock() &#123;&#125;      \n    &#125;\n    static final class NonfairSync extends Sync &#123;\n        final boolean writerShouldBlock() &#123;return false; &#125;\n        final boolean readerShouldBlock() &#123;return apparentlyFirstQueuedIsExclusive();&#125;\n    &#125;\n    static final class FairSync extends Sync &#123;\n        final boolean writerShouldBlock() &#123;return hasQueuedPredecessors();&#125;\n        final boolean readerShouldBlock() &#123;return hasQueuedPredecessors();&#125;\n    &#125;\n&#125;\nStampedLock：在读写锁的基础上提供了乐观读锁。在读多写少的情况下，大部分操作都不会被写操作干扰，只有在真正被干扰的情况下再加读锁重复执行读操作\n\n不可重入且不支持条件变量Condition，没有实现Lock和ReadWriteLock接口，而是实现CLH锁（AQS也是基于此）\nCLH锁是对自旋锁的一种改良，是一种隐式的链表队列，StampedLock通过CLH进行线程的管理，通过同步状态值state来表示锁的状态和类型\n\n\n不可重入的原因：StampedLock在获取锁的时候会返回一个 long 型的数据戳，该数据戳用于稍后的锁释放参数，当前线程持有了锁再次获取锁还是会返回一个新的数据戳\n性能更好：StampedLock的乐观读锁允许一个写线程获取写锁，所以不会导致所有写线程阻塞，也就是当读多写少的时候，写线程有机会获取写锁，减少了线程饥饿的问题，吞吐量大大提高\n\n\n\n2.3补充\n关键字：volatile、synchronized、final\n\nvolatile：每次都去主内存读取，修改立即写入内存（c语言中的volatile的意思是禁用cpu缓存）\n解决可见性问题：用volatile修饰的变量，在编译成机器指令时，会加入特殊指令，使得CPU对此变量的修改立即写入内存，并通过其它CPU更新缓存数据\n解决有序性问题：volatile通过禁止指令重排序来解决有序性问题，并且是部分指令重排\n内存屏障：JMM定义了4个细粒度的内存屏障，其底层依赖CPU提供的内存屏障指令（StoreStore、StoreLoad、LoadLoad、LoadStore）分别禁止屏障前后的写写、写读、读读、读写操作重排\nJMM内存模型定义部分禁止重排序的方法：volatile写操作后或者volatile读操作前会添加[StoreLoad]来防止volatile写和读的重排序，一般选择添加在写后面，因为读多写少。\n\n\n解决原子性问题\n在32位计算机上，读写64位的long或double类型数据，会执行两次内存读写操作，如果用volatile修饰，那么编译器会在两次读或写之间锁定总线指令，保证变量读写的原子性，但在64位机上就不需要了\n自增语句（count++）因为是对寄存器的值进行操作，但是volatile对变量只能保证立刻写入内存让所有CPU的缓存失败，所以不能影响寄存器内的值，需要synchronized关键字\n\n\n\n\nsynchronized：通过让原本并发执行的代码串行执行，并且每次加锁和释放锁，都会同步CPU缓存和内存中的数据，可以解决可见性、有序性、原子性的问题\nfinal：JMM对final的语义做了增强，禁止编译器将构造函数中对final变量的写操作，重排序到对象引用之后，也就是禁止初始化对象（构造函数中的语句）和将内存空间赋值给引用的重排序，否则在多线程环境下，一个线程可能看到final变量的两个不同的值\n\n\nsynchronized和volatile有什么区别（互补）\n\nvolatile关键字是线程同步的轻量级实现，所以性能比synchronized好，但是volatile只能用于变量而synchronized可以修饰方法以及代码块\nvolatile关键字能保证数据的可见性，但不能保证数据的原子性，synchronized关键字两者都能保证\nvolatile关键字主要用于解决变量在多个线程之间的可见性，而synchronized关键字解决的是多个线程之间访问资源的同步性\n\n\nsynchronized和ReentrantLock有什么区别\n\n相同点：两者都是可重入锁，即线程可以再次获取自己的内部锁，不可重入的此时会产生死锁\nReentrantLock属于可中断锁，获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理；synchronized锁属于不可中断锁，一旦线程申请了锁，就只能等到拿到锁之后才能进行其他的逻辑处理\nsynchronized依赖于JVM（用户不能直接看到代码）而ReentrantLock依赖于API（lock、unlock等方法）\nReentrantLock 比 synchronized 增加了一些高级功能，如可中断锁、公平锁、可超时锁、非阻塞锁、选择性通知（锁可以绑定多个条件）\nsynchronized需要和wait、notify结合才能实现等待/通知机制，ReentrantLock类通过Condition接口和newCondition方法实现\nCondition接口可以实现多路通知功能，也就是在一个Lock对象中可以创建多个Condition实例（对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活\n在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知”\nsynchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll方法的话就会通知所有处于等待状态的线程，这样会造成很大的效率问题\n而Condition实例的signalAll()方法，只会唤醒注册在该Condition实例中的所有等待线程\n\n\n\n\n非阻塞同步\n\n悲观锁（阻塞同步）：\n\n乐观锁（非阻塞同步）：先进行操作，操作完成之后再判断操作是否成功，是否有并发问题，如果有则进行失败补偿，如果没有就算操作成功\n\n在 Java 中应用最广泛的非阻塞同步就是 CAS。从 JDK1.5 以后，可以使用 CAS 操作，该操作由 sun.misc.Unsafe 类里的 compareAndSwapInt() 和 compareAndSwapLong() 等方法实现。通常情况下 sun.misc.Unsafe 类 对于开发者是不可见的，因此，JDK 提供了很多 CAS 包装类 简化开发者的使用，如 AtomicInteger。使用 Java 自带的 Atomic 原子类，可以避免同步锁带来的并发访问性能降低的问题，减少犯错的机会\n\n\n\n\n3.同步2.1条件变量\nObject类：执行wait()或notify()前先加锁、使用while循环避免假唤醒，底层依赖ObjectMonitor\npublic class QueueCond&#123;\n  private List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();\n  private int count &#x3D; 0;\n  \n  public void put(String elem)&#123;\n    synchronized(this)&#123;&#x2F;&#x2F;加锁\n      list.add(count,elem);\n      count++;&#x2F;&#x2F;更新状态变量\n      this.notify();&#x2F;&#x2F;通知\n    &#125;\n  &#125;\n  \n  public String get()&#123;\n    synchronized(this)&#123;&#x2F;&#x2F;加锁\n      while(count &lt;&#x3D; 0)&#123;&#x2F;&#x2F;检查状态变量是否满足条件\n        try&#123;\n          this.wait();&#x2F;&#x2F;等待并释放锁，被唤醒之后重新竞争获取锁\n        &#125;catch(InterruptedException e)&#123;\n          return null;\n        &#125;\n      &#125;&#x2F;&#x2F;以下为业务逻辑\n      count--;\n      return list.get(count);\n    &#125;\n  &#125;\n&#125;\nCondition接口：使用前后需要lock和unlock，使用中要while，底层依赖ConditionObject（AQS的内部类）\npublic class QueueCondJUC&#123;\n  private List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();\n  private int count &#x3D; 0;\n  private Lock lock &#x3D; new ReentrantLock();\n  private Condition condition &#x3D; lock.newCondition();\n  \n  private void put(String elem)&#123;\n    lock.lock();&#x2F;&#x2F;加锁\n    try&#123;\n      list.add(count,elem);\n      count++;&#x2F;&#x2F;更新状态变量\n      condition.signal();&#x2F;&#x2F;通知\n    &#125;finally&#123;\n      lock.unlock();&#x2F;&#x2F;解锁\n    &#125;\n  &#125;\n  public String get()&#123;\n    lock.lock();&#x2F;&#x2F;加锁\n    try&#123;\n      while(count &lt;&#x3D; 0)&#123;&#x2F;&#x2F;检查状态变量是否满足条件\n        try&#123;\n          condition.await();&#x2F;&#x2F;等待并释放锁，被唤醒之后重新竞争获取锁\n        &#125;catch(InterruptedException e)&#123;\n          return null;\n        &#125;\n      &#125;&#x2F;&#x2F;以下为业务逻辑\n      count--;\n      return list.get(count);\n    &#125;finally&#123;\n      lock.unlock();&#x2F;&#x2F;解锁\n    &#125;\n  &#125;\n&#125;\n\n2.2信号量（Semaphore）\nSemaphore类\n\n信号量与锁的区别是：释放锁的线程必须持有锁，而信号量则不用。即没有调用acquire()函数的线程也可以直接调用release()函数，用来增加可用许可个数。此时，信号量不再是用来限制对临界区的并发访问，而是用来对共享资源的并发访问\n如果信号量中的许可个数为1，那么信号量就退化成了互斥锁；如果互斥量的许可个数大于1，信号量就可以看作是一种共享锁\n\npublic class Semaphore implements java.io.Serializable &#123;\n  &#x2F;&#x2F;第一组，默认一次获取或释放的许可（permit）个数为1\n  public void acquire() throws InterruptedException &#123;&#125;&#x2F;&#x2F;可中断获取\n  public void acquireUninterruptibly() &#123;&#125;&#x2F;&#x2F;不可中断获取\n  public boolean tryAcquire()&#123;&#125;;&#x2F;&#x2F;非阻塞获取\n  public boolean tryAcquire(long timeout, TimeUnit unit)&#x2F;&#x2F;可超时获取\n        throws InterruptedException &#123;&#125;\n  public void release()&#123;&#125;\n\n  &#x2F;&#x2F;第二组，默认制定一次获取或释放的许可个数\n  public void acquire(int permits) throws InterruptedException &#123;&#125;&#x2F;&#x2F;可中断获取\n  public void acquireUninterruptibly(int permits) &#123;&#125;&#x2F;&#x2F;不可中断获取\n  public boolean tryAcquire(int permits)&#123;&#125;;&#x2F;&#x2F;非阻塞获取\n  public boolean tryAcquire(int permits, long timeout, TimeUnit unit)&#x2F;&#x2F;可超时获取\n        throws InterruptedException &#123;&#125;\n  public void release(int permits)&#123;&#125;\n&#125;\n应用：共享资源并发访问控制\npublic class QueueSemaphore&#123;\n  private static final int Q_SIZE &#x3D; 20;\n  &#x2F;&#x2F;表示队列中的空闲位置\n  private Semaphore semaphore &#x3D; new Semaphore(Q_SIZE);\n  private list&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(Q_SIZE);\n  private int count &#x3D; 0;\n  \n  public void put(String elem)&#123;\n    &#x2F;&#x2F;当可用许可个数为0时，线程执行put函数时会阻塞在acquireUniterruptibly()函数中\n    semaphore.acquireUniterruptibly();\n    synchronized(this)&#123;\n      list.add(count, elem);\n      count++;\n    &#125;\n  &#125;\n  public String get()&#123;\n    if(count &#x3D;&#x3D; 0) return null;\n    synchronized(this)&#123;\n      if(count &#x3D;&#x3D; 0) return null;&#x2F;&#x2F;双重检测\n      String ret &#x3D; list.get(--count);\n      semaphore.release();\n      return ret;\n    &#125;\n  &#125;\n&#125;\n原理\n\n调用semaphore.acquire()，线程尝试获取许可证，如果 state &gt;= 0的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 state的值 state=state-1。如果 state&lt;0的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程\n调用semaphore.release();，线程尝试释放许可证，并使用 CAS 操作去修改 state的值 state=state+1。释放许可证成功之后，同时会唤醒同步队列中的一个线程。被唤醒的线程会重新尝试去修改 state的值 state=state-1，如果 state&gt;=0则获取令牌成功，否则重新进入阻塞队列，挂起线程。\n\npublic class Semaphore implements java.io.Serializable &#123;\n  &#x2F;&#x2F;实现AQS，模版模式\n  private final Sync sync;\n  abstract static class Sync extends AbstractQueuedSynchronizer &#123;\n    Sync(int permits) &#123;setState(permits);&#125;\n    protected final boolean tryReleaseShared(int releases) &#123;&#125;\n  &#125;\n\n  static final class NonfairSync extends Sync &#123;\n    NonfairSync(int permits) &#123;super(permits);&#125;\n    protected int tryAcquireShared(int acquires) &#123;\n      return nonfairTryAcquireShared(acquires);\n    &#125;\n  &#125;\n  \n  &#x2F;*\n  final int nonfairTryAcquireShared(int acquires) &#123;\n    for (;;) &#123;\n      int available &#x3D; getState();&#x2F;&#x2F;许可个数存放在state变量中\n      int remaining &#x3D; available - acquires;\n      if (remaining &lt; 0 ||\n          compareAndSetState(available, remaining))\n        return remaining;\n    &#125;\n  &#125;\n  *&#x2F;\n\n  static final class fairSync extends Sync &#123;\n    fairSync(int permits) &#123;super(permits);&#125;\n    protected int tryAcquireShared(int acquires) &#123;\n      for (;;) &#123;\n        if (hasQueuedPredecessors()) return -1;&#x2F;&#x2F;比NonfairSync多了这一行\n        int available &#x3D; getState();\n        int remaining &#x3D; available - acquires;\n        if (remaining &lt; 0 ||\n            compareAndSetState(available, remaining))\n          return remaining;\n      &#125;\n    &#125;\n  &#125;\n\n  public Semaphore(int permits) &#123;&#x2F;&#x2F;默认非公平模式\n    sync &#x3D; new NonfairSync(permits);\n  &#125;\n\n  public Semaphore(int permits, boolean fair) &#123;&#x2F;&#x2F;指定工作模式（公平&#x2F;非公平）\n    sync &#x3D; fair ? new FairSync(permits) : new NonfairSync(permits);\n  &#125;\n  &#x2F;&#x2F;暂时省略核心方法的实现\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;acquireUninterruptibly()函数&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F;位于Semaphore.java中\npublic void acquireUninterruptibly() &#123;\n  sync.acquireShared(1);\n&#125;\n&#x2F;&#x2F;位于AbstractQueuedSynchronizer.java中\npublic final void acquireShared(int arg) &#123;\n  if (tryAcquireShared(arg) &lt; 0)&#x2F;&#x2F;竞争获取许可，返回值&lt;0表示失败，需要排队等待许可\n    doAcquireShared(arg);&#x2F;&#x2F;排队等待许可\n&#125;\n&#x2F;&#x2F;其中tryAcquireShared()函数的代码实现位于NonfairSync和FairSync中，实现见上\n&#x2F;&#x2F;两种实现均通过自旋+CAS的方式获取许可，唯一区别是从等待队列中取还是可以插队\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;release()函数&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n\n2.3Latch&amp;Barrier\nCountDownLatch：等其他线程结束，允许count个线程阻塞在一个地方，直至所有线程的任务都执行完毕（是一次性的，不能重复使用）\npublic class DemoJoin&#123;\n  public static void main(String[] args) throws InterruptedException&#123;\n    Thread t1 &#x3D; new Thread(new RunnableForJoin());\n    THread t2 &#x3D; new THread(new RunnableForJoin());\n    t1.start();\n    t2.start();\n    t1.join();&#x2F;&#x2F;join只用来等待线程执行结束，并且必须知道被等待线程是谁\n    t2.join();\n  &#125;\n  public static class RunnableForJoin implements Runnable&#123;\n    @Override\n    public void run()&#123;\n      &#x2F;&#x2F;业务逻辑\n    &#125;\n  &#125;\n&#125;\npublic class DemoLatch&#123;\n  private static final CountDownLatch latch &#x3D; new CountDownLatch(2);\n  public static void main(String[] args) throws InterruptedException&#123;\n    new Thread(new RunnableForLatch()).start();\n    new Thread(new RunnbaleForLatch()).start();\n    latch.await();&#x2F;&#x2F;等待something执行完成而非等待线程结束，并且不需要知道在等谁\n    &#x2F;&#x2F;执行后续逻辑\n  &#125;\n  public static class RunnableForLatch implements Runnable&#123;\n    @Override\n    public void run()&#123;\n      &#x2F;&#x2F;do something\n      latch.countDown();\n      &#x2F;&#x2F;do otheer thing\n    &#125;\n  &#125;\n&#125;\nCyclicBarrier：CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活\npublic class Demo&#123;\n  &#x2F;&#x2F;创建parties为10的CyclicBarrier对象，用于10个线程之间相互等待，尽管10个线程的启动（执行\n  &#x2F;&#x2F;start函数）的时间不同，但每个线程结束都会调用await函数，将paeties减一，然后检查parties\n  &#x2F;&#x2F;如果不为0，则当前线程阻塞等待，如果parties为0，则当前线程唤醒所有调用了await函数的线程。\n  private static final CyclicBarrier barrier &#x3D; new CyclicBarrier(10);\n  public static void main(String[] args)&#123;\n    for(int i&#x3D;0; i&lt;10; ++i)&#123;\n      new Thread(new Runnbale()&#123;\n        @Override\n        public void run()&#123;\n          try&#123;\n            barrier.await();\n          &#125;catch(InterruptedException e)&#123;&#x2F;&#x2F;当前线程被中断\n            e.printStackTrace();\n          &#125;catch(BrokenBarrierException e)&#123;&#x2F;&#x2F;其他线程调用await()期间被中断\n            e.printStachTrace();\n          &#125;\n          &#x2F;&#x2F;执行业务逻辑\n        &#125;\n      &#125;).start();\n    &#125;\n    &#x2F;&#x2F;主线程需要等待以上10个线程执行结束，方法有以下3种：\n    &#x2F;&#x2F;1.sleep() 2.join() 3.CountDownLatch()\n  &#125;\n&#125;\n\n4.JUC1.并发阻塞（xxxBlockingQueue）\n\n\n\n\n\n\n\n\n线程安全和支持读写阻塞，阻塞并发队列一般用于实现生产者-消费者模型\n\nxxxBlockingQueue：ArrayBlockingQueue、LinkedBlockingQueue、LinkedBlockingDeque、PriorityBlockingQueue的实现原理类似，都是基于ReentrantLock锁来实现线程安全，基于Condition条件变量来实现阻塞等待\nArrayBlockingQueue：有界队列实现类，底层采用数组来实现，一旦创建容量不能改变\n使用方法和普通队列类似，只不过增加了读写可阻塞，支持公平和非公平两种工作模式，默认为非公平\n支持读写阻塞的put和take函数（ReentrantLock+Condition）\n非阻塞的offer和poll函数，只通过ReentrantLock锁来保证线程安全，没有通过条件变量来实现阻塞读写\n\n\nLinkedBlockingQueue：基于链表实现的有界阻塞并发队列，默认大小为Integer.MAX_VALUE，可以指定队列大小\nLinkedBlockingDeque：与LinkedBlockingQueue的区别在于，它是一个双端队列，支持两端读写操作\nPriorityBlockingQueue：是一个无界阻塞并发优先级队列，底层基于支持扩容的堆来实现，写操作永远不需要阻塞，只有读操作会阻塞，不可插入null值且插入对象必须可比较大小（comparable）\n\n\nDelayQueue\n延迟阻塞并发队列，底层基于PriorityQueue来实现，因为PriorityQueue支持动态扩容，所以DelayQueue为无界队列，写永远都不会阻塞，只有读会阻塞\nDelayQueue中存储的每个元素都必须实现Delayed接口，提供延迟被读取时间delayTime，PriorityQueue按照delayTime的大小将元素组织成最小顶堆，也就是说，堆顶的元素是delayTime最小的元素，应该最先被读取到\ntake函数，包含两个逻辑，针对leader线程的逻辑和针对非leader线程的逻辑。当多个线程先后调用take函数，第一个线程就是leader线程，剩下的就是非leader线程。第一个线程执行读取操作完成之后，第二个线程便称为leader线程。\n非leader线程直接调用await函数阻塞，等待leader线程执行完成之后调用signal来唤醒\nleader线程读取的是队首的元素，如果队首的元素delayTime大于0，那么leader线程会调用awaitNanos阻塞delayTime时间，当delayTime时间过去之后，leader线程自动唤醒，为了避免假唤醒（插队情况见下），leader线程会检查队首元素的delayTime是否真正变为小于等于0，如果是，则队首元素出队，调用signal唤醒第二个线程，第二个线程就成了leader线程\n插队情况：如果一个线程执行take函数时，如果检查发现队列不为空，并且队首元素的delayTime小于等于0，于是，不管是不是有其他线程在调用await或awaitNanos阻塞等待，这个线程都会直接读取队首元素并返回\n\n\n\n\n较少使用\nSynchronousQueue：用于两个线程之间传递数据，每个put操作必须阻塞等待take操作，队列中不存储任何元素\nLinkedTransferQueue：基于链表实现的无界阻塞并发队列，是LinkedBlockingQueue和SynchronousQueue的综合体，提供了transfer函数，跟SynchronousQueue的put函数的功能相同，调用transfer的线程会一直阻塞，直到数据被其他线程消费才会返回\n\n\n\n2.分段加锁（ConcurrentHashMap）\n原理\n底层数据结构：ConcurrentHashMap底层采用数组+链表/红黑树（1.7使用分段数组+链表）\n实现线程安全的方式\nJDK1.7的ConcurrentHashMap：对整个桶数组进行分割分段，每一把锁只锁其中的一部分数据，多线程访问不同段的数据就不会产生锁竞争\nJDK1.8的ConcurrentHashMap：直接用Node数组+链表/红黑树来实现，并发控制使用synchronized和CAS来操作\nTreeNode是存储红黑树节点，被TreeBin包装，TreeBin通过root属性维护红黑树的根节点，因为红黑树在旋转的时候，根节点可能会被它原来的子节点替换掉，在这个时间点如果有其他线程要写这颗红黑树就会产生线程不安全问题，所以在ConcurrentHashMap中TreeBin通过waiter属性维护当前使用这颗红黑树的线程，来防止其他线程的进入\n\n\nConcurrentHashMap比HashTable效率高的原因：ConcurrentHashMap中，table数组被分段加锁，如果table数组的大小为n，那么就对应存在n把锁，每一个链表独享一把锁，不同链表之间的操作可以多线程并行执行，互不影响，以此来提高并发性能。而HashTable使用synchronized（同一把锁）来保证线程安全，效率低，当一个线程使用put时，另一个线程既不能使用put，也不能使用get\n\n\nConcurrentHashMap类\nHashMap、HashTable、ConcurrentHashMap\nHashMap不是线程安全的：在扩容之后的resize时，如果有两个线程同时在resize，一个线程resize结束了，另一个线程才开始resize，这个时候，后开始的线程因为不知道链表结构已经被改变了，所以会继续之前的逻辑，造成链表节点环形引用\nHashTable和ConcurrentHashMap的区别：线程安全的实现方式不同\n底层数据结构：HashTable使用数组加链表；ConcurrentHashMap底层采用数组+链表/红黑树（1.7使用分段数组+链表）\n实现线程安全的方式\nJDK1.7的ConcurrentHashMap：对整个桶数组进行分割分段，每一把锁只锁其中的一部分数据，多线程访问不同段的数据就不会产生锁竞争\nJDK1.8的ConcurrentHashMap：直接用Node数组+链表/红黑树来实现，并发控制使用synchronized和CAS来操作\nHashTable（同一把锁）：使用synchronized来保证线程安全，效率低，当一个线程使用put时，另一个线程既不能使用put，也不能使用get\n\n\nTreeNode是存储红黑树节点，被TreeBin包装，TreeBin通过root属性维护红黑树的根节点，因为红黑树在旋转的时候，根节点可能会被它原来的子节点替换掉，在这个时间点如果有其他线程要写这颗红黑树就会产生线程不安全问题，所以在ConcurrentHashMap中TreeBin通过waiter属性维护当前使用这颗红黑树的线程，来防止其他线程的进入\n\n\n\n\nConcurrentHashMap\nHashTable和SynchronizedMap都通过简单的对所有方法加锁，来解决线程安全问题，SynchronziedMap的引入是为了让JCF框架的类结构更加清晰，线程安全容器和非线程安全容器相分离，线程安全容器通过统一的方式（Collections的synchronizedXXX方法）来创建\nJDK8版本的ConcurrentHashMap比JDK7版本的分段加锁力度更小，并发度更高，扩容方式有所不同，size实现更高效等优势\nConcurrentHashMap中，table数组被分段加锁，如果table数组的大小为n，那么就对应存在n把锁，每一个链表独享一把锁，不同链表之间的操作可以多线程并行执行，互不影响，以此来提高并发性能\n\n\nget函数的实现原理\nget函数就是读操作，没有加锁的处理逻辑，get函数可以跟任何操作（读操作、写操作、树化、扩容）并行执行，并发性能极高\nget与其他操作没有线程安全问题，但get和扩容操作之间因为有线程安全问题，所以需要特殊处理\n\n\nput函数的实现原理\n写操作：两种加锁方式，链表为空的时候，通过CAS操作将table[index]指向写入数据对应的节点；链表不为空，先对头节点使用synchronized加锁，再执行写操作\n树化：写入操作完成后，如果链表中的节点个数大于等于树化阈值（默认为8），put会执行树化操作，尽管是写时复制操作，但是在树化的同时执行写入操作或扩容，会导致数据丢失，因此树化操作也需要使用synchronzied加锁\n扩容：扩容需要对整个table的所有链表加锁，也是通过分段加锁分段执行，对HashMap增加了两点改进\n写时复制：\n在创建好新的table数组之后，采用写时复制的方法，一点点复制，在全部复制完之后，才会将table引用指向新创建的table数组\ntable会出现三种不同类型的链表，已复制未加锁链表、在复制已加锁链表、未复制未加锁链表，根据类型不同决定在那个table处理读、写、树化操作\n类型的标记由新节点类型ForwardingNode标记，此节点类型的hash值为-1。在扩容的时候，将复制完解锁前的链表头节点换成ForwardingNode节点，并将ForwardingNode节点中的nextTable属性指向新创建的table数组，读、写、树化table数组的某个链表时，如果头节点的hash值为-1.就在这个节点的nextTable属性所指向的table数组中重新查找对应的链表，在执行相应操作\n\n\n复制替代搬移：扩容基于复制而非搬移实现，将老的table数组中的节点中的key、value等数据，复制一份存储在一个新创建的节点中，再将新创建的节点插入到新的table数组中\n多个线程共同协作完成扩容：\n每个线程根据transferIndex来决定具体负责哪几个链表的复制，transferIndex初始化为table.length，多个线程通过CAS修改transferIndex共享变量，谁成功更新，谁就获得[transferIndex-stride, transferIndex)之间的stride个链表的复制权，争夺失败的线程自旋重新执行CAS\n执行table引用更新的线程：ConcurrentHashMap定义了一个int类型的sizeCtl变量，用来标记当前正在参与扩容的线程个数，进入和退出的线程通过CAS操作增减sizeCtl，如果变为0，那么这个线程就是最后一个线程，负责引用更新\n\n\n\n\n\n\nsize函数的实现原理\n扫描统计：每次调用size函数时，都把table数组中的所有链表都遍历一遍，统计得到总的元素个数。每次扫描都需要加锁，导致并发性能降低，执行效率也非常低\n实时统计：ConcurrentHashMap中维护一个size成员变量，每当执行增、删元素操作时，同步更新size，无论将size设置为AtomicInteger还是通过CAS更新size，在高并发场景下，都会存在性能问题，进而影戏那个增、删操作的性能\n非一致性统计：借鉴LongAdder的实现思路，每个链表维护一个实时统计的cellSize，表示这个链表的节点个数，当调用size函数时，每个链表的cellSize相加即可得到元素总个数，但会导致统计结果不一致\n\n\n\n\n\n3.写时复制（CopyOnWriteArrayList、CopyOnWriteArraySet）\n主要应用于并发容器中，为了避免读操作和写操作（增、删、改）同时发生而产生的线程安全问题，写时复制将原始容器中的数据复制一份放入新创建的容器，然后对新创建的容器进行写操作，而对读操作继续在原始容器上进行，这样读写之间不会存在数据访问冲突，当写操作执行完成后，新创建的容器替代原始容器\n这样读操作完全不需要加锁，写入也不会阻塞读取操作，只有写入和写入之间需要进行同步等待\n\n\n弱一致性：CopyOnWriteArrayList源码显示，写操作的结果并非对读操作立即可见，这就导致了短暂的数据不一致，称为弱一致性，在某些业务场景下，会引发bug\n解决办法：CopyOnWriteArrayList提供了用于遍历容器的迭代器\n\n\n连续存储：JUC提供了CopyOnWriteArrayList、CopyOnWriteArraySet，却没有提供CopyOnWriteLinkedList、CopyOnWriteHashMap等其他类型的写时复制容器的原因：因为执行写操作需要复制整个数据，对于链表和哈希表来说，因为数据在内存中不是连续存储的，所以耗时非常大，写操作的性能无法满足工业级通用类对性能的要求。CopyOnWriteArrayList、CopyOnWriteArraySet底层都是基于数组来实现的，而且使用了JVM底层提供的native方法，通过C++代码中的指针实现了内存块的快速拷贝\n\n5.无锁编程\nCAS：CAS指的是先检查后更新这类复合操作，全称为Compare And Set或Compare And Swap。在CAS操作失败后，可以选择自旋直到CAS成功 或 执行失败处理相关的业务逻辑\n\n原子类：原子类的每个操作都可以看成是原子操作，在多线程环境下，执行原子类的操作不会出现线程安全问题\n\nLongAdder\n\n基本用法\npublic class CounterLongAdder&#123;\n  private LongAdder ladder &#x3D; new LongAdder();\n  \n  public void add(long value)&#123;\n    ladder.add(value);\n  &#125;\n  public long get()&#123;\n    &#x2F;&#x2F;sum用来返回累加之后的总和，高并发情况下，不能返回精确的累加值，为了高性能付出的代价\n    return ladder.sum();\n  &#125;\n&#125;\n数据分片\n\n\n去伪共享：主要用于提高多线程并发执行效率，在DIsruptor高性能消息队列中也有用到\n\n伪共享：CPU操作缓存的最小单元是缓存行，不同CPU上的缓存行大小不同，可以为32字节、64字节或128字节。计算Cell对象大小，Cell对象头占12字节，value成员变量为long类型，占8个字节，对象头与value成员变量之间有4字节对齐填充，所以一个Cell对象占24字节，如果一个缓存行大小为64字节，那么两个Cell对象就可能存储在同一个缓存行中。当t1更改cellA的时候，会把缓存行设为无效，导致t2对cellB的缓存也会失效，t1和t2互相影响，导致缓存频繁失效\n为了解决伪共享的问题，可以使用@Contended注解。标记在类上会强制这个类的对象独占一个缓存行，不够的做对齐填充，标记在变量上的作用相同，强制这个变量独占一个缓存行\n\n\n非准确求和：LongAdder中的sum()函数会累加base和cells中的Cell对象的value值，和便是最终的累加值。但这个值是不准确的。因为LongAdder在执行sum()函数时，并没有加锁，也就是说，在执行sum()的同时，有可能其他线程正在执行add()函数。所以会使得累加值不准确\n\n\n\nThreadLocal（又称线程本地存储区「Thread Local Storage，简称为 TLS」）：使用ThreadLocal线程局部变量替代共享变量，以实现在不需要加锁的情况下达到线程安全。其作用域范围介于类的成员变量和函数内局部变量之间，既是线程私有的，又可以在函数之间共享，不但避免了线程安全问题，还能避免参数传递带来的代码耦合问题\n\n每个线程都有自己的私有的本地存储区域，不同线程之间彼此不能访问对方的 TLS 区域。使用 ThreadLocal 变量 的 set(T value)方法可以将数据存入该线程本地存储区，使用 get() 方法可以获取到之前存入的值\n\n实现原理\npublic class ThreadLocal&lt;T&gt; &#123;\n\n    &#x2F;**\n     * 下面的 getMap()方法 传入当前线程，获得一个ThreadLocalMap对象，说明每一个线程维护了\n     * 自己的一个 map，保证读取出来的value是自己线程的。\n     *\n     * ThreadLocalMap 是ThreadLocal静态内部类，存储value的键值就是ThreadLocal本身。\n     *\n     * 因此可以断定，每个线程维护一个ThreadLocalMap的键值对映射Map。不同线程的Map的 key值 是一样的，\n     * 都是ThreadLocal，但 value 是不同的。\n     *&#x2F;\n    public T get() &#123;\n        Thread t &#x3D; Thread.currentThread();\n        ThreadLocalMap map &#x3D; getMap(t);\n        if (map !&#x3D; null) &#123;\n            ThreadLocalMap.Entry e &#x3D; map.getEntry(this);\n            if (e !&#x3D; null) &#123;\n                @SuppressWarnings(&quot;unchecked&quot;)\n                T result &#x3D; (T)e.value;\n                return result;\n            &#125;\n        &#125;\n        return setInitialValue();\n    &#125;\n\n    public void set(T value) &#123;\n        Thread t &#x3D; Thread.currentThread();\n        ThreadLocalMap map &#x3D; getMap(t);\n        if (map !&#x3D; null)\n            map.set(this, value);\n        else\n            createMap(t, value);\n    &#125;\n&#125;\n\n\n\nUnsafe类\n\nUnsafe对象的获取\npublic final class Unsafe &#123;\n  &#x2F;&#x2F; 单例对象\n  private static final Unsafe theUnsafe;\n  ......\n  private Unsafe() &#123;\n  &#125;\n  @CallerSensitive\n  public static Unsafe getUnsafe() &#123;\n    Class var0 &#x3D; Reflection.getCallerClass();\n    &#x2F;&#x2F; 仅在引导类加载器&#96;BootstrapClassLoader&#96;加载时才合法，在我们去调用他的时候，因为类加载器不对，会抛出异常\n    if(!VM.isSystemDomainLoader(var0.getClassLoader())) &#123;\n      throw new SecurityException(&quot;Unsafe&quot;);\n    &#125; else &#123;\n      return theUnsafe;\n    &#125;\n  &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;正确的获取方式&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nField field &#x3D; Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);\nfield.setAccessible(true);\nreturn (Unsafe)field.get(null);\nUnsafe功能\n\n内存操作：内存分配、调整大小、设置为指定值、内存拷贝、内存释放\n\n使用的是堆外内存，好处如下：\n对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是 JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在 GC 时减少回收停顿对于应用的影响。\n提升程序 I/O 操作的性能。通常在 I/O 通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。\n\n\n\nprivate void memoryTest() &#123;\n    int size &#x3D; 4;\n    long addr &#x3D; unsafe.allocateMemory(size);&#x2F;&#x2F;4字节长度\n    long addr3 &#x3D; unsafe.reallocateMemory(addr, size * 2);&#x2F;&#x2F;重新分配一块8字节长度\n    System.out.println(&quot;addr: &quot;+addr);\n    System.out.println(&quot;addr3: &quot;+addr3);\n    try &#123;\n        unsafe.setMemory(null,addr ,size,(byte)1);\n        for (int i &#x3D; 0; i &lt; 2; i++) &#123;\n            unsafe.copyMemory(null,addr,null,addr3+size*i,4);\n        &#125;\n        System.out.println(unsafe.getInt(addr));\n        System.out.println(unsafe.getLong(addr3));\n    &#125;finally &#123;\n        unsafe.freeMemory(addr);\n        unsafe.freeMemory(addr3);\n    &#125;\n&#125;\n内存屏障（例子：StampedLock）\n\n通过阻止编译器和CPU对代码进行重排序，内存屏障就是阻止屏障两边的指令重排序来避免编译器和硬件的不正确优化\n内存屏障可以看做对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作\n主要解决：运行中的线程不是直接读取主内存中的变量的，只能操作自己工作内存中的变量，然后同步到主内存中，并且线程的工作内存是不能共享的。但是子线程借助于主内存，通过屏障，将修改后的结果同步给了主线程，进而修改主线程中的工作空间\n\n&#x2F;&#x2F;内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前\npublic native void loadFence();\n&#x2F;&#x2F;内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前\npublic native void storeFence();\n&#x2F;&#x2F;内存屏障，禁止load、store操作重排序\npublic native void fullFence();\n对象操作\n\n可以通过内存偏移量获取字段值\n\n\n数据操作\n\nCAS 操作\n\n线程调度\n\nClass 操作\n\n类加载：\n静态变量的操作方法：\n\n\n系统信息：返回系统相关信息，如系统指针的大小（addressSize）、内存页的大小（pageSize）\n\n\n\n\n\nFuture类\n\n\n\n\n\n\n\n\n\nFuture模式：异步思想的典型应用，主要用在一些执行耗时任务的场景，避免程序一直原地等待耗时任务执行完成，将耗时任务交给一个子线程来异步执行，等事情干完后，再通过Future类获取到耗时任务的执行结果\n\nJava中Future是JUC包下的一个泛型接口，定义了5个方法，主要包括下面4个功能\n\n取消任务\n判断任务是否取消\n判断任务是否已经执行完成\n获取任务执行结果\n\n&#x2F;&#x2F; V 代表了Future执行的任务返回值的类型\npublic interface Future&lt;V&gt; &#123;\n    &#x2F;&#x2F; 取消任务执行\n    &#x2F;&#x2F; 成功取消返回 true，否则返回 false\n    boolean cancel(boolean mayInterruptIfRunning);\n    &#x2F;&#x2F; 判断任务是否被取消\n    boolean isCancelled();\n    &#x2F;&#x2F; 判断任务是否已经执行完成\n    boolean isDone();\n    &#x2F;&#x2F; 获取任务执行结果\n    V get() throws InterruptedException, ExecutionException;\n    &#x2F;&#x2F; 指定时间内没有返回计算结果就抛出 TimeOutException 异常\n    V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutExceptio\n\n&#125;\nCallable和Future有什么关系\n\nFutureTask提供了Future接口的基本实现，常用来封装Callable和Runnable，具有取消任务、查看任务是否执行完成以及获取任务执行结果的方法，可以作为任务直接被线程执行\n&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);\nFuture&lt;?&gt; submit(Runnable task);\nFutureTask相当于对Callable 进行了封装，管理着任务执行的情况，存储了 Callable 的 call 方法的任务执行结果\n\n\n\nCompletableFuture类\n\n\nFuture在实际使用过程中存在一些局限性比如不支持异步任务的编排组合、获取计算结果的 get()方法为阻塞调用，Java8引入CompletableFuture类来解决这些缺陷\nCompletableFuture同时实现了 Future和 CompletionStage接口\nCompletionStage接口描述了一个异步计算的阶段，很多计算可以分成多个阶段或步骤，此时可以通过它将所有步骤组合起来，行成异步计算的流水线\n\n\n\n\n\n附录\n乐观锁和悲观锁\n\n悲观锁总是假设最坏的情况，认为共享资源每次访问的时候都会出现问题，所以每次在获取资源的时候都会上锁，如synchronized、ReentrantLock等独占锁，常用于多写场景\n\n乐观锁总是假设最好的情况，认为共享资源每次访问的时候都不会出现问题，无需加锁也无需等待，所以只是在提交修改的时候去验证对应的资源是否被其他线程修改了，如JUC的atomic包下面的原子变量类使用了乐观锁的一种实现方式CAS实现的，常用于多读场景\n\n乐观锁存在哪些问题（ABA问题、循环时间长、只能保证一个共享变量的原子操作）\n\nABA问题：如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 \n“ABA”问题。\n\nABA 问题的解决思路是在变量前面追加上版本号或者时间戳\n\n\n循环时间长：CAS经常会用到自旋操作来进行重试，如果长时间不成功，会给 CPU 带来非常大的执行开销\n\n如果 JVM 能支持处理器提供的 pause指令那么效率会有一定的提升，pause 指令有两个作用：\n可以延迟流水线执行指令，使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零\n可以避免在退出循环的时候因内存顺序冲而引起 CPU 流水线被清空，从而提高 CPU 的执行效率\n\n\n\n\nCAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5 开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作\n\n\n\n乐观锁实现方式一：版本号机制\n\n一般是在数据表中加上一个数据版本号 version字段，表示数据被修改的次数。当数据被修改时，version值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的version 值相等时才更新，否则重试更新操作，直到更新成功\n\n\n乐观锁实现方式一：CAS算法\n\nCAS 的全称是 Compare And Swap（比较与交换），用于实现乐观锁，被广泛应用于各大框架中。CAS 的思想很简单，就是用一个预期值和要更新的变量值进行比较，两值相等才会进行更新\nCAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。CAS涉及到三个操作数，当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。\nV ：要更新的变量值(Var)\nE ：预期值(Expected)\nN ：拟写入的新值(New)\n\n\n当多个线程同时使用 CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;Unsafe类提供了方法来实现CAS操作，内部使用C++内联汇编来实现\n&#x2F;**\n\t*  CAS\n  * @param o         包含要修改field的对象\n  * @param offset    对象中某field的偏移量\n  * @param expected  期望值\n  * @param update    更新值\n  * @return          true | false\n  *&#x2F;\npublic final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);\n\npublic final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);\n\npublic final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);\n\n\n\n\n线程暂停的四种方法\n\njoin：线程A在运行期间，可以调用线程B的join()方法，让线程B和线程A联合。这样，线程A就必须等待线程B执行完毕后，才能继续执行\nsleep：使用当前正在执行的线程休眠millis秒，线程处于阻塞状态\nyield：当前正在执行的线程暂停一次，允许其他线程执行，不阻塞，线程进入就绪状态，如果没有其他等待执行的线程，这个时候当前线程就会马上恢复执行\nstop：强迫线程停止执行，已过时，不推荐使用\n\n\n把ArrayList变成线程安全有哪些方法\n\n使用Collections.synchronizedList()方法将ArrayList转换为线程安全的list，会通过在访问方法上添加synchronized方法来保证线程安全\n使用CopyOnWriteArrayList类来替代ArrayList，通过写时复制机制来保证写操作的线程安全性，在读操作时不需要添加锁，提高读取效率\n使用Lock接口来实现同步，可以用ReentrantLock类来实现对ArrayList的同步操作，该类提供了与synchronized类似的功能，但是提供了更灵活的操作，如trylock()\n使用读写锁，用ReentrantReadWriteLock类来实现对ArrayList的读写操作的同步，该类提供了读锁和写锁两种锁，多个线程可以同时获取读锁，但是只有一个线程可以获取写锁，写操作前先获取写锁\n\n\n\n","slug":"Java Concurrent","date":"2023-04-13T23:56:43.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"b4296f0600f693552b5b6c6b665f6025","title":"Java特性","content":"Java1.基础知识1.关键字\ntrue, false, 和 null 虽然不是关键字，但它们是不能用作标识符的文字和保留字\nstrictfp（精确浮点数，跨平台产生相同结果）、native（原生方法）\n\n\n\n\n\nclass\nreturn\nbyte\ntry\nif\n\n\n\nimport\npublic\nboolean\ncache\nelse\n\n\nextends\nprotected\nshort\nfinally\nfor\n\n\nimplements\nprivate\nint\nthrow\nwhile\n\n\nenum\n==final==\nchar\nthrows\ndo\n\n\ninterface\n==static==\nlong\nresource\nswitch\n\n\npackage\nabstract\nfloat\n==volatile==\ncase\n\n\nnew\nnative\ndouble\n==synchronized==\ndefault\n\n\nsuper\nconst\nvoid\n==transient==\nbreak\n\n\nthis\ngoto\ninstanceof\nstrictfp\ncontinue\n\n\n2.概念辨析\n值传递与引用传递\n\n引用类型（数组、接口、类）的数据存储在堆上，栈上存储的是堆的地址，直接更改对象对所有引用都可见，但不能像C++那样让引用指向新的对象\n引用数据判等：==判断两个引用是否指向同一对象，equals方法+重写的hashcode方法判断属性是否相等\n\n\n深拷贝、浅拷贝、引用拷贝\n\n深拷贝与浅拷贝：深拷贝会复制整个对象，包括对象包含的内部对象；浅拷贝会在堆上创建一个新对象，但是对象内部引用类型变量只会复制引用地址，不会直接复制内部数据\n\n引用拷贝：两个不同引用指向同一对象\n\n示例\npublic class Address implements Cloneable&#123;\n    private String name;\n    &#x2F;&#x2F; 省略构造函数、Getter&amp;Setter方法\n    @Override\n    public Address clone() &#123;\n        try &#123;\n            return (Address) super.clone();\n        &#125; catch (CloneNotSupportedException e) &#123;\n            throw new AssertionError();\n        &#125;\n    &#125;\n&#125;\n\npublic class Person implements Cloneable &#123;\n    private Address address;\n    &#x2F;&#x2F; 省略构造函数、Getter&amp;Setter方法\n    @Override\n    public Person clone() &#123;\n        try &#123;\n\t\t\t\t\t\t&#x2F;&#x2F;浅拷贝\n            Person person &#x3D; (Person) super.clone();\n            return person;\n        &#125; catch (CloneNotSupportedException e) &#123;\n            throw new AssertionError();\n        &#125;\n    &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;浅拷贝&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nPerson person1 &#x3D; new Person(new Address(&quot;武汉&quot;));\nPerson person1Copy &#x3D; person1.clone();\n&#x2F;&#x2F; true\nSystem.out.println(person1.getAddress() &#x3D;&#x3D; person1Copy.getAddress());\n\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;深拷贝&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n@Override\npublic Person clone() &#123;\n    try &#123;\n        Person person &#x3D; (Person) super.clone();\n\t\t\t\t&#x2F;&#x2F;深拷贝\n        person.setAddress(person.getAddress().clone());\n        return person;\n    &#125; catch (CloneNotSupportedException e) &#123;\n        throw new AssertionError();\n    &#125;\n&#125;\nPerson person1 &#x3D; new Person(new Address(&quot;武汉&quot;));\nPerson person1Copy &#x3D; person1.clone();\n&#x2F;&#x2F; false\nSystem.out.println(person1.getAddress() &#x3D;&#x3D; person1Copy.getAddress());\n\n\n重载和重写的区别\n\n重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理（如构造函数）；重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，就要覆盖父类方法（如Override）\n如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的\n\n\n接口和抽象类\n\n共同点：都不能被实例化、都可以包含抽象方法，都可以有默认实现方法（default声明，子类可不实d现）\n不同点：\n接口主要是对API声明（参数类型、返回值类型、函数名），抽象类主要是为了代码复用\n一个类可以实现多个接口，但只能继承自一个抽象类\n接口中的成员变量只能是public static final类型的，不能被修改且必须有初始值，而抽象类的成员变量默认default，可在子类中被重新定义，也可被重新赋值\n\n\n\n\nfinal和static\n\n只有成员变量能被static、public、protected、private修饰，局部变量不行，但是两者都能被final修饰\n\n\n引用类型转换：仅限于有继承关系的类之间，分为向上转换和向下转换两种\n\n向上转换，自动类型转换，总是可以的\n向下转换需要保证转换的对象本身就是子类类型的，只不过暂时转换为了父类型，现在只是再转回去而已\n\n\n\n3.语法糖\nswitch支持String与枚举：int比数、char比ascii码、字符串用hashCode()和equals()，其它如short、byte、int都需要转换为整数\n\n泛型和类型擦除：编译时会使用泛型做类型检查，但是当代码编译为字节码之后，泛型中的类型参数和通配符都替换为上界限（==类型擦除==）\n\n泛型遇到重载：因为都会转成父类型，所以List&lt;String&gt;和List&lt;Integer&gt;这种重载会编译失败\n\n当泛型遇到catch：泛型的类型参数不能用在catch语句中，因为异常处理是由JVM在运行时刻来进行的，类型信息被擦除了，所以JVM是无法区分两个异常类型MyException&lt;String&gt;和MyException&lt;Integer&gt;的\n\n创建对象时：不能使用new T()来创建类型参数对象，在代码编译成字节之后类型信息已经擦除，所以，在运行时，JVM无法确定具体类型，也就无法知道T是否存在无参构造函数\n\n当泛型内包含静态变量：由于经过类型擦除，所有的泛型类实例都关联到同一份字节码上，泛型类的所有静态变量是共享的\npublic class StaticTest&#123;\n    public static void main(String[] args)&#123;\n        GT&lt;Integer&gt; gti &#x3D; new GT&lt;Integer&gt;();\n        gti.var&#x3D;1;\n        GT&lt;String&gt; gts &#x3D; new GT&lt;String&gt;();\n        gts.var&#x3D;2;\n        System.out.println(gti.var); &#x2F;&#x2F;输出为2\n    &#125;\n&#125;\nclass GT&lt;T&gt;&#123;\n    public static int var&#x3D;0;\n    public void nothing(T x)&#123;&#125;\n&#125;\n因为需要继承自Object，所以基本类型不可以传入类型参数，只有引用类型可以。但是有语法糖可以让List&lt;int&gt;中的int替换为Integer，但是开发上依旧需要为每个基本类型分别定义多个不同的函数接口\n\n\n\n自动装箱与拆箱：原始类型byte, short, char, int, long, float, double, boolean 对应的封装类为Byte, Short, Character, Integer, Long, Float, Double, Boolean\n\n基本类型和包装类型的区别：包装类型不赋值时是null，可用于范型，占用空间大\n\n基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中（JIT优化，逃逸分析，分配到栈上）\n基本数据类型存放在栈中是一个常见的误区！基本数据类型的成员变量如果没有被 static修饰的话（不建议这么使用，应该要使用基本数据类型对应的包装类型），就存放在堆中\n类静态成员变量存放在方法区中！（方法区又叫静态区，跟堆一样，被所有线程共享，方法区包含所有的class和static变量）\n\n\n常量池\n\nInteger等包装类使用了常量池技术，IntegerCache类（享元模式）中会缓存值为-128到127之间的Integer对象，当通过自动装箱，也就是调用valueOf()来创建Integer对象时，如果要创建的Integer对象的值在-128到127之间，会从IntegerCache中直接返回，否则才会真正调用new方法创建，详见Integer类的valueOf()（JVM也提供了方法，可以自定义缓存的最大值）\n\nByte、Short、Integer、Long这四种包装类默认创建了数值[-128,128]的相应类型的缓存数据（存放在一个Cache数组中，由static代码块直接初始化），Character创建了数值在[0，127]范围的缓存数据，Boolean直接返回True或False（return (b ? TRUE : FALSE);）\n\n所有整型包装类对象之间值的比较，全部使用 equals 方法比较\nInteger i1 &#x3D; 40; &#x2F;&#x2F;触发自动装箱，使用缓存中的对象\nInteger i2 &#x3D; new Integer(40); &#x2F;&#x2F;新创建的对象\nSystem.out.println(i1&#x3D;&#x3D;i2); &#x2F;&#x2F;返回false\n\n\n示例代码：项目首选基本类型，业务相关可选包装类用null表示空而不是0\n&#x2F;&#x2F;自动装箱，语法糖，底层实现为：Integer iobj &#x3D; Integer。valueOf(12);\nInteger iobj &#x3D; 12;\n&#x2F;&#x2F;自动拆箱，语法糖，底层实现为：int i &#x3D; iobj.intValue();\nint i &#x3D; iobj;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;触发自动装箱和拆箱的几种情况&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\n&#x2F;&#x2F;将基本类型数据赋值给包装类变量（包括参数传递）时，触发自动装箱\nint i1 &#x3D; 5\nInteger iobj1 &#x3D; 5;&#x2F;&#x2F;1\niobj &#x3D; i1;&#x2F;&#x2F;1\nList&lt;Integer&gt; list &#x3D; new ArrayList&lt;&gt;();\nlist.add(i1);&#x2F;&#x2F;1\n&#x2F;&#x2F;将包装类对象赋值给基本类型变量（包括参数传递）时，触发自动拆箱\nInteger iobj2&#x3D; new Integer(6);\nint i2 &#x3D; iobj2;&#x2F;&#x2F;2\n&#x2F;&#x2F;当包装类对象参与算术运算、关系运算（&lt;,&gt;）时，触发自动拆箱操作\nInteger iobj3 &#x3D; iobj1 + iobj2;\nboolean bl &#x3D; (iobj1 &lt; iobj2);\nbl &#x3D; (iobj1 &lt; 2);\n&#x2F;&#x2F;当包装类对象参与关系运算（&#x3D;&#x3D;），且另一方是基本类型数据时，触发自动拆箱操作。\nInteger iobj4 &#x3D; new Integer(123);\nbl &#x3D; (iob4 &#x3D;&#x3D; 123);\n\n\n方法变长参数：String… args用一个数组实现，用foreach遍历，编译后会被转变成数组\n\n枚举：当我们使用enum来定义一个枚举类型的时候，编译器会自动创建一个final类型的类继承Enum类，所以枚举类型不能被继承（public enum t&#123;&#125; =&gt; public final class T extends Enum&#123;&#125;）\n\n内部类：\n\n会独立于外部类，生成一个新的class文件，名字为外部类名$内部类名.class或外部类名$[序号].class，静态匿名内部类可访问静态成员变量+静态函数；普通匿名内部类不可访问外部函数中非final修饰的局部变量\n外部函数通过类似参数传递的方式，将局部变量通过值传递的方式传入到匿名内部类，这是外部函数局部变量的副本，所以如果能访问非final修饰的局部变量的话，内部类对其的更改不起作用，违反直觉，类似于形参的改变不影响实参\n\npublic interface I&#123;&#125;\npublic class A&#123;\n    private class B&#123;&#125; &#x2F;&#x2F;类似于ArrayList的内部类Itr\n    private class C implements I&#123;&#125;&#x2F;&#x2F;实现外部接口的内部类\n    public class D&#123;&#125;&#x2F;&#x2F;public修饰的内部类\n\t\tpublic static class E&#123;&#125;&#x2F;&#x2F;静态内部类\n    \n    public B getB()&#123; return new B(); &#125;\n    public I getC()&#123; return new C(); &#125;\n    public D getD()&#123; return new D(); &#125;\n&#125;\npublic class Demo&#123;\n    public static void main(String[] args)&#123;\n        A a &#x3D; new A();\n        A.B b &#x3D; a.getB();&#x2F;&#x2F;编译报错，满足封装原则\n        I c &#x3D; a.getC();&#x2F;&#x2F;可访问\n        A.D d1 &#x3D; a.getD();\n        A.D d2 &#x3D; a.new D();\n\t\t\t\tA.E e &#x3D; new A.E();&#x2F;&#x2F;静态内部类的对象可以独立于外部类单独创建\n    &#125;\n&#125;\n条件编译：if的条件是final且为false时，对应代码块不被编译，主要出于对代码优化的考虑\n\n断言：其实断言的底层实现就是if语言，如果断言结果为true，则什么都不做，程序继续执行，如果断言结果为false，则程序抛出AssertError来打断程序的执行\n\n数值字面量：不管是整数还是浮点数，都允许在数字之间插入任意多个下划线，为了方便阅读\n\n增强for循环：for-each用了普通的for循环和Iterator迭代器的hasNext()方法，在遍历过程中不能增删内部元素，会抛出异常（可以使用Iterator.remove()方法在删除当前迭代对象的同时维护索引的一致性）\n\ntry-with-resource：在try()中写资源申请，就不用在finally中判断是否为null在关闭了，编译期帮助我们关闭了（资源类需要实现Java.lang.AutoClosale接口）\n\nlambda表达式：只有一个函数的接口叫做函数式接口，可以用Lambda表达式简化\n\nLambda表达式\n(类型 a,类型 b)-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a，b为输入参数\n(a,b)-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a，b为输入参数\na-&gt;&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;a为输入参数\n&#123;语句1;语句2; ... ;return 输出;&#125;&#x2F;&#x2F;没有入参\n方法引用：当Lambda中的逻辑已经有现成的方法实现时，可以直接使用方法引用。方法引用要求所引用的方法的参数列表的返回值，跟函数接口中未实现方法的参数列表和返回值完全一致，格式如下\n&#x2F;&#x2F;对象::实例方法\n&#x2F;&#x2F;类::静态方法\n&#x2F;&#x2F;类::实例方法\npublic class FPDemo &#123;\n    public static void main(String] args) &#123;\n        List&lt;String&gt; strList &#x3D; Arrays.asList(&quot;wz-a.java&quot;, &quot;wz-b.txt&quot;, &quot;c.java&quot;);\n        strList.stream()\n\t\t\t\t\t\t&#x2F;&#x2F;直接引用String的方法\n            .filter(((Predicate&lt;String&gt;) String::isEmpty).negate())\n            &#x2F;&#x2F; .filter(s-&gt;s.isEmpty())\n            .filter(s-&gt;s.startsWith(&quot;wz-&quot;))\n            .map(String::length)\n            &#x2F;&#x2F;.map(s-&gt;s.length())\n            .forEach(l-&gt;System.out.printIn(I));&#x2F;&#x2F;输出9、8\n    &#125;\n&#125;\n\n\n\n2.进阶知识1.特殊语法\n反射：在运行的过程中动态告知JVM去创建对象、创建方法、获取类信息（构造函数、方法、成员变量、注解），重要应用见Spring框架的依赖注入\n\nClass类：是一个存储类的信息的特殊的类，提供了大量的方法，可以获取类的信息，比如获取类中的方法，获取构造函数，获取成员变量等\n\nConstructor类：用来存储构造函数的信息，如通过newInstance()方法来进行有参/无参构造\nMethod类：存储方法的信息，如通过invoke()方法可以执行类中的对应方法\nField类：用来存储成员变量的信息\n\n\n获取反射的三种方法\n\n通过对象获取反射\nObject obj &#x3D; new Object(); &#x2F;&#x2F; 创建一个对象\nClass&lt;?&gt; clazz &#x3D; obj.getClass(); &#x2F;&#x2F; 获取 Class 对象\n通过类名获取反射\nClass&lt;?&gt; clazz &#x3D; Class.forName(&quot;com.example.MyClass&quot;); &#x2F;&#x2F; 获取 Class 对象\n通过类字面常量获取反射\nClass&lt;?&gt; clazz &#x3D; MyClass.class; &#x2F;&#x2F; 获取 Class 对象\n\n\n反射攻击：在Constructor、Method、Field类，包含一个公共的方法，能够改变构造函数、方法、成员变量的访问权限public void setAccessible(boolean flag)，利用这个方法，可以将私有的构造函数、方法、成员变量设置为可以访问的，这样就可以超越权限限制，在代码中访问私有的构造函数、方法和成员变量（打破单例类只能实例化一个对象的限制的情况）\n\n\n\n注解：注解相当于给元素打了一个tag，任何编译器或者应用程序通过反射可以访问的代码元素，都可以用注解去标识\n\n自定义注解：通过反射来读取注解，重要应用为Spring用注解代替XML配置文件\n&#x2F;&#x2F;Java内建注解\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Override &#123;\n&#125;\n\n&#x2F;&#x2F;自定义注解\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\npublic @interface RateLimit &#123;\n\tpublic enum TimeUnit &#123; SECOND,MINUTE, HOUR, DAY,MONTH&#125;\n    string apiName();\n\tint limitCount();\n\tTimeUnit timeUnit() default TimeUnit.SECOND;\n&#125;\n元注解\n\n@Target：用来描述注解的使用范围（如类、接口、方法、成员变量等）\n@Retention：用来描述注解的可见范围、或叫生命周期（如源码可见、字节码可见、运行时可见）\n@Documented：表示注解信息会输出到Javadoc文档中\n@interface：class、interface、enum、@interface这四者是平级关系，@interface用来定义注解，在注解中，还可以定义一些变量，特殊的是注解使用方法来定义变量，对于只有一个变量的注解，可以将其定义为value，这样，在使用时，可以不指定变量的名称\n\n\n实践应用\n\n替代注释：Guava提供@VisibleForTesting注解在方法上进行标记，这个注解只起到注释的作用，并没有实际的作用\n作为标记：Java中有一种特殊的接口，叫做标记接口（Marker Interface）。标记接口中不包含任何方法，跟注解类似，起到标记作用，比如RandomAccess、Cloneable、Serializable，可以根据标记接口判断对象是否可以执行某些操作\n替代XML文件\n@Configuration注解修饰的类中的@Bean创建首字母小写的对象\n@Component注解创建同名对象，使用@Autowired注入对象\n\n\n\n\n\n\n动态代理\n\n静态代理：通过实现接口或继承的方式，通过注入原始类并添加新功能的方式实现。实现简单，但会导致项目中的类成倍增加，所有相关的类都需要增加代理类，重复代码多\n动态代理\n一般静态指的编译阶段，动态指的运行阶段。在代理模式上，静态代理指的是在编译阶段时生成代理类的字节码，动态代理指的是运行时生成代理类的字节码，且字节码只存在与内存中，并不会生成对应的class文件\n之所以可以实现动态代理，是因为JVM设计得非常灵活，只要是符合类的格式的字节码，都可以在运行时被JVM解析并加载，不管这个字节码是来自预先编译好的(class文件)，还是在内存中临时生成的(典型应用:动态代理)，又或者从网络加载而来的(典型应用: Applet)。这部分内容涉及到JVM的类加载机制，见JVM\n实现方法一：利用JDK提供的类来实现（InvocationHandler接口+Proxy类）\n&#x2F;&#x2F;为UserController类实现动态代理，当为其它Controller类中的方法也添加时间统计代码时，\n&#x2F;&#x2F;可以复用CtrlProxyHandler类，并通过Proxy类的newProxyInstance()静态方法生成对应的代理类对象\npublic class CtrlProxyHandler implements InvocationHandler &#123;\n    private Object origBean;\n\n    public CtrlProxyHandler(Object origBean) &#123;\n        this.origBean &#x3D; origBean;\n    &#125;\n    @override\n    public Object invoke(Object proxy， Method method, Object[] args) throws Throwable &#123;\n        long startTime &#x3D; system.currentTimeMillis();\n        \n\t\t\t\t&#x2F;&#x2F;所有方法的调用都会变成调用invoke方法，参数为生成的代理类、要调用的方法、对应的参数\n\t\t\t\t&#x2F;&#x2F;通过Proxy类的newProxyInstance()创建的代理对象在调用方法的时候，实际会调用到实现InvocationHandler接口的类的\n\t\t\t\t&#x2F;&#x2F;invoke()方法，可以在invoke()方法中自定义处理逻辑，比如在方法执行前后做什么事情\n        Object res &#x3D; method.invoke(origBean, args);\n        \n        long costTime &#x3D; System.currentTimeMillis() - startTime;\n        System.out.printIn(origBean.getClass().getSimpleName()+&quot;#&quot;+ method.getName() + &quot; cost time: &quot; + costTime);\n        return res;\n    &#125;\n&#125;\n\npublic class JDKProxyDemo &#123;\n    public static void main(String] args) &#123;\n        UserController userController &#x3D; new UserController();\n        CtrlProxyHandler handler &#x3D; new CtrlProxyHandler(userController);\n        &#x2F;&#x2F;用Proxy的静态方法生成代理类\n        IUserController userControllerProxy &#x3D; (IUserController)Proxy.newProxyInstance\n            (handler.getClass().getClassLoader(), UserController.class.getInterfaces(), handler);\n        userControllerProxy.login(&quot;139********&quot;，&quot;*********&quot;);\n    &#125;\n&#125;\n\n\nnewProxyInstance函数：\npublic static Object newProxyInstance(\n    ClassLoader loader, Class&lt;?&gt;[]interfaces,InvocationHandler h)&#123;\n\t\t&#x2F;&#x2F;参数为：被代理的类的类加载器（上面的代码都是同一个类加载器AppClassLoader，但是如果定义了别的类加载器就需要注意）、\n\t\t&#x2F;&#x2F;被代理的类实现的所有接口、动态代理处理器（实现InvocationHandler接口，重写invoke方法）\n    \n    &#x2F;&#x2F;1)生成动态代理类\n    &#x2F;&#x2F;2)加载动态代理类\n\t\t&#x2F;&#x2F;动态代理类具有哪些方法：只跟接口有关，跟原始类没有任何关系，这也是基于JDK实现的动态代理要求原始类必须有接口定义才行\n    Class&lt;?&gt; cl&#x3D; getProxyClass0(loader, intfs);\n\n    &#x2F;&#x2F;3)实例化动态代理类对象\n    final Class&lt;?&gt;[] constructorParams &#x3D; &#123; InvocationHandler.class &#125;;\n    final Constructor&lt;?&gt; cons &#x3D; cl.getConstructor(constructorParams);\n    return cons.newlnstance(new Object]&#123;h);\n&#125;d f\n\n\nClassLoader loaderloader表示类加载器，用于加载动态代理类到JVM\nClass&lt;?&gt;[] interfaces用于生成动态代理类，接口中的方法就是动态代理类包含的方法\nInvocationHandler h用于创建（实例化）动态代理类对象\n\n\nProxyGenerator类：newProxyInstance()函数调用ProxyGenerator类(JDK提供的生成字节码的类)，按照类的字节码格式，生成动态代理类的字节码，并存储到内存（proxyClassFile）中\n&#x2F;&#x2F;生成动态代理类的名称\nfinal String proxyClassNamePrefix &#x3D; &quot;$Proxy&quot;;\nlong num &#x3D; nextUniqueNumber.getAndIncrement();\nString proxyName &#x3D; proxyPkg + proxyClassNamePrefix + num;\n\n&#x2F;&#x2F;ProxyGenerator类似字节码类库，可以生成动态代理类的字节码\nbyte[] proxyClassFile &#x3D; ProxyGenerator.generateProxyClass(\n    proxyName, interfaces, accessFlags);\ntry &#123;\n    &#x2F;&#x2F;通过JVM的类加载器来加载动态代理类\n    return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length);\n&#125; \ncatch(ClassFormatError e) &#123; &#x2F;&#x2F;如果生成的动态代理类的字节码格式有误，则报错\n    throw new lllegalArgumentException(e.toString());\n&#125;\n\n\n实现方法二：使用第三方的字节码类库来实现，比如CGLIB、BECL、ASM、Javassit等直接编辑字节码\n\n\n\n\n\n\n\n\n2.工具类\nString\nString不可变的原因\nfinal修饰的数组，数组内容是可变的private final char value[];\n但是String没有暴露更改该数组的公共方法\n因为String类是final修饰的，所以子类无法继承，避免了子类破坏String的不可变性\n\n\n常量池技术\nString类型跟Integer等包装类类似，使用常量池技术，并且==只有使用字符串常量赋值时，才触发==，如果字符串常量在常量池中已经创建过，则直接使用已经创建的对象。用new创建的对象不在常量池中\n除了使用字符串常量赋值外，还可以使用intern()方法，将分配在堆上的String对象，原模原样在常量池中复制一份。当无法用字符串常量赋值，但又有大量重复字符串时，就可以使用intern()方法复制到常量池中，代码中使用常量池中的String对象，原String对象就被JVM回收掉\n\n\n其它\nsubstring()\nsubstring(int beginIndex, int endIndex)方法截取并返回下标在[beginIndex, endIndex)范围内的子串\n在JDK7及其以上版本中，substring()方法会生成新的String对象来存储子串，但如果传入参数正好等于字符串的长度，那么会返回字符串本身，不会创建新对象\n在JDK6及以前的版本，通过substring()方法获取到的子串会共享char数组，并有count和offset属性标志子串的长度和起点\n\n\n运算符重载：C++能直接重载运算符，但Java并不支持（重载运算符是函数式编程、并且语法太复杂），但是String类却实现了加法操作String sc = sa + sb;，主要是因为String比较常用，所以延续了基本类型及其包装类的设计，这样使用起来就方便和统一\nStringBuilder与StringBuffer\n因为String不可变，用+拼接效率低，每次都需要创建新的String对象，所以Java设计了StringBuilder\nStringBuilder支持修改和动态扩容，可以用append()函数拼接，可以把StringBuilder看作是char类型的ArrayList（ArrayList）\n在平时开发中，经常用+号连接多个字符串，实际上底层就采用StringBuilder来实现\nStringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。\n相同情况下使用 StringBuilder相比使用 StringBuffer仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。\n\n\n\n\n\n\n\n\n3.JCF框架\nArrayList动态扩容：在增加元素的时候要检测是否需要扩容，首先确定最小扩容量（最小是10），然后判断是否需要扩容（最小扩容量大于当前数组长度），执行grow函数进行扩容，扩容为原来的1.5倍，如果不够的话就直接使用最小扩容量来作为长度，避免多次扩容，若是1.5倍长度大于数组最大长度，则需要看最小扩容量是否大于最大容量，如果是则为MAX_VALUE否则为MAX_VALUE-8\npublic boolean add(E e) &#123;\n    ensureCapacityInternal(size + 1);  &#x2F;&#x2F; Increments modCount!!\n    elementData[size++] &#x3D; e;\n    return true;\n&#125;\n\nprivate void ensureCapacityInternal(int minCapacity) &#123;\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n&#125;\n\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;确定是否需要扩容，主要用在添加大量元素之前，减少增量分配的次数，通过提前扩容，可以提升性能&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic void ensureCapacity(int minCapacity) &#123;\n    int minExpand &#x3D; (elementData !&#x3D; DEFAULTCAPACITY_EMPTY_ELEMENTDATA)\n        &#x2F;&#x2F; any size if not default element table\n        ? 0\n        &#x2F;&#x2F; larger than default for default empty table. It&#39;s already\n        &#x2F;&#x2F; supposed to be at default size.\n        : DEFAULT_CAPACITY;\n\t\t&#x2F;&#x2F;如果期待最小容量大于已有的最大容量\n    if (minCapacity &gt; minExpand) &#123;\n        ensureExplicitCapacity(minCapacity);\n    &#125;\n&#125;\n&#x2F;&#x2F;得到最小扩容量\nprivate static int calculateCapacity(Object[] elementData, int minCapacity) &#123;\n    if (elementData &#x3D;&#x3D; DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;\n        return Math.max(DEFAULT_CAPACITY, minCapacity);\n    &#125;\n    return minCapacity;\n&#125;\n&#x2F;&#x2F;得到最小扩容量，通过最小扩容量扩容\nprivate void ensureCapacityInternal(int minCapacity) &#123;\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n&#125;\n&#x2F;&#x2F;判断是否需要扩容\nprivate void ensureExplicitCapacity(int minCapacity) &#123;\n    modCount++;\n\n    &#x2F;&#x2F; overflow-conscious code\n    if (minCapacity - elementData.length &gt; 0)\n\t\t\t\t&#x2F;&#x2F;调用grow方法进行扩容，调用此方法代表已经开始扩容了\n        grow(minCapacity);\n&#125;\nprivate void grow(int minCapacity) &#123;\n    &#x2F;&#x2F;oldCapacity为旧容量，newCapacity为新容量\n    int oldCapacity &#x3D; elementData.length;\n    &#x2F;&#x2F;将oldCapacity 右移一位，其效果相当于oldCapacity &#x2F;2，\n    &#x2F;&#x2F;我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，\n    int newCapacity &#x3D; oldCapacity + (oldCapacity &gt;&gt; 1);\n    &#x2F;&#x2F;然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，\n    if (newCapacity - minCapacity &lt; 0)\n        newCapacity &#x3D; minCapacity;\n    &#x2F;&#x2F;再检查新容量是否超出了ArrayList所定义的最大容量，\n    &#x2F;&#x2F;若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE，\n    &#x2F;&#x2F;如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为MAX_ARRAY_SIZE。\n    if (newCapacity - MAX_ARRAY_SIZE &gt; 0)\n        newCapacity &#x3D; hugeCapacity(minCapacity);\n    &#x2F;&#x2F; minCapacity is usually close to size, so this is a win:\n    elementData &#x3D; Arrays.copyOf(elementData, newCapacity);\n&#125;\n&#x2F;&#x2F;比较minCapacity和MAX_ARRAY_SIZE\nprivate static int hugeCapacity(int minCapacity) &#123;\n    if (minCapacity &lt; 0) &#x2F;&#x2F; overflow\n        throw new OutOfMemoryError();\n    return (minCapacity &gt; MAX_ARRAY_SIZE) ?\n        Integer.MAX_VALUE :\n        MAX_ARRAY_SIZE;\n&#125;\nHashMap\n\nSet容器包括HashSet、LinkedHashSet、TreeSet，从代码实现上来说，这三个类底层分别是依赖HashMap、LinkedHashMap、TreeMap。例如：往HashSet中存储对象obj，底层将obj作为key，一个空的Object对象作为value，一并存储到HashMap中\n\n底层为哈希表，对key求哈希作为hash值，包裹hash值、key和value为Node对象，作为哈希表（数组+链表）的组成节点。key不能重复，存储重复的key，新value会覆盖旧value（可以存一个key为null的键值对，但是不同key的value都可以是null）\n&#x2F;&#x2F; 包含另一个“Map”的构造函数\n public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123;\n     this.loadFactor &#x3D; DEFAULT_LOAD_FACTOR;\n     putMapEntries(m, false);&#x2F;&#x2F;下面会分析到这个方法\n &#125;\nfinal void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123;\n    int s &#x3D; m.size();\n    if (s &gt; 0) &#123;\n        &#x2F;&#x2F; 判断table是否已经初始化\n        if (table &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; pre-size\n            &#x2F;&#x2F; 未初始化，s为m的实际元素个数\n            float ft &#x3D; ((float)s &#x2F; loadFactor) + 1.0F;\n            int t &#x3D; ((ft &lt; (float)MAXIMUM_CAPACITY) ?\n                    (int)ft : MAXIMUM_CAPACITY);\n            &#x2F;&#x2F; 计算得到的t大于阈值，则初始化阈值\n            if (t &gt; threshold)\n                threshold &#x3D; tableSizeFor(t);\n        &#125;\n        &#x2F;&#x2F; 已初始化，并且m元素个数大于阈值，进行扩容处理\n        else if (s &gt; threshold)\n            resize();\n        &#x2F;&#x2F; 将m中的所有元素添加至HashMap中\n        for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123;\n            K key &#x3D; e.getKey();\n            V value &#x3D; e.getValue();\n            putVal(hash(key), key, value, false, evict);\n        &#125;\n    &#125;\n&#125;\n哈希函数\nstatic final int hash(Object key) &#123;\n    int h;\n\t\t&#x2F;&#x2F;key为null的值存储在下标为0的位置，但一个HashMap只能存储一个值为null的key\n\t\t&#x2F;&#x2F;hashCode底层为JNI，定义在Object类中，根据对象在内存中的地址来计算哈希值，子类中可以重写\n\t\t&#x2F;&#x2F;h^(h&gt;&gt;&gt;16)：数组长度一般不超过2^16，所以通过将h的高16位和低16位异或，来增加参与运算的信息\n    return (key &#x3D;&#x3D; null) ? 0 : (h &#x3D; key.hashCode()) ^ (h &gt;&gt;&gt; 16);\n&#125;\n&#x2F;&#x2F;确定插入数组时的位置，使用位操作与数组长度n进行取模计算（前提是n为2的幂次方），防止索引越界\nint index &#x3D; hash(key)&amp;(n-1); &#x2F;&#x2F; n-1为 11111，与其进行&amp;运算，相当于对n取余数\n\npublic V get(Object key) &#123;\n    Node&lt;K,V&gt; e;\n    return (e &#x3D; getNode(hash(key), key)) &#x3D;&#x3D; null ? null : e.value;\n&#125;\nfinal Node&lt;K,V&gt; getNode(int hash, Object key) &#123;\n    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;\n    if ((tab &#x3D; table) !&#x3D; null &amp;&amp; (n &#x3D; tab.length) &gt; 0 &amp;&amp;\n        (first &#x3D; tab[(n - 1) &amp; hash]) !&#x3D; null) &#123;&#x2F;&#x2F; hash表不为空，待查找链表有值\n        if (first.hash &#x3D;&#x3D; hash &amp;&amp; &#x2F;&#x2F; always check first node，先查hash(key)，再查key.equals()\n            ((k &#x3D; first.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))&#x2F;&#x2F;检测是否哈希冲突\n            return first;\n        if ((e &#x3D; first.next) !&#x3D; null) &#123;\n            if (first instanceof TreeNode) &#x2F;&#x2F;已经树化，进行树上的查找\n                return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);\n            do &#123;\n                if (e.hash &#x3D;&#x3D; hash &amp;&amp;\n                    ((k &#x3D; e.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))\n                    return e;\n            &#125; while ((e &#x3D; e.next) !&#x3D; null);&#x2F;&#x2F;未树化，进行链表上的遍历查找\n        &#125;\n    &#125;\n    return null;\n&#125;\n装载因子：table大小（n）和装载因子（loadFactor）可以用默认的也可以通过构造函数传入，一般为0.75：\n\n权衡时间效率和空间效率之后的结果\n大概是[0.5,1]之间，因为小于0.5会有一半空间从来未用，当大于1时，哈希冲突的概率会大大增加，即使有链表和树化，也会影响性能\n因为table数组的大小n都是2的倍数，而且触发扩容的阈值threshold = n * loadfactor，所以，在[0.5,1]之间，只有0.75能使得得到的阈值一直是整数\n\npublic HashMap(int initialCapacity, float loadFactor) &#123;\n\t\t&#x2F;&#x2F;...initialCapacity和loadFactor的可行性检验代码...\n    this.loadFactor &#x3D; loadFactor;\n\t\t&#x2F;&#x2F;直接赋值的原因：此时table数组只声明未创建，其值为null，在第一次调用put()函数后，\n\t\t&#x2F;&#x2F;HashMap会先用threshold作为数组大小创建table数组，再将其重新赋值为真正的扩容阈值\n\t\t&#x2F;&#x2F;this.table &#x3D; new T[this.threshold];\n\t\t&#x2F;&#x2F;this.threshold *&#x3D; this.factor;\n    this.threshold &#x3D; tableSizeFor(initialCapacity);\n&#125;\n&#x2F;&#x2F;initialCapacity需要是2的幂次方，如果不是，需要寻找比initialCapacity大的第一个2的幂次方数\nstatic final int tableSizeFor(int cap) &#123; &#x2F;&#x2F; 1100  12 应该返回 10000\n    int n &#x3D; cap - 1; &#x2F;&#x2F; 1011\n    n |&#x3D; n &gt;&gt;&gt; 1; &#x2F;&#x2F; 0101 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 2; &#x2F;&#x2F; 0011 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 4; &#x2F;&#x2F; 0000 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 8; &#x2F;&#x2F; 0000 - 1111\n    n |&#x3D; n &gt;&gt;&gt; 16; &#x2F;&#x2F; 0000 - 1111\n    return (n &lt; 0) ? 1 : (n &gt;&#x3D; MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#x2F;&#x2F; 10000 16\n&#125;\n动态扩容：put后，若元素个数超过threshold=n*loadFactor时触发（n为table大小，loadFactor为装载因子）\n\nHashMap的默认初始化大小为16，之后每次扩充容量为原来的2倍，如果指定了大小，也会选择2的幂次来作为初始值\n因为Hashmap的容量大小是2的幂次方，所以可以通过&amp;运算来优化%运算。例如：（16 % 5 ）等价于 （16 &amp; （5 - 1））\n为了能把数据分配均匀，Hash值的范围是-2147483648 到 2147483647，很难碰撞，但是需要对数组取模，操作如上\n\n\n因为容量变大，位置会发生变化，将每个节点的hash值与新的容量取模，取模操作仍可以用位运算来替代，但JDK8中优化为：如果node.hash&amp;oldCap == 0，则节点在新table数组中的下标不变；如果node.hash &amp; oldCap != 0，则节点在新table数组中的下标变为i+oldCap（i为在原数组的下标）\n扫描table数组中的每一条链表，根据节点的下标是否更改，将链表中的节点分配到lo链表和hi链表，lo链表中存储的是下标值未变的节点，hi链表存储的是下标值有所改变的节点。处理完一条链表后，将lo链表和hi链表分别存储到新的table数组中的对应位置\n\npublic V put(K key, V value) &#123;\n    return putVal(hash(key), key, value, false, true);\n&#125;\n\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent,\n               boolean evict) &#123;\n    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;\n    if ((tab &#x3D; table) &#x3D;&#x3D; null || (n &#x3D; tab.length) &#x3D;&#x3D; 0)\n        n &#x3D; (tab &#x3D; resize()).length; &#x2F;&#x2F;使用resize创建新table\n    if ((p &#x3D; tab[i &#x3D; (n - 1) &amp; hash]) &#x3D;&#x3D; null)&#x2F;&#x2F;数组中链表头不存在，初始化\n        tab[i] &#x3D; newNode(hash, key, value, null);\n    else &#123;&#x2F;&#x2F;数组中插入位置有链表头，遍历\n        Node&lt;K,V&gt; e; K k;\n        if (p.hash &#x3D;&#x3D; hash &amp;&amp;\n            ((k &#x3D; p.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k))))&#x2F;&#x2F;先检查第一个节点\n            e &#x3D; p;&#x2F;&#x2F;找到\n        else if (p instanceof TreeNode)\n            e &#x3D; ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);\n        else &#123;\n\t\t\t\t\t\t&#x2F;&#x2F;遍历链表\n            for (int binCount &#x3D; 0; ; ++binCount) &#123;\n                if ((e &#x3D; p.next) &#x3D;&#x3D; null) &#123;&#x2F;&#x2F;没找到，新建节点\n                    p.next &#x3D; newNode(hash, key, value, null);\n\t\t\t\t\t\t\t\t\t\t&#x2F;&#x2F; 如果链表元素个数大于等于TREEIFY_THRESHOLD（8）\n                    if (binCount &gt;&#x3D; TREEIFY_THRESHOLD - 1) &#x2F;&#x2F; -1 for 1st\n                        treeifyBin(tab, hash); &#x2F;&#x2F;树化？红黑树转换，并不会直接转换成红黑树\n                    break;\n                &#125;\n                if (e.hash &#x3D;&#x3D; hash &amp;&amp;\n                    ((k &#x3D; e.key) &#x3D;&#x3D; key || (key !&#x3D; null &amp;&amp; key.equals(k)))) &#x2F;&#x2F;找到\n                    break;\n                p &#x3D; e;&#x2F;&#x2F;继续遍历\n            &#125;\n        &#125;\n        if (e !&#x3D; null) &#123; &#x2F;&#x2F; existing mapping for key\n            V oldValue &#x3D; e.value;\n            if (!onlyIfAbsent || oldValue &#x3D;&#x3D; null)\n                e.value &#x3D; value;&#x2F;&#x2F;更新值\n            afterNodeAccess(e);&#x2F;&#x2F;见LinkedHashMap\n            return oldValue;\n        &#125;\n    &#125;\n    ++modCount;\n    if (++size &gt; threshold)\n        resize();\n    afterNodeInsertion(evict);&#x2F;&#x2F;见LinkedHashMap\n    return null;\n&#125;\n\nfinal Node&lt;K,V&gt;[] resize() &#123;\n    Node&lt;K,V&gt;[] oldTab &#x3D; table;\n    int oldCap &#x3D; (oldTab &#x3D;&#x3D; null) ? 0 : oldTab.length;\n    int oldThr &#x3D; threshold;\n    int newCap, newThr &#x3D; 0;\n    if (oldCap &gt; 0) &#123;\n        &#x2F;&#x2F; 超过最大值就不再扩充了，就只好随你碰撞去吧\n        if (oldCap &gt;&#x3D; MAXIMUM_CAPACITY) &#123;\n            threshold &#x3D; Integer.MAX_VALUE;\n            return oldTab;\n        &#125;\n        &#x2F;&#x2F; 没超过最大值，就扩充为原来的2倍\n        else if ((newCap &#x3D; oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;&#x3D; DEFAULT_INITIAL_CAPACITY)\n            newThr &#x3D; oldThr &lt;&lt; 1; &#x2F;&#x2F; double threshold\n    &#125;\n    else if (oldThr &gt; 0) &#x2F;&#x2F; initial capacity was placed in threshold\n        newCap &#x3D; oldThr;\n    else &#123;\n        &#x2F;&#x2F; signifies using defaults\n        newCap &#x3D; DEFAULT_INITIAL_CAPACITY;\n        newThr &#x3D; (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);\n    &#125;\n    &#x2F;&#x2F; 计算新的resize上限\n    if (newThr &#x3D;&#x3D; 0) &#123;\n        float ft &#x3D; (float)newCap * loadFactor;\n        newThr &#x3D; (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);\n    &#125;\n    threshold &#x3D; newThr;\n    @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)\n        Node&lt;K,V&gt;[] newTab &#x3D; (Node&lt;K,V&gt;[])new Node[newCap];\n    table &#x3D; newTab;\n    if (oldTab !&#x3D; null) &#123;\n        &#x2F;&#x2F; 把每个bucket都移动到新的buckets中\n        for (int j &#x3D; 0; j &lt; oldCap; ++j) &#123;\n            Node&lt;K,V&gt; e;\n            if ((e &#x3D; oldTab[j]) !&#x3D; null) &#123;\n                oldTab[j] &#x3D; null;\n                if (e.next &#x3D;&#x3D; null)\n                    newTab[e.hash &amp; (newCap - 1)] &#x3D; e;\n                else if (e instanceof TreeNode)\n                    ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);\n                else &#123;\n                    Node&lt;K,V&gt; loHead &#x3D; null, loTail &#x3D; null;\n                    Node&lt;K,V&gt; hiHead &#x3D; null, hiTail &#x3D; null;\n                    Node&lt;K,V&gt; next;\n                    do &#123;\n                        next &#x3D; e.next;\n                        &#x2F;&#x2F; 原索引\n                        if ((e.hash &amp; oldCap) &#x3D;&#x3D; 0) &#123;\n                            if (loTail &#x3D;&#x3D; null)\n                                loHead &#x3D; e;\n                            else\n                                loTail.next &#x3D; e;\n                            loTail &#x3D; e;\n                        &#125;\n                        &#x2F;&#x2F; 原索引+oldCap\n                        else &#123;\n                            if (hiTail &#x3D;&#x3D; null)\n                                hiHead &#x3D; e;\n                            else\n                                hiTail.next &#x3D; e;\n                            hiTail &#x3D; e;\n                        &#125;\n                    &#125; while ((e &#x3D; next) !&#x3D; null);\n                    &#x2F;&#x2F; 原索引放到bucket里\n                    if (loTail !&#x3D; null) &#123;\n                        loTail.next &#x3D; null;\n                        newTab[j] &#x3D; loHead;\n                    &#125;\n                    &#x2F;&#x2F; 原索引+oldCap放到bucket里\n                    if (hiTail !&#x3D; null) &#123;\n                        hiTail.next &#x3D; null;\n                        newTab[j + oldCap] &#x3D; hiHead;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n    return newTab;\n&#125;\n链表树化：降低单个链表长度（jdk1.8新增的特性，1.7仅有链表）\n\n当某个链表中的节点个数大于等于8（TREEIFY_THRESHOLD静态常量），并且table数组的大小大于等于64时，将会把链表转化为红黑树，这个过程就叫treeify（树化）\n\n如果table数组长度小于64，即便链表中的节点个数大于等于8，也不会触发treeify，而是触发扩容操作，将长链表拆分为短链表\n\n当红黑树中节点个数比较少时，HashMap会再将其转换回链表，因为维护红黑树的成本比较高，对于少许节点，使用链表存储更高效，红黑树转换为链表的过程，叫做untreeify，促发untreeify的场景有以下两个：\n\n删除键值对：如果红黑树满足以下结构，则会触发untreeify，这个结构的红黑树的节点个数应该处于[2,6]之间，尽管treeify的阈值是8，但untreeify的阈值是[2,6]之间的某个数，之所以不相等是为了避免频繁的插入删除操作，导致节点个数在7，8之间频繁波动\n&#x2F;&#x2F;removeTreeNode函数中\nif (root &#x3D;&#x3D; null || root.right &#x3D;&#x3D; null ||\n    (rl &#x3D; root.left) &#x3D;&#x3D; null || rl.left &#x3D;&#x3D; null) &#123;\n    tab[index] &#x3D; first.untreeify(map);  &#x2F;&#x2F; too small\n    return;\n&#125;\n扩容：每一条链表都会分割为lo和hi两条，同理红黑树也会分割为lt和ht两个红黑树，lt中存储的是下标位置不变的节点，ht中存储的是下标位置变化的节点。不过，在构建lt和ht之前，会先统计属于lt和ht的节点个数lc和hc，如果lc小于等于6（UNTREEIFY_THRESHOLD静态常量），在新的table数组中，HashMap会使用链表来存储下标不变的节点，同理，如果hc小于等于6，在新的table数组中，HashMap会使用链表来存储下标改变的节点。\n\n\n\n\n\n红黑树是一种自平衡的二叉查找树，可以保证在最坏情况下基本动态操作的时间复杂度为O(log n)。红黑树中的每个节点都有一个颜色属性，可以是红色或黑色。红黑树满足以下5个性质。通过这些性质，红黑树可以保证在插入和删除节点时，自动调整树的结构，以保持树的平衡和性质的满足。相比于普通的二叉查找树，红黑树的平衡性更好，查找、插入和删除都具有更稳定的时间复杂度，因此在很多场景下被广泛应用。\n\n每个节点要么是红色，要么是黑色。\n根节点是黑色的。\n每个叶子节点（NIL节点，空节点）是黑色的。\n如果一个节点是红色的，则它的两个子节点都是黑色的。\n对于每个节点，从该节点到其所有后代叶子节点的简单路径上，均包含相同数目的黑色节点。\n\n\n\n\nCollections\n\nsort()：用来对List进行排序，默认为从小到大，支持传入Comparator接口的匿名类改为降序，底层依赖Arrays\n基本类型数组排序算法：JDK8及以后使用DualPivotQuickSort()，JDK7及其以前使用快排，使用不稳定排序\nDualPivotQuickSort根据长度和元素类型，使用双轴快速排序算法、插入排序、计数排序、归并排序等算法来组合进行排序操作\n\n\n对象数组排序算法：JDK8及其以后使用TimSort()，JDK7及其以前使用归并排序，使用的是稳定的排序方式\nTimSort用非递归版本归并排序，归并到阈值后开始进行二分插入排序算法，即在插入时选择用二分查找来确定插入位置\n\n\n\n\nbinarySearch()：用来对已排序的List容器进行二分查找，因为涉及元素比较，所以需要传入实现Comparable接口的对象或者主动传入Comparator接口的匿名类对象\nindexedBinarySearch：查找mid使用的是链表的get函数，需要从头遍历链表来得到对应值\niteratorBinarySearch：查找mid使用的是新定义的get函数，从上一次迭代器的位置（mid）开始向前或向后查找，需要遍历的范围变小了，执行效率就变高了\n\n\nsynchronizedXXX()：JCF中的容器都是非线程安全的，当要使用线程安全的容器时，首选使用JUC并发容器，但当没有合适的JUC并发容器可以使用时，可以使用Collectinos类中的synchronizedXXX()函数来创建线程安全的容器\n\n\n\n4.IO类库\n\nIO类库（装饰器模式、适配器模式、工厂模式、观察者模式）\n\n明确要操作的数据是数据源还是数据目的（要读还是要写）+要操作的数据是字节还是字符（字符流比字节流多了一个字符编码转换的环节）\n\n输入（读）：InputStream （字节）、Reader（字符）\n\n\n输出（写）：OutputStream（字节）、Writer（字符）\n\n\n装饰器模式：实现相同接口，使用组合调用被装饰方法，并在前后增加上新的装饰方法\n&#x2F;&#x2F; 装饰器模式的代码结构(下面的接口也可以替换成抽象类)\npublic interface IA &#123;\n  void f();\n&#125;\npublic class A implements IA &#123;\n  public void f() &#123; &#x2F;&#x2F;... &#125;\n&#125;\npublic class ADecorator implements IA &#123;\n  private IA a;\n  public ADecorator(IA a) &#123;\n    this.a &#x3D; a;\n  &#125;\n  \n  public void f() &#123;\n    &#x2F;&#x2F; 功能增强代码\n    a.f();\n    &#x2F;&#x2F; 功能增强代码\n  &#125;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;装饰器类是对原始类的增强，不能独立使用，使用方式如下&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nInputStream in &#x3D; new FileInputStream(&quot;...&quot;);\nInoutStream bin &#x3D; new BufferedInputStream(in);\nbyte[] data &#x3D; new byte[1024];\nwhile(bin.read(data) !&#x3D; -1)&#123;\n    &#x2F;&#x2F;处理data数组\n&#125;\n\n\n明确数据存在的具体设备\n\n硬盘（文件）：Filexxx（4）\nInputStream fis &#x3D; new FileInputStream(&quot;input.txt&quot;)\nwhile ((content &#x3D; fis.read()) !&#x3D; -1) &#123;\n    System.out.print((char) content);\n&#125;\n\nFileOutputStream output &#x3D; new FileOutputStream(&quot;output.txt&quot;)\nbyte[] array &#x3D; &quot;JavaGuide&quot;.getBytes();\noutput.write(array);\n管道：Pipedxxx（4）\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;Java中的管道是同一个进程内的两个线程之间通信的工具&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\nPipedOutputStream out &#x3D; new PipedOutputStream();\ntry &#123;\n    PipedInputStream in &#x3D; new PipedInputStream(out);\n    new Thread(new Runnable() &#123;\n        @Override\n        public void run() &#123;\n            try &#123;\n                out.write(&quot;Hi Dajunnnnnn&quot;.getBytes());\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;).start();\n    new Thread(new Runnable() &#123;\n        @Override\n        public void run() &#123;\n            byte[] buffer &#x3D; new byte[512];\n            try &#123;\n                in.read(buffer);\n                System.out.println(new String(buffer));\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;).start();\n&#125; catch (IOException e) &#123;\n    e.printStackTrace();\n&#125;\n内存：CharArrayxxx（字符）、Stringxxx（字符）、ByteArrayxxx（字节）\n\n在大部分情况下，直接对byte数组，char数组进行读写即可，不需要这种内存读写类\n\n情境一：实现兼容，调用第三方类库中的某个函数来处理byte数组中的数据时，但这个函数的入参是InputStream类型的，那么就需要将待处理byte数组封装成ByteArrayInputStream对象，在传递给这个函数\nbyte[] source &#x3D; &quot;测试数据&quot;.getBytes();\nInputStream in &#x3D; new ByteArrayInputStream(source);\n&#x2F;&#x2F;用in代替source继续处理\n情景二：编写单元测试时，这些内存读写类可以替代文件或网路，将测试数据内置于内存，准备起来更加容易\n&#x2F;&#x2F;待测试函数\npublic int readFromFIle(InputStream inputStream)&#123;...&#125;\n\n&#x2F;&#x2F;测试代码\npublic void test_readFromFile()&#123;\n    byte[] testData &#x3D; new byte[512];\n    &#x2F;&#x2F;构建测试数据，填入testData数组\n    InputStream in &#x3D; new ByteInputStream(testData);\n    int res &#x3D; readFromFile(in);\n    &#x2F;&#x2F;assert 判断返回值是否符合预期\n&#125;\n\n\n键盘/屏幕：System.in、System.out、System.err（使用内部的PrintStream和InputStream）\n\n定义在System类中的静态InputStream对象\n定义在System类中的静态PrintStream，需要嵌套OutputStream来使用\n\n\n网络：Socket\n&#x2F;&#x2F;java.io类库并没有提供专门的类用于网络I&#x2F;O的读写，而是直接复用InputStream&#x2F;OutputStream类进行网络I&#x2F;O的读写\nSocket socket &#x3D; new Socket(&quot;127.29.2.4&quot;,8090);\nOutputStream out &#x3D; socket.getOutputStream();\nout.write(&quot;hi&quot;.getBytes());\n\nInputStream in &#x3D; socket.getInputStream();\nbyte[] data &#x3D; new byte[1024];\nwhile(in.read(data) !&#x3D; -1)&#123;\n    &#x2F;&#x2F;do something\n&#125;\n\n\n明确是否需要额外的功能\n\n需要高效（缓冲流，用来包装别的类）：Bufferedxxx（4），在内存中维护一个8192字节的缓存区\n\nBufferedInputStream会在内存中维护一个8192字节大小的缓存，如果缓存中没有足够的数据，那么read()函数会从I/O设备中读取8192个字节存储到缓存中，然后read()函数再从缓存中返回需要的数据量。如果缓存中有足够多的数据，read()函数直接从缓存中读取数据，而不会触发真正I/O操作，可以减少I/O操作的次数，但是如果每次请求的数据量大于等于8192字节，那么BufferedInputStream就不起作用了\n同理OutputStream用于缓存写入I/O设备中的数据，当积攒到一定量（默认为8192字节），再一次性将其写入I/O设备，减少I/O操作的次数，提高程序的性能\n\n&#x2F;&#x2F; 新建一个 BufferedInputStream 对象，需要传入一个原始类对象，通过装饰器设计模式来增加功能\nBufferedInputStream bufferedInputStream &#x3D; new BufferedInputStream(new FileInputStream(&quot;input.txt&quot;));\n&#x2F;&#x2F; 读取文件的内容并复制到 String 对象中\nString result &#x3D; new String(bufferedInputStream.readAllBytes());\nSystem.out.println(result);\n支持基本类型数据读写：DataInputStream、DataOutStream\n\nDataInputStream支持将输入流中读取的数据解析为基本类型（byte、char、short、int、float、double等），DataOutputStream类支持将基本类型数据转化为字节数组写入输出流\n\nDataOutputStream out &#x3D; new DataOutputStream(new FileOutputStream(&quot;...&quot;));\nout.writeInt(12);\nout.writeChar(&quot;a&quot;);\nout.writeFloat(12,12f);\nout.close();\n\nDataIntputStream in &#x3D; new DataInputStream(new FileInputStream(&quot;...&quot;));\nSystem.out.println(in.readInt());\n&#x2F;&#x2F;readChar()、writeChar()也可以按字符为单位读取、写入数据，但是，DataInputStream一次只能处理一个字符，\n&#x2F;&#x2F;而字符流可以处理char数组，并且字符流提供的函数更多，功能更丰富\nSystem.out.println(in.readChar());\nSystem.out.println(in.readFloat());\nin.close();\n支持对象读写：ObjectInputStream、ObjectOutputStream\n&#x2F;&#x2F;ObjectOutputStream支持将对象序列化之后写入到输出流\nObjectOutputStream out &#x3D; new ObjectOutputStream(new FileOutputStream(&quot;.&quot;));\nout.writeObject(new Person(12,&quot;Dajunnnnnn&quot;));\n&#x2F;&#x2F;ObjectInputStream支持将从输入流中读取到的数据反序列化为对象\nObjectInputStream in &#x3D; new ObjectInputStream(new FileInputStream(&quot;.&quot;));\nPerson p &#x3D; (Person) in.readObject();\n保证数据的输出形式（打印流）：PrintStream、PrintWriter\n&#x2F;&#x2F;PrintStream和PrintWrite可以将数据按照一定的格式，转化为字符串，写入到输出流\nPrintStream printStream &#x3D; new PrintStream(new FileOutputStream(&quot;..&quot;));\nprintStream.print(124);&#x2F;&#x2F;int-&gt;Integer-&gt;toString(),写入字符串“124”\nprintStream.print(&quot;hello %d&quot;,43);&#x2F;&#x2F;写入字符串“hello 43”\n需要转换（字符流通向字符的桥梁）：InputStreamReader、OutputStreamWriter\n\nInputStreamReader可以充当InputStream的装饰器类，OutputStreamWriter可以充当OutputStream的装饰器类，它们可以将字节流转化为字符流\n\nOutputStream outStream &#x3D; new FileOutputStream(&quot;&#x2F;Users&#x2F;wangzheng&#x2F;a.txt&quot;);\nOutputStreamWriter writer &#x3D; new OutputStreamWriter(outStream, &quot;gbk&quot;);\nwriter.write(&quot;王a争&quot;); &#x2F;&#x2F;按照gbk编码将字符串写入文件\n回退（允许读取字节，然后再将它们回推到流中）：PushbackInputStream、PushbackReader\nString s &#x3D; &quot;abcdefg&quot;;\ntry (ByteArrayInputStream in &#x3D; new ByteArrayInputStream(s.getBytes());\n\t\t\t\tPushbackInputStream pbin &#x3D; new PushbackInputStream(in)) &#123;\n    int n;\n    while ((n &#x3D; pbin.read()) !&#x3D; -1) &#123;\n        System.out.print((char) n);\n        if(&#39;b&#39; &#x3D;&#x3D; n) pbin.unread(&#39;U&#39;);\n    &#125;&#x2F;&#x2F;输出 abUcdefg\n&#125;\n多个源（序列流）：SequenceInputStream\n\n\n\n\n\nNIO类库（JDK1.4引入，也称：New I/O、Non-blocking I/O、Network I/O，主要用于网络编程）\n\n核心概念：java.nio中引入Channel代替Stream，并且引入新的概念：Buffer，用来存储待写入或读取的数据\n\nBuffer：在IO库中，通常使用byte数组来接收数据，分为字节流解析和字符流解析。在nio中， 将将这些部分抽离出来，封装到Buffer中，通过不同的Channel和不同的Buffer组合在一起，实现不同的IO读写需求（常见的Buffer有：ByteBuffer、CharBuffer、ShortBuffer、IntBuffer、LongBuffer、FloatBuffer、DoubleBuffer、MappedByteBuffer）\nFileChannel channel &#x3D; FileChannel.open(Paths.get(&quot;...&quot;));\nByteBuffer buffer &#x3D; ByteBuffer.allocate(512);\nwhile(channel.read(buffer) !&#x3D; -1)&#123;\n    &#x2F;&#x2F;处理buffer中的数据data\n&#125;\nChannel：同步（FileChannel、DatagramChannel、SocketChannel、ServerSocketChannel），异步（AdynchronousFileChannel、AsynchronousSocketChannel、AsynchronousServerSocketChannel）\n\n同步\n常用的同步的Channel有：FileChannel（文件）、DatagramChannel（UDP）、SocketChannel（TCP）、ServerSocketChannel（TCP，服务器，即可以使用accept()函数监听客户端SocketChannel的连接请求）\nChannel既可以读也可以写，每个Channel类通过实现不同的接口组合，来支持不同的功能组合\nChannel有两种运行方式：阻塞（等待数据读写）和非阻塞（不等待数据读写）方式，其中除FileChannel只支持阻塞模式外，其余三个都同时支持两种方式，默认为阻塞方式，可以调用configureBlocking(false)函数将其设置为非阻塞模式，非阻塞Channel一般会配合Selector，用于实现多路复用I/O模型\n\n\n异步\njdk7在已有Selector的情况下，进行了升级，引入了支持异步模式的Channel，主要包括：AdynchronousFileChannel、AsynchronousSocketChannel、AsynchronousServerSocketChannel\n在异步模式下，Channel不再注册到Selector，而是注册到操作系统内核中，由内核来通知某个Channel可读、可写或可连接，java.nio收到通知之后，为了不阻塞主线程，会使用线程池去执行事先注册的回调函数\n\n\n\n\nSelector：注册Channel到Selector，Selector隔一段时间轮询是否有Channel可读、可写、可连接\n\n多路复用I/O：用来解决while轮询的问题。为了实现多路复用，Unix提供了epoll库、Windows提供了iocp库、BSD提供了kequeue库，Java作为一种跨平台语言，对不同操作系统的实现进行了封装，提供了统一的Selector，可以将需要监听的Channel，调用registor()函数，注册到Selector中，Selector底层会通过轮询的方式，查看哪些Channel可读、可写、可连接等，并将其返回处理（避免手写轮询代码）\n\nSeverSocketChannel serverChannel &#x3D; ServerSocketChannel.open();\nserverChannel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;,1192));\nserverChannel.configureBlocking(false);\nByteBuffer buffer &#x3D; ByteBuffer.allocate(1024);\n\nSocketChannel clinetChannel &#x3D; null;\n&#x2F;&#x2F;在网络编程中，使用非阻塞模式，线程需要通过while循环，不停轮询调用read()、write()、accept()函数，\n&#x2F;&#x2F;查看是否有数据可读，是否可写，是否有客户端连接到来\nwhile(clinetChannel &#x3D;&#x3D; null)&#123;\n    clientChannel &#x3D; serverChannel.accept();\n&#125;\n\nwhile(clientChannel.read(buffer) &#x3D;&#x3D; -1);\n\nbuffer.flip();&#x2F;&#x2F;将buffer从用于读变成用于写\nwhile(buffer.hasRemaining())&#123;\n    clientChannel.write(buffer);&#x2F;&#x2F;echo,读了啥就写啥\n&#125;\n\n\nJava IO模型：堆Unix5种I/O模型：阻塞I/O模型、非阻塞I/O模型、多路复用I/O模型、信号驱动I/O模型、异步I/O模型的封装\n\n阻塞I/O模型（BIO）：阻塞IO+线程，多线程+read阻塞等待\n\n利用阻塞模式搭配多线程来实现服务器，因为read()是阻塞函数，所以需要提前创建线程池和大量线程，来等待客户端发来的连接。如果有n个客户端连接服务器，那么服务器需要创建n+1个线程，其中n个线程用于调用read()函数，1个线程用来调用accept()函数接收连接。当线程比较多时，内存资源的消耗就会比较大。\n同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间；在客户端连接数量不高时没问题，当面对十万甚至百万连接时会无能为力\n\npublic class BioEchoServer &#123;\n    public static void main(String[] args) throws IOException &#123;\n        ServerSocket serverSocket &#x3D; new ServerSocket();\n        serverSocket.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 1192));\n        while (true) &#123;\n            &#x2F;&#x2F; accept()为阻塞函数，直到有连接到来才返回\n            Socket clientSocket &#x3D; serverSocket.accept();\n            &#x2F;&#x2F; 为每个客户端单独创建一个线程处理\n            new Thread(new ClientHandler(clientSocket)).start();\n        &#125;\n    &#125;\n\n    private static class ClientHandler implements Runnable &#123;\n        private Socket socket;\n        public ClientHandler(Socket socket) &#123;\n            this.socket &#x3D; socket;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            byte[] data &#x3D; new byte[1024];\n            while (true) &#123; &#x2F;&#x2F;持续接收客户端发来的数据\n                try &#123;\n                    &#x2F;&#x2F; read()为阻塞函数，直到读取到数据再返回\n                    socket.getInputStream().read(data);\n                    &#x2F;&#x2F; write()为阻塞函数，全部写完成才会返回\n                    socket.getOutputStream().write(data); &#x2F;&#x2F;echo\n                &#125; catch (IOException e) &#123;\n                    &#x2F;&#x2F; log and exit\n                    break;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n非阻塞I/O模型（NIO）：Selector+非阻塞IO，注册channel（不会一直占用线程） + Selector多路复用（隔一段时间轮询，找能执行的channel）\n\n非阻塞模型利用非阻塞模式和Selector多路复用器来开发服务器，也叫做多路复用I/O模型。只有实现了SelectableChannel接口的Channel才可以注册到Selector中被监听，比如DatagramChannel、SocketChannel、ServerSocketChannel，FileChannel无法被Selector监听\n\n在NioEchoServer类中，如果有n可客户端连接服务器，那么就会创建n+1个Channel，其中一个serverChannel用于接受客户端的连接，另外n个clientChannel用于与客户端进行通信。这n+1个Channel均注册到Selector中。Selector会间隔一定时间轮训这n+1个Channel，查找可连接、可读、可写的Channel，然后再进行连接、读取、写入操作\n\n在NIO中，主线程通常只有一个，但是可以使用Selector来管理多个Channel，实现多个连接的非阻塞读写操作。当有多个Channel需要进行IO操作时，Selector会轮询这些Channel，检查它们的状态是否可读或可写，如果有可读或可写的Channel，就将其加入到一个已选择键集合中，等待程序处理。这样，一个线程就可以同时处理多个Channel，提高了系统的并发处理能力\nNIO底层是用Selector、Channel和ByteBuffer来实现的。主线程在循环使用select方法进行阻塞等待，当有acceptable、readable或者writable事件发生的时候，循环就会往下走，将对应的事件交给对应的事件处理器进行处理\nSelector是一个可以监控多个通道（Channel）是否有数据可读或可写的对象，当一个或多个Channel准备好读或写时，Selector会通知程序进行读写操作，而不是像BIO一样阻塞等待IO操作完成\n\n\n多路复用I/O模型：只需要一个线程即可，解决了阻塞I/O模型线程开销大的问题。但是如果某些clientChannel耗时比较久，那么其它clientChannel便需要阻塞，使得服务器响应的延迟变高，但可以用过线程池中取线程来处理，而不是所有的clientChannel都在一个线程中处理。跟非阻塞I/O的区别在于不管有没有数据可读，阻塞I/O模型中的每个clientSocket都会一直占用线程。而这里的多线程只会处理经过Selector筛选之后有可读数据的clientChannel，并且处理完之后就释放回线程池，线程的利用率更高\n\n\npublic class NioEchoServer &#123;\n    public static void main(String[] args) throws IOException &#123;\n        &#x2F;&#x2F; Selector\n        Selector selector &#x3D; Selector.open();\n\n        &#x2F;&#x2F; create serverChannel and register to selector\n        ServerSocketChannel serverChannel &#x3D; ServerSocketChannel.open();\n        serverChannel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 1192));\n        serverChannel.configureBlocking(false);&#x2F;&#x2F;非阻塞\n      \t&#x2F;&#x2F;注册\n        serverChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n        ByteBuffer buffer &#x3D; ByteBuffer.allocate(1024);\n        while (true) &#123;\n            int channelCount &#x3D; selector.select(); &#x2F;&#x2F;取来准备好的selector\n            if (channelCount &gt; 0) &#123;\n                Set&lt;SelectionKey&gt; keys &#x3D; selector.selectedKeys();\n                Iterator&lt;SelectionKey&gt; iterator &#x3D; keys.iterator();\n                while (iterator.hasNext()) &#123;\n                    SelectionKey key &#x3D; iterator.next();&#x2F;&#x2F;链表\n                    if (key.isAcceptable()) &#123;\n                        &#x2F;&#x2F; create clientChannel and register to selector\n                        SocketChannel clientChannel &#x3D; serverChannel.accept();\n                        clientChannel.configureBlocking(false);&#x2F;&#x2F;非阻塞\n                        clientChannel.register(selector, SelectionKey.OP_READ);\n                    &#125; else if (key.isReadable()) &#123;\n                        SocketChannel clientChannel &#x3D; (SocketChannel) key.channel();\n                        clientChannel.read(buffer);\n                        buffer.flip(); &#x2F;&#x2F;从&quot;用于读&quot;变为&quot;用于写&quot;\n                        if (buffer.hasRemaining())&#123;&#x2F;&#x2F;也可以注册到selector中\n                            clientChannel.write(buffer); &#x2F;&#x2F;echo\n                        &#125;\n                        buffer.clear(); &#x2F;&#x2F;重复利用\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n异步I/O模型（AIO）：异步IO，通过异步Channel，数据读取完成后会执行回调函数\n\n与NIO不同的是，AIO不需要用户线程等待IO操作完成，而是由操作系统来完成IO操作，操作系统完成IO操作后会通知用户线程处理。AIO适用于连接数较多且连接时间较长的场景，如高性能网络服务器等\n通过异步Channel调用accept()、read()、write()函数。当有连接建立、数据读取完成、数据写入完成时，底层会通过线程池执行对应的回调函数。这种服务器的实现方式叫做异步I/O模型\n\npublic class AioEchoServer &#123;\n    public static void main(String[] args) throws IOException, InterruptedException &#123;\n        AsynchronousServerSocketChannel serverChannel &#x3D; AsynchronousServerSocketChannel.open();\n        serverChannel.bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 1192));\n        &#x2F;&#x2F; 异步accept()\n        serverChannel.accept(null, new AcceptCompletionHandler(serverChannel));\n        Thread.sleep(Integer.MAX_VALUE);\n    &#125;\n\n    private static class AcceptCompletionHandler implements CompletionHandler&lt;AsynchronousSocketChannel, Object&gt; &#123;\n        private AsynchronousServerSocketChannel serverChannel;\n        public AcceptCompletionHandler(AsynchronousServerSocketChannel serverChannel) &#123;\n            this.serverChannel &#x3D; serverChannel; \n        &#125;\n\n        @Override\n        public void completed(AsynchronousSocketChannel clientChannel, Object attachment) &#123;\n            &#x2F;&#x2F; in order to accept other client&#39;s connections\n            serverChannel.accept(attachment, this);\n            ByteBuffer buffer &#x3D; ByteBuffer.allocate(1024);\n            &#x2F;&#x2F; 异步read()\n            clientChannel.read(buffer, buffer, new ReadCompletionHandler(clientChannel)); \n        &#125;\n\n        @Override\n        public void failed(Throwable exc, Object attachment) &#123;\n            &#x2F;&#x2F; log exc exception\n        &#125;\n    &#125;\n\n    private static class ReadCompletionHandler implements CompletionHandler&lt;Integer, ByteBuffer&gt; &#123;\n        private AsynchronousSocketChannel clientChannel;\n        public ReadCompletionHandler(AsynchronousSocketChannel clientChannel) &#123;\n            this.clientChannel &#x3D; clientChannel;\n        &#125;\n\n        @Override\n        public void completed(Integer result, ByteBuffer buffer) &#123;\n            buffer.flip();\n            &#x2F;&#x2F; 异步write()。回调函数为null，写入完成就不用回调了\n            clientChannel.write(buffer, null, null); &#x2F;&#x2F; echo\n        &#125;\n\n        @Override\n        public void failed(Throwable exc, ByteBuffer attachment) &#123;\n            &#x2F;&#x2F; log exc exception\n        &#125;\n    &#125;\n&#125;\n\n\n\n\n文件（高速IO）：上下文切换耗时（环境重置、缓存失效）、系统调用（read、write、open、close）、新技术（DMA、mmap、零拷贝）\n\n用户态和内核态\n\n系统调用：操作系统内核包含各种操作硬件资源的系统调用，应用程序必须通过操作系统提供的系统调用才能访问硬件资源。\n库函数：系统调用比较底层，所以Linux又提供了库函数，比如Glibc库、Posix库，对系统调用进行封装，提供更加简单易用的函数，供应用程序开发使用，比如：Glibc中的malloc()函数封装了sbrk()系统调用，fread()、fwrite()封装了read()、write()系统调用，在开发应用程序的时候，既可以使用库函数，也可以直接使用系统调用\nShell：Linux还提供了Shell这一程序，即命令行，Shell能在不进行编程的情况下，通过命令行中运行Shell命令或脚本，达到访问硬件的目的，比如cp拷贝文件、rm删除文件\n用户态&amp;内核态：为避免应用程序在运行时，访问到内核所用的内存空间，操作系统将虚拟内存分为内核空间和用户空间两部分，CPU因此有内核态和用户态两种，在内核态CPU拥有最高权限，可以执行所有的机器指令并且可以访问硬件，而且内核态能访问所有虚拟内存空间，在用户态则不能\n上下文切换：当应用调用操作系统的系统调用时，会涉及内核态与用户态的上下文切换，主要耗时的操作有：\n寄存器保存与恢复耗时：因为内核空间不使用应用程序的函数调用栈，会分配新的函数调用栈，所以在上下文切换时需要更新更多栈相关的寄存器，比如SS栈基址寄存器。除此之外，应用程序和内核程序的代码存储位置也不同，CS代码段基址寄存器也需要更新。并且更新前会保存下来原始值，以便切换回用户态之后恢复执行\n缓存失效带来的性能损耗：CPU有L1、L2、L3三级Cache，用于缓存将要执行的代码以及所需的内存数据，上下文切换会导致CPU缓存失效\n\n\n\n\nIO读写底层原理\n\nLinux操作系统下，Java的I/O类库调用open()、read()、write()系统调用来实现，通过open()返回Linux下I/O设备的文件描述符，来和I/O设备建立连接。\n操作系统为每个文件描述符都分配一个内核读缓存区和一个内核写缓存区（数据会先被放到内核读写缓存区，读缓冲区不够时才会从磁盘读取文件，写缓冲区满时才会写入到磁盘中），内核读写缓存区只有在第一次调用read()或write()系统调用时，才会真正被分配内存空间。默认读缓冲区的大小为8192字节，写缓冲区的大小为16384字节。当然，也可以根据业务需求，通过系统调用，来重新设置，这样做的目的主要是为了减少与I/O设备的交互次数\n在读写完成后需要调用close()系统调用\n\n\n新技术：DMA（替代CPU读写数据）、mmap（将大文件映射到虚拟内存地址上）、零拷贝（直接从内核读缓冲区拷贝到内核写缓冲区）\n\nDMA（Direct Memory Access）：通过在主板上安装一个叫做DMAC (DMA Controller，DMA控制器)的协处理器(或叫芯片)，协助CPU来完成I/O设备的数据读写工作（现在很多IO设备都自带DMAC）。DMAC替代CPU从设备中读取数据或向设备写入数据，通过中断通知CPU，CPU利用率提高了\n\n\nmmap（memory-mapped file，内存映射文件）：提高文件读写性能的有效技术，一般用于文件读写，不适用于网络这种数据未知的I/O设备\n&#x2F;&#x2F;java.nio.FileChannel类中\npublic MappedByteBuffer map(MapMode mode , long position, long size);\n\n\n通过将文件或文件中的某段映射到用户空间中的某段虚拟内存地址上，如果没加载到物理内存，则触发缺页中断；如果有脏页，操作系统自动写回磁盘或者调用msync()立即写回\nmmap相当于直接将数据在磁盘和用户空间之间互相拷贝，相对于使用read()、write()系统调用读写文件，数据拷贝次数由2次减少为1次，并且减少了内核态和用户态上下文切换的耗时，之后读写文件就像读写内存一样\n对于少量文件读写，使用read()、write()更合适，对于大文件的读写，一般使用mmap，并且需要一些测试来验证性能。进程间通信当两个应用程序都采用MAP_SHARED模式创建匿名的内存映射文件时，这两个应用程序会共享物理内存，一个应用程序可以读取另一个程序写入物理内存的数据，以此来实现互相通信\n\nint main(void)&#123;\n    char file &#x3D; &quot;&#x2F;users&#x2F;root&#x2F;in.txt&quot;;\n    int fd &#x3D; open(file, O_RDWR,0666);\n    if(fd &lt;0)&#123;\n        printf(&quot;open file failedl\\\\n&quot;);\n        return -1;\n    &#125;\n    &#x2F;&#x2F;映射文件开头(offset&#x3D;0)的512字节(length&#x3D;512)到ptr\n    size_t length &#x3D; 512;\n    int offset &#x3D; 0;\n\t\t&#x2F;&#x2F;mmap，后面就和使用fd一样了\n    char *ptr &#x3D; mmap(null, length, PROT_READ|PROT_WRITE, MAP_SHARED, fd , offset);\n    if (ptr &#x3D;&#x3D; MAP_FAILED)&#123;\n        printf(&quot;mmap failed.&quot;);\n        return -1;\n    &#125;\n    &#x2F;&#x2F;创建好内存映射文件之后，fd就没用了，可以释放了\n    close(fd);\n    \n    &#x2F;&#x2F;操作ptr就等同于读写文件\n    for (int i &#x3D; 0; i &lt; length; i++)&#123;\n        ptr[i] &#x3D; &#39;a&#39; + (length%26);\n    &#125;\n\n    for (int i &#x3D; 0; i &lt;N, i++)&#123;\n        printf(&quot;%c&quot;,ptr[i]);\n    &#125;\n    &#x2F;&#x2F;删除内存映射文件，释放占用的虚拟内存空间\n    munmap(ptr, length);\n    return 0;\n&#125;\n零拷贝（Zero-copy）：sendfile 系统调用实现了零拷贝技术，零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运，使用零拷贝的项目有nginx、kafka\n\n\n主要用于两个I/O设备之间互相传输数据，特别是将文件中的数据发送到网络或者将从网络接受的数据存储到文件这一场景中\n&#x2F;&#x2F;java.nio.FileChannel\npublic abstract long transferTo(long position,long count WritableByteChannel target);\npublic abstract long transferFrom(ReadableByteChannel src, long position,long count);\n零拷贝不需要将数据拷贝到应用程序缓冲区，而是直接从内核读缓冲区拷贝到内核写缓冲区，应用程序只需要进行一次系统调用（执行sendfile()），就可以将文件发送到网络\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;sys&#x2F;sendfile.h&gt;\n#include &lt;sys&#x2F;stat.h&gt;\n#include sys&#x2F;types.h&gt;\nint main (int argc, char*argv[)&#123;\n    int read_fd;\n    int write_fd;\n    struct stat stat_buf;\n    off_t offset &#x3D; o;\n    read_fd &#x3D; open (argv[1],O_RDONLY);\n    fstat (read_fd, &amp;stat_buf) ;\n    write_fd &#x3D; open (argv[2],O_WRONLY \\\\ O_CREAT, stat_buf.st_mode);\n    sendfile (write_fd, read_fd, &amp;offset, stat_buf.st_size);\n    close(read_fd);\n    close (write_fd);\n    return 0;\n&#125;\n\n\n\n\n\n\n\n5.Exception体系\n\n\n\n\n\n\n\n\n相比较C语言返回错误码的方式，可以携带更多的错误信息（message、stack trace等），并且可以将业务代码和异常处理代码分离，这样代码的可读性会更好\n\n异常体系\n\n\n继承自Error的异常：表示程序无法处理的严重错误，这些错误有可能导致线程或JVM终止\n继承自Exception的异常：也叫做受检异常（Checked Exception）或编译时异常（Compile Exception），在编写代码的时候，需要主动取捕获或者在函数定义中声明此类异常，否则编译就会报错\n继承自RuntimeException的异常：也叫做非受检异常（Unchecked Exception）或者运行时异常（Runtime Exception），在编写代码的时候，可以不主动取捕获和在函数定义中声明此类异常，不处理也可以通过编译\n\n\n自定义异常：要么继承自Exception，要么继承自RuntimeException，但是现在一般都依赖框架来编程，受检和非受检异常大部分情况下都会被框架兜底捕获并处理，并不会直接导致程序的终止，所以从这个角度来看，继承自哪个异常均可\n&#x2F;&#x2F;受检异常的使用违反开闭原则，整条调用链都需要修改代码；非受检异常需要主动处理，但容易被遗忘\npublic class UserNotExistingException extends Exception&#123;\n    public UserNotExistingException()&#123;\n        super();\n    &#125;\n    public UserNotExistingException(String msg,Throwable cause)&#123;\n        super(msg,cause)；\n    &#125;\n    public UserNotExistingException(String msg)&#123;\n        super(msg);\n    &#125;\n    public UserNotExistingException(Throwable cause)&#123;\n        super(cause);\n    &#125;\n&#125;\n异常处理\n\n打印调用链：在函数内部，如果某代码的异常行为，并不会导致调用此函数的上层代码出现异常行为，也就是说，上层代码并不关心被调用函数内部的这个异常，我们就可以在函数内部将这个异常捕获并打印日志记录\npublic void f() throws LowLevelException&#123;...&#125;\n&#x2F;&#x2F;捕获后记录日志\npublic void g()&#123;\n    try&#123;\n        f();\n    &#125;catch(LowLevelException e)&#123;\n        log.warn(&quot;...&quot;,e);&#x2F;&#x2F;使用日志框架记录日志\n    &#125;\n&#125;\n使用throws抛出异常：如果函数内部的异常行为会导致调用此函数的上层代码出现异常行为，那么，就必须让上层代码感知此异常的存在\n&#x2F;&#x2F;原封不动再抛出\n&#x2F;&#x2F;如果LowLevelException是非受检异常，则不需要再函数g()定义中声明\npublic void g() throws LowLevelException&#123;\n    f();\n&#125;\n使用new创建新的异常：如果此异常跟函数的业务相关，上层代码在调用此函数时，知道如何处理异常，那么直接将其抛出即可；如果此异常跟业务无关，上层代码无法理解这个异常的含义，那么就需要包装成新的跟函数业务相关的异常重新抛出\n&#x2F;&#x2F;包装成新异常抛出\npublic void g()&#123;\n    try&#123;\n        f();\n    &#125;catch(LowLevelExceptioin e)&#123;\n\t\t\t\t&#x2F;&#x2F;异常调用链可以完整的描述异常发生的整个过程，但需要特别注意的是，捕获异常并包裹成新的异常抛出时，\n\t\t\t\t&#x2F;&#x2F;一定要将先前的异常通过cause参数（下面代码中的e）传递进新的异常，否则，异常调用链会断开\n        throw new HighLevelException(&quot;...&quot;,e);\n    &#125;\n&#125;\n\n\n异常实现原理：异常代码块执行顺序：不管try监听的代码块有没有异常抛出，finally代码块总是被执行，并且在finally代码执行完成之后，try代码块和catch代码块中的return语句才会被执行\n\n\n异常表：对应于上图最后一部分的Exception table，其中from、to、target都表示字节码的行号，当行号在[from，to）之间的代码抛出type类型的异常时，JVM会跳转至target行字节码继续执行\n异常兜底：第50行代码开始，主要是捕获try代码块和catch代码块中未被捕获的异常，然后再执行完finally代码块之后，在原封不动的将异常抛出\nfinally内联：JVM在生成字节码时，会将finally代码块内联（插入）到try代码块和catch代码块中的return语句之前，这样就可以实现不管程序是否抛出异常，finally代码块总是会被执行，并且再函数返回之前执行。如果finally有return语句，会提前返回\n\n\n异常性能分析\n\n使用new创建异常：在堆上创建异常对象，初始化成员变量，调用异常父类Throwable中的fillInStackTrace()函数生成栈追踪信息，通过getStackTrace()函数打印stackTrace栈追踪信息（当调用层次过深时，会导致fillInStackTrace耗时高，所以在递归中不要轻易抛出异常）\n&#x2F;&#x2F;当创建异常时函数调用栈中的所有函数的信息，栈追踪信息记录了异常产生的整个函数调用链路，方便定位此异常是如何产生的\nprivate StackTraceElement[] stackTrace;\npublic final class StackTraceElement implements java.io.Serializable &#123;\n    &#x2F;&#x2F; Normally initialized by VM (public constructor added in 1.5)\n    private String declaringClass;&#x2F;&#x2F;函数所属类名\n    private String methodName;&#x2F;&#x2F;函数名\n    private String fileName;&#x2F;&#x2F;函数所属类文件名\n    private int    lineNumber;&#x2F;&#x2F;异常抛出时，函数执行到了哪一行\n    &#x2F;&#x2F;...\n&#125;\n&#x2F;&#x2F;通过getStackTrace()函数，将异常的stackTrace栈追踪信息打印出来\nRuntimeException e &#x3D; new RuntimeException(&quot;oops&quot;);\nStackTraceElement[] stackTrace &#x3D; e.getStackTrace();\nfor(StackTraceElement element : stackTrace)&#123;\n    System.out.println(element);\n&#125;\n使用throw抛出异常：当有函数抛出异常时，JVM会在底层执行栈展开（stack unwinding），依次将函数调用栈中的函数栈帧弹出，直到找到哪个函数可以捕获这个异常为止，然后JVM从这个函数继续再执行（不同于return导致的栈展开，异常导致的栈展开会有一个在函数的异常表中查找是否有可匹配的处理规则的过程，这样的查找在调用层次过深时和耗时）\n&#x2F;&#x2F;throw new RuntimeException(&quot;oops&quot;)这样一个异常抛出代码包括两个操作：创建异常和抛出异常等价于下面的两行代码\nRuntimeException e &#x3D; new RuntimeException(&quot;oops!&quot;);\nthrow e;\n打印异常调用链\n\n\n\n\n\n\n\n\n\n每个异常的stackTrace栈追踪消息都是一直到main函数的，不可以只记录生命周期内的函数，因为stackTrace栈追踪信息是在异常创建时生成的，在打印异常时，异常的声明周期未必就一定结束，所以无法只填充生命周期内所经历的函数\n\n原封不动抛出：相当于没捕获\n\n封装成新的异常抛出\ntry&#123;\n    &#x2F;&#x2F;...\n&#125;catch(IOException e)&#123;\n\t\t&#x2F;&#x2F;将捕获的异常通过cause参数传递给新的异常，调用链就不会断，主要调用了下面的Throwable的构造函数\n    throw new RuntimeException(&quot;oops&quot;,e);\n&#125;\n\npublic class Throwable&#123;\n    private String detailMessage;\n    private Throwable cause &#x3D; this;&#x2F;&#x2F;异常调用\n    private StackTraceElement[] stackTrace &#x3D; UNASSIGNED_STACK;\n    \n    public Throwable(String message, Throwable cause) &#123;\n        fillInStackTrace();&#x2F;&#x2F;生成stackTrace\n        detailMessage &#x3D; message;\n        this.cause &#x3D; cause;\n    &#125;\n    &#x2F;&#x2F;...\n&#125;\n记录日志：一般在开发中使用日志框架来记录异常，异常调用链信息会输出到日志文件中，方便开发者事后查看，一般不推荐使用e.pringStackTrace()来打印异常日志，因为会打印到标准出错输出System.err中，即命令行中，这不方便保存以便反复查看\ntry&#123;\n    &#x2F;&#x2F;...\n&#125;catch(IOException e)&#123;\n    log.error(&quot;...&quot;,e);\n    &#x2F;&#x2F;e.printStackTrace() 不推荐\n&#125;\n\n\n\n\n异常最佳实践：对于业务异常只需要将一些有用的信息，记录在异常的detailMessage成员变量中即可，通过向构造函数的参数writableStackTrace传入false，即可禁止在创建异常的同时调用fillStackTrace()函数\nprotected Throwable(String message, Throwable cause,\n                    boolean enableSuppression,\n                    boolean writableStackTrace) &#123;\n    if (writableStackTrace) &#123;\n        fillInStackTrace();\n    &#125; else &#123;\n        stackTrace &#x3D; null;\n    &#125;\n    detailMessage &#x3D; message;\n    this.cause &#x3D; cause;\n    if (!enableSuppression)\n        suppressedExceptions &#x3D; null;\n&#125;\n&#x2F;&#x2F;使用，可以解决高并发下程序中大量业务异常导致的程序变慢的问题\npublic class UserNotExistingException extends Throwable&#123;\n    public UserNotExistingException() &#123;\n        super(null,null,true,false);\n    &#125;\n\n    public UserNotExistingException(String message) &#123;\n        super(message,null,true,false);    &#125;\n\n    public UserNotExistingException(String message, Throwable cause) &#123;\n        super(message,cause,true,false);\n    &#125;\n\n    public UserNotExistingException(Throwable cause) &#123;\n        super(null,cause,true,false);\n    &#125;\n&#125;\n\n3.附录1.代码设计原则\nSOLID\n\nSRP单一职责原则：==A class or module should hava a single responsibility==\n\n不要设计大而全的类，要设计粒度小、功能单一的类。也就是说，如果一个类包含了两个或以上业务不相干的功能，那么他的职责就不够单一，应该被拆分成多个功能单一、粒度更细的类。\n要判断职责是否单一，不能脱离具体的应用场景，所以可以先写一个粗粒度的类，满足业务需求，随着业务的发展，如果粗粒度的类越来越庞大，代码越来越多，这个时候，就可以将这个独粒度的类拆分成几个更细粒度的类，这就是所谓的==持续重构==。\n==技巧==：\n类中的代码行数（200行内）、函数或属性过多（少于10个），会影响代码的可读性和可维护性，我们就需要对类进行拆分\n类依赖的其他类过多，或者依赖类的其他类过多，不符合高内聚、低耦合的设计思想，我们就需要对类进行拆分\n私有方法过多，我们就要考虑能否将私有方法独立到新的类中，设置为public方法，供更多的类使用，从而提高代码的复用性\n比较难给类起一个合适的名字，很难用一个业务名词概括或者只能用一些笼统的Manager、Context之类的词语来命名，这就说明类的职责定义的可能不够清晰\n类中大量的方法都是集中操作类中的某几个属性。\n\n\n\n\nOCP开闭原则：==Software entities(modules,classes,functions) should be open for extension,but closed for modification==\n\n添加一个新功能应该是，在已有的代码基础上扩展代码（新增模块、类、方法等），而非修改已有的代码（修改模块、类、方法等）\n\n指导思想：为了尽量写出扩展性好的代码，我们要时刻具备扩展意识、抽象意识、封装意识，这些潜意识可能比任何开发技巧都重要。\n\n==方法==：多态、依赖注入、基于接口而非实现编程、大部分设计模式（装饰、策略、模板、职责链、状态）\n\n\n\nLSP里式替换原则：==子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为不变及正确性不被破坏==\n\n虽然从定义描述和代码实现上来看，多态和里氏替换有点类似，但他们的关注角度是不一样的。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法，它是一种代码实现的思路。而里氏替换原则是一种设计原则，是用来直到继承关系中子类该如何设计的，子类的设计要保证在替换父类的时候，不改变原有程序的逻辑以及不破坏原有程序的正确性。\n==按照协议来设计==：子类在设计的时候，要遵守父类的行为约定（或协议），父类定义了函数的行为约定，子类可以改变函数内部实现逻辑，但不能改变函数原有的行为约定（函数声明是实现的功能、对输入、输出、异常的约定、注释中所罗列的任何特殊说明）。\n==技巧==：用父类的单元测试来验证子类的代码，如果某些单元测试运行失败，就有可能违背里氏替换原则\n\n\nISP接口隔离原则：==Clients should not be forced to depend upon interfaces that they do not use==\n\n一组API接口集合：在设计微服务或者类库接口的时候，如果部分接口只被部分调用者使用，那么我们就需要将这部分接口隔离出来，单独给对应的调用者使用，而不是强迫其他调用者也依赖这部分不会被用到的接口。\n单个API接口或函数：函数的设计要功能单一，不要将多个功能逻辑在一个函数中实现。接口隔离原则跟单一职责原则有点类似，但是单一职责原则针对的是模块、类、接口的设计；而接口隔离原则相对于单一职责原则，一方面他更侧重于接口的设计，另一方面它提供了一种判断接口是否职责单一的标准（如果调用者只使用部分接口或接口的部分功能，那接口的设计就不够指责单一）。\nOOP中的接口概念：拆分成小接口，而不是一个大而全的config接口\n\n\nDIP依赖倒置原则：==High-level modules shouldn’t depend on low-level modules. Both modules should depend on abstractions. In addition, abstractions shouldn’t depend on details. Details depend on abstractions.（高层模块和低层模块的划分就是，在调用链上，调用者属于高层，被调用者属于低层）==\n\n\n\nKISS原则：==尽量保持简单==\n\n不要使用同事可能不懂的技术来实现代码，例如正则表达式或编程语言中的高级语法\n不要重复造轮子，要善于使用已经有的工具类库\n不要过度优化，不要过度使用一些奇技淫巧（位运算、复杂条件语句、过于底层函数）来优化代码，牺牲代码的可读性\n\n\nYAGNI原则：==You ain’t gonna need it 你不会需要它==，不要去设计当前用不到的功能，不要去编写当前用不到的代码，即不要过度设计，只需要预留好扩展点。\n\nDRY原则：==Don’t repeat yourself==，不要写重复的代码。\n\n实现逻辑重复：尽管代码的实现逻辑是重复的，但是语义上不是重复的，可以判定它并不违反DRY原则\n功能语义重复：实现逻辑不重复，但语义重复，那么也就是功能重复，我们认为它违反了DRY原则\n代码执行重复：例如对输入校验了两次\n\n\nLOD原则（Law of Demeter）：==Each unit should have only limited knowledge about other units: only units “closely” related to the current unit. Or: Each unit should only talk to its friends; Don’t talk to strangers.==\n\n高内聚，松耦合：\n\n高内聚：用来指导类本身的设计，相近的功能应该放到同一个类中，不想近的功能不要放到同一个类中，相近的功能往往会被同时更改，放到一个类中，代码容易维护\n低耦合：用来指导类与类之间依赖关系的设计，在代码中，类与类之间的依赖关系应该简单清晰，一个类的代码改动不会或者很少导致依赖类的代码改动\n\n\n迪米特法则\n\n\n\n\n2.JDBC连接数据库\n加载数据库驱动程序：使用Class.forName()方法加载对应的数据库驱动程序，例如：Class.forName(“com.mysql.jdbc.Driver”);\n建立数据库连接：使用DriverManager.getConnection()方法建立与数据库的连接，需要指定数据库的URL、用户名和密码，例如：Connection conn = DriverManager.getConnection(“jdbc:mysql://localhost/mydatabase”, “username”, “password”);\n创建Statement对象：使用Connection对象的createStatement()方法创建一个Statement对象，用于执行SQL语句，例如：Statement stmt = conn.createStatement();\n执行SQL语句：使用Statement对象的executeQuery()或executeUpdate()方法执行SQL语句，例如：ResultSet rs = stmt.executeQuery(“SELECT * FROM mytable”);\n处理查询结果：如果执行的是查询语句，需要使用ResultSet对象来处理查询结果，例如：while (rs.next()) { String name = rs.getString(“name”); int age = rs.getInt(“age”); }\n关闭数据库连接：在程序结束时，需要使用Connection对象的close()方法关闭数据库连接，例如：conn.close();\n\n3.Socket编程示例4.Collections工具类\n排序\nvoid reverse(List list)&#x2F;&#x2F;反转\nvoid shuffle(List list)&#x2F;&#x2F;随机排序\nvoid sort(List list)&#x2F;&#x2F;按自然排序的升序排序\nvoid sort(List list, Comparator c)&#x2F;&#x2F;定制排序，由Comparator控制排序逻辑\nvoid swap(List list, int i , int j)&#x2F;&#x2F;交换两个索引位置的元素\nvoid rotate(List list, int distance)&#x2F;&#x2F;旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面\n查找、替换操作\nint binarySearch(List list, Object key)&#x2F;&#x2F;对List进行二分查找，返回索引，注意List必须是有序的\nint max(Collection coll)&#x2F;&#x2F;根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll)\nint max(Collection coll, Comparator c)&#x2F;&#x2F;根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c)\nvoid fill(List list, Object obj)&#x2F;&#x2F;用指定的元素代替指定list中的所有元素\nint frequency(Collection c, Object o)&#x2F;&#x2F;统计元素出现次数\nint indexOfSubList(List list, List target)&#x2F;&#x2F;统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target)\nboolean replaceAll(List list, Object oldVal, Object newVal)&#x2F;&#x2F;用新元素替换旧元素\n同步控制（不推荐，多线程下应该直接使用JUC下的并发集合）：Collections提供了多个synchronizedXXX方法，该方法可以指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题\nsynchronizedCollection(Collection&lt;T&gt;  c) &#x2F;&#x2F;返回指定 collection 支持的同步（线程安全的）collection。\nsynchronizedList(List&lt;T&gt; list)&#x2F;&#x2F;返回指定列表支持的同步（线程安全的）List。\nsynchronizedMap(Map&lt;K,V&gt; m) &#x2F;&#x2F;返回由指定映射支持的同步（线程安全的）Map。\nsynchronizedSet(Set&lt;T&gt; s) &#x2F;&#x2F;返回指定 set 支持的同步（线程安全的）set。\n\n5.其它\n序列化和反序列化\n\n序列化就是将数据结构或对象转换成二进制字节流的过程，反序列化就是将序列化生成的二进制字节流转换成数据结构或对象的过程\n实现Serializable接口，即可使用JDK自带的序列化\nserialVersionUID：属于版本控制的作用，反序列化时，会检查其是否和当前类的serialVersionUID一致，反序列化之后，static变量并没有被序列化，因为他是静态变量，位于方法区，反序列化时就像是默认赋予给了对象一样\n\n\n常见应用场景：网络传输、数据库存储、对象存储到内存中\n不想进行序列化的变量：使用transient关键字修饰，transient可以阻止实例中那些用此关键字修饰的变量序列化，当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复\ntransient只能修饰变量，不能修饰类和方法\ntransient修饰的变量，在反序列化后变量会被置成类型的默认值（如int会被置为0）\nstatic变量因为不属于任何对象，所以无论有没有transient修饰，均不会被序列化\n\n\n\n\nSPI：服务提供者的接口，如SLF4J是Java的一个日志接口，具体实现由Logback、Log4j、Log4j2 等等\n\ncomparable 和 Comparator 的区别\n\nComparable接口出自java.lang包，它有一个compareTo(Object obj)方法用来排序\n\n一般用在自己的类声明中，在声明的过程中实现Comparable接口\n&#x2F;&#x2F;通过this和参数来比较\n@Override\npublic int compareTo(Person o) &#123;\n    if (this.age &gt; o.getAge()) &#123;\n        return 1;\n    &#125;\n    if (this.age &lt; o.getAge()) &#123;\n        return -1;\n    &#125;\n    return 0;\n&#125;\n&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;使用&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;\npublic static void main(String[] args) &#123;\n\t\t&#x2F;&#x2F;要么传入的key实现comparable接口，要么构造时传入的comparator接口的匿名对象\n    TreeMap&lt;Person, String&gt; pdata &#x3D; new TreeMap&lt;Person, String&gt;();\n    pdata.put(new Person(&quot;张三&quot;, 30), &quot;zhangsan&quot;);\n    pdata.put(new Person(&quot;李四&quot;, 20), &quot;lisi&quot;);\n    pdata.put(new Person(&quot;王五&quot;, 10), &quot;wangwu&quot;);\n    pdata.put(new Person(&quot;小红&quot;, 5), &quot;xiaohong&quot;);\n    &#x2F;&#x2F; 得到key的值的同时得到key所对应的值\n    Set&lt;Person&gt; keys &#x3D; pdata.keySet();\n    for (Person key : keys) &#123;\n        System.out.println(key.getAge() + &quot;-&quot; + key.getName());\n\n    &#125;\n&#125;\n\n\nComparator接口实际上是出自java.util包，它有一个compare(Object obj1, Object obj2)方法用来排序\n\n通过实现Comparator接口的对象，作为参数传递新的比较方法到工具类（sort方法）中\n&#x2F;&#x2F;通过传入的两个参数，确定比较规则，返回比较结果\nCollections.sort(arrayList, new Comparator&lt;Integer&gt;() &#123;\n    @Override\n    public int compare(Integer o1, Integer o2) &#123;\n        return o2.compareTo(o1);\n    &#125;\n&#125;);\n\n\n\n\nJava集合使用注意事项（阿里巴巴Java开发手册）\n\n判断所有集合内部的元素是否为空，使用 isEmpty() 方法，而不是 size()==0 的方式：isEmpty()方法的可读性更好，并且时间复杂度为 O(1)\n\n在使用 java.util.stream.Collectors 类的 toMap() 方法转为 Map 集合时，一定要注意当 value 为 null 时会抛 NPE 异常\n\ntoMap方法调用了Map接口的merge方法，merge方法就先调用Objects.requireNonNull方法来判断value是否为空\n\n\n不要在 foreach 循环里进行元素的 remove/add 操作（抛出ConcurrentModificationException异常，即fail-fast机制）。remove 元素请使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁\n\n可以使用java.util.concurrent包下面的类\n\n可以使用Collection的removeIf方法\nList&lt;Integer&gt; list &#x3D; new ArrayList&lt;&gt;();\nfor (int i &#x3D; 1; i &lt;&#x3D; 10; ++i) &#123;\n    list.add(i);\n&#125;\nlist.removeIf(filter -&gt; filter % 2 &#x3D;&#x3D; 0); &#x2F;* 删除list中的所有偶数 *&#x2F;\nSystem.out.println(list); &#x2F;* [1, 3, 5, 7, 9] *&#x2F;\n\n\n集合去重：可以利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的 contains() 进行遍历去重或者判断包含操作\n\nHashSet的contains方法依赖底部的HashMap，时间复杂度时时O（1）的\nArrayList的contains方法是通过遍历所有元素来实现的，时间复杂度是O（n）\n\n&#x2F;&#x2F; Set 去重代码示例\npublic static &lt;T&gt; Set&lt;T&gt; removeDuplicateBySet(List&lt;T&gt; data) &#123;\n\n    if (CollectionUtils.isEmpty(data)) &#123;\n        return new HashSet&lt;&gt;();\n    &#125;\n    return new HashSet&lt;&gt;(data);\n&#125;\n\n&#x2F;&#x2F; List 去重代码示例\npublic static &lt;T&gt; List&lt;T&gt; removeDuplicateByList(List&lt;T&gt; data) &#123;\n\n    if (CollectionUtils.isEmpty(data)) &#123;\n        return new ArrayList&lt;&gt;();\n\n    &#125;\n    List&lt;T&gt; result &#x3D; new ArrayList&lt;&gt;(data.size());\n    for (T current : data) &#123;\n        if (!result.contains(current)) &#123;\n            result.add(current);\n        &#125;\n    &#125;\n    return result;\n&#125;\n使用集合转数组的方法，必须使用集合的 toArray(T[] array)，传入的是类型完全一致、长度为 0 的空数组\nString [] s&#x3D; new String[]&#123;\n    &quot;dog&quot;, &quot;lazy&quot;, &quot;a&quot;, &quot;over&quot;, &quot;jumps&quot;, &quot;fox&quot;, &quot;brown&quot;, &quot;quick&quot;, &quot;A&quot;\n&#125;;\nList&lt;String&gt; list &#x3D; Arrays.asList(s);\nCollections.reverse(list);\n&#x2F;&#x2F;没有指定类型的话会报错，new String[0]起到一个模版的作用，指定了返回参数的类型，0是为了节省空间，因为只是为了说明返回的类型\ns&#x3D;list.toArray(new String[0]);\n使用工具类Arrays.asList把数组转换成集合时，返回的不是平常使用的ArrayList，而是Arrays的一个内部类，\n\n不能使用其修改集合相关的方法，它的add、remove、clear方法会抛出**UnsupportedOperationException**异常，该Arrays内部类里面并没有上述方法\nasList收到的是传入集合的地址值，而不是数据，所以get方法在参数为0的时候返回地址值，在参数为1的时候返回数组越界异常\n所以最好手动实现工具类，通过for循环来一个一个add进list里\n\n&#x2F;&#x2F;方法一\nList list &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;))\n&#x2F;&#x2F;方法二\nfor循环一个一个加进去\n&#x2F;&#x2F;方法三 Stream\n\t\tInteger [] myArray &#x3D; &#123; 1, 2, 3 &#125;;\n\t\tList myList &#x3D; Arrays.stream(myArray).collect(Collectors.toList());\n\t\t&#x2F;&#x2F;基本类型也可以实现转换（依赖boxed的装箱操作）\n\t\tint [] myArray2 &#x3D; &#123; 1, 2, 3 &#125;;\n\t\tList myList &#x3D; Arrays.stream(myArray2).boxed().collect(Collectors.toList());\n&#x2F;&#x2F;方法四：使用Guava\n&#x2F;&#x2F;不可变集合\n\tList&lt;String&gt; il &#x3D; ImmutableList.of(&quot;string&quot;, &quot;elements&quot;);  &#x2F;&#x2F; from varargs\n\tList&lt;String&gt; il &#x3D; ImmutableList.copyOf(aStringArray);      &#x2F;&#x2F; from array\n&#x2F;&#x2F;可变集合\n\tList&lt;String&gt; l1 &#x3D; Lists.newArrayList(anotherListOrCollection);    &#x2F;&#x2F; from collection\n\tList&lt;String&gt; l2 &#x3D; Lists.newArrayList(aStringArray);               &#x2F;&#x2F; from array\n\tList&lt;String&gt; l3 &#x3D; Lists.newArrayList(&quot;or&quot;, &quot;string&quot;, &quot;elements&quot;); &#x2F;&#x2F; from varargs\n&#x2F;&#x2F;方法五：使用Java9的List.of方法\n\tInteger[] array &#x3D; &#123;1, 2, 3&#125;;\n\tList&lt;Integer&gt; list &#x3D; List.of(array);\n\t\n\n\nCloneable接口实现原理\n\n\n","slug":"Java Base","date":"2023-04-13T11:25:47.000Z","categories_index":"","tags_index":"language","author_index":"Dajunnnnnn"},{"id":"d8c28dc067d04110f6447cc712799b2d","title":"DataStructure","content":"数据结构1.数据结构1.1常用方法\n\n\n接口\nAPI\n\n\n\nCollection\nsize、isEmpty、contains、toArray、add、remove、clear\n\n\nList\nget(index)、set(index)、add(index,element)、remove(index)、indexOf()、lastIndexOf()、subList(from, to)、sort\n\n\nQueue\noffer(element)、poll()、peek()\n\n\nDuque\nofferFirst(E e)、offerLast(E e)、pollFirst()、pollLast()、peekFirst()、peekLast()、push(E e)、pop()\n\n\nSet\nsize、isEmpty、contains、toArray、add、remove\n\n\nSortedSet\nSortedSet subSet(E fromElement, E toElement)、headSet(E toElement)、tailSet(E fromElement)、first、last\n\n\nMap\nsize、isEmpty、containsKey、containsValue、get、put、remove、keySet、values、entrySet\n\n\nMap补\ngetOrDefault(Object key, V defaultValue)、putIfAbsent(K key, V value)、replace(K key, V oldValue, V newValue)\n\n\n1.2工具类\n\n\n类名\nAPI\n\n\n\nString\ncharAt、toCharArray、split、substring（新String）、indexOf、lastIndexOf、replace、length\n\n\nString补\ntrim、toLowerCase、toUpperCase、split(String regex)、format（格式化输出，同c）\n\n\nStringBuilder\nappend、toString、charAt、length、delete、replace、insert、reverse、indexOf、lastIndexOf\n\n\nCollections\nsort（list）、binarySearch、reverse、swap、fill、copy、replaceAll、emptyXXX\n\n\nArrays\nsort、binarySearch、equals、fill、asList、copyOf、copyOfRange\n\n\nMath\nmin、max、abs、sqrt(double)、pow(double, double)、ceil（上整）、floor（下整）、round（四舍五入）\n\n\nMath补\nInteger.MAX_VALUE、Integer.MIN_VALUE、\n\n\nScanner\nnext（下一String）、nextInt、nextLong、nextLine（nextInt不会洗掉换行符，需要nextLine吸掉）\n\n\nSystem.out\nprintln、print、format(“x = %d, y = %f\\n”, x, y)\n\n\n\n简化代码：输入一串数字组成的字符\n&#x2F;&#x2F; 1 2 3 4 5...\nint[] nums&#x3D;Arrays.stream(scanner.nextLine().split(&quot; &quot;)).mapToInt(Integer::parseInt).toArray();\n\n1.3补充知识\nArrayList\n\n实现特殊接口\n\nRandomAccess：Arrays的静态方法binarySearch会根据接口调用不同的实现方法\nCloneable：使用clone方法，返回一个浅拷贝\n\n\n底层为可动态扩容的数组（支持存储null数据）\n\n首先==确定最小扩容量==，默认最小为10，如果传入的所需容量比10大，则按传入的所需容量来扩容\n\n然后==判断是否需要扩容==，如果前一阶段判定的需要容量比内部数组的长度大，则进行扩容\n\n使用位移操作，将容量扩展为内部数组长度的1.5倍，如果比需要容量小，则直接使用需要容量，防止多次扩容，然后使用System.arraycopy来复制数据\npublic static native void arraycopy(Object src,  int  srcPos,Object dest, int destPos,int length);\n\n\n使用modCount：来记录容量更改的次数，每次调用ensureCapacityInternal就将modCount加1，容量不够使才改容量。用来确定迭代的过程中，是否有其他线程更改过数据，如果有人修改过，则抛出ConcurrentModificationException异常\n\n一种转换方式\nList&lt;int[]&gt; merged &#x3D; new ArrayList&lt;int[]&gt;();\nint[][] result &#x3D; merged.toArray(new int[merged.size()][]);\n\n\nLinkedList\n\n可以根据引用的接口不同，使用不同方法，支持List、Queue、Deque，根据结构的不同可以调用不同的方法\n底层为双向链表，并且有头尾指针，支持存储null数据\n\n\nArrayDeque\n\n基于数组实现，性能比LinkedList好，也可用来实现栈\n\n\nPriorityQueue\n\n底层依赖堆来实现（使用可变长数组），默认情况下为小顶堆，最先出队列的为当前队列中的最小值，支持Comparator接口\nQueue&lt;Integer&gt; minH &#x3D; new PriorityQueue&lt;&gt;(); &#x2F;&#x2F;小顶堆，默认大小为11\nQueue&lt;Integer&gt; maxH &#x3D; new PriorityQueue&lt;&gt;((i1, i2) -&gt; i2 - i1); &#x2F;&#x2F;大顶堆，默认大小为11\n&#x2F;&#x2F; 支持数组\nPriorityQueue&lt;int[]&gt; pq &#x3D; new PriorityQueue&lt;int[]&gt;(new Comparator&lt;int[]&gt;() &#123;\n    public int compare(int[] pair1, int[] pair2) &#123;\n        return pair1[0] !&#x3D; pair2[0] ? pair2[0] - pair1[0] : pair2[1] - pair1[1];\n    &#125;\n&#125;);\n不支持存储NULL和non-comparable对象，通过堆元素的上浮和下沉，实现了在O(logn)的时间复杂度内插入和删除堆顶元素\n\n堆的构建过程，需要比较节点中数据的大小，所以，添加到优先级队列中的元素，需要能够比较大小，方法有两种：基于Comparable接口和基于Comparator接口，都有时则优先使用comparator，详见siftUp\nprivate void siftUp(int k, E x) &#123;\n    if (comparator !&#x3D; null)\n        siftUpUsingComparator(k, x);\n    else\n        siftUpComparable(k, x);\n&#125;\n\n\nSet（HashSet、LinkedHashSet、TreeSet）\n\n底层实现分别为：HashMap、LinkedHashMap、TreeMap，存储对象的时候，使用对象作为key，一个空的Object对象作为value，插入到底层的Map中，不管\n如何检查重复：无论Set中是否已经存在了某元素，都会直接在底层进行插入，通过add方法的返回值来确定插入前是否有相同的元素\n应用场景：HashSet用于==不需要保证元素插入和取出顺序==的场景；LinkedHashSet用于==保证元素的插入和取出顺序满足FIFO==的场景（LinedHashMap底层使用双向有序链表+哈希表）；TreeSet用于支持对元素==自定义排序规则==的场景\n\n\nHashMap（==数组+链表/红黑树==）\n\n底层为哈希表，对key求哈希作为hash值，包裹hash值、key和value为Node对象，作为哈希表（数组+链表）的组成节点。key不能重复，存储重复的key，新value会覆盖旧value（可以存一个key为null的键值对，但是不同key的value都可以是null）\n底层数组长度为2的倍数：hash函数可以使用与n-1取交替代与n取余、装载因子使用0.75使得阈值（n*0.75）一直为整数、初始化的时候选择比传入参数大的最小2的幂次方数\n动态扩容：默认初始化大小为16，每次超过阈值的时候就扩容为原来的2倍；扫描数组的每一条链表，根据节点下标决定是否要更改，插入到lo链表（不需改）和hi链表（需要改），处理完一条链表，将新链表插入到对应位置\n新位置确定方式：如果node.hash&amp;oldCap == 0，则节点在新table数组中的下标不变；如果node.hash &amp; oldCap != 0，则节点在新table数组中的下标变为i+oldCap（i为在原数组的下标）\n链表树化：当某个链表中的节点个数大于等于8（TREEIFY_THRESHOLD静态常量），并且table数组的长度大于等于64时，将会把链表转化为红黑树；如果table长度不满足则触发扩容操作；如果红黑树节点数在[2，6]之间，则退化为链表\n\n\n\n\nArrays的sort\n\nCollections的sort函数底层依赖的Arrays类的sort函数，如List接口中的sort的默认实现\n&#x2F;&#x2F;支持数组的排序\nArrays.sort(intervals, new Comparator&lt;int[]&gt;() &#123;\n    public int compare(int[] interval1, int[] interval2) &#123;\n        return interval1[0] - interval2[0];\n    &#125;\n&#125;);\n基本类型：使用==DualPivotQuickSort==，jdk7之前使用快排\n\n对快排进行改进，选取两个pivot，通过数组的长度决定什么时候选用双轴快排、插入排序、归并排序、记数排序\n\n\n对象数组：使用==TimSort==，jdk7之前使用归并\n\n使用非递归版本归并排序算法，在归并排序的过程中，大的排序区间不断分解为小的待排序区间，如果带排序区间的长度小于MIN_MERGE（32），就不再继续分解，转而执行二分插入排序算法\n二分插入排序：将数组分为已排序区间和未排序区间，通过二分查找，查找插入位置，当找到后，通过调用System.arraycopy()函数，将插入点之后的数据整体快速后移一位，腾出位置给要插入的数据\n\n\n\n\nString（final数组）\n\nString不可变的原因：内部是final修饰的数组（引用不可改但是数据可改）、没有提供更改数组的方法、String类也是final的子类无法继承，避免了子类破坏String的不变性\n常量池技术：使用字符串常量赋值时触发，直接复用常量池已存在的对象，也可以使用intern方法复制堆上对象到常量池并回收堆上的对象（判等的时候使用equals()）\n运算符重载：因为String比较常用，所以延续了基本类型和包装类的设计，实现了加法操作String sc = sa + sb;，底层使用了StringBuilder来实现（StringBuffer加了锁，是线程安全的）\n\n\n\n2.算法2.1复杂度分析\n分析方法\n加法原则：总复杂度等于量级最大的那段代码的复杂度\n乘法原则：嵌套代码的复杂度等于嵌套内外的代码复杂度乘积\n其他方法：某一条语句执行的总次数；数据被访问的次数；使用递归树来分析\n\n\n空间复杂度\n不关注存储数据所需要的空间，而是关注算法所需要的额外存储消耗（循环、递归调用栈、辅助存储）\n由于现有题型大多以耗时为指标，所以尽可能使用==以空间换时间==的思想\n\n\n时间复杂度\n不看低阶和常数系数、加法取大、乘法取积\n分类：最好、最坏、平均\n\n\n\n2.2技巧\n双指针\n\n前缀和数组：原始数组不会被修改的情况下，频繁查询某个区间的累加和\n\n\n\n前缀和数组中两个元素的差，及这段区间的累加和\n示例：原数组{3,5,2,-1,4,1}；前缀和数组{0,3,8,10,8,12,13}\n\n\n差分数组：频繁对原数组的某个区间的元素进行增减\n\n\n原理：对i→n的所有元素都加3，对j+1→n的所有元素都减3\n示例：原数组{8,2,6,3,1}；差分数组{8,-6,4,-3,-2}\n\n\n单调栈：满足单调性的栈结构\n\n\n插入过程：将一个元素插入单调栈时，为了维护栈的单调性，需要先弹出一些元素直到新插入的元素可以不破坏单调性\n\n伪代码\ninsert x\nwhile !sta.empty() &amp;&amp; sta.top()&lt;x\n    sta.pop()\nsta.push(x)\n\n\n并查集（Union-Find）\nclass UF &#123;\n    &#x2F;&#x2F; 连通分量个数\n    private int count;\n    &#x2F;&#x2F; 存储每个节点的父节点\n    private int[] parent;\n\n    &#x2F;&#x2F; n 为图中节点的个数\n    public UF(int n) &#123;\n        this.count &#x3D; n;\n        parent &#x3D; new int[n];\n        for (int i &#x3D; 0; i &lt; n; i++) &#123;\n            parent[i] &#x3D; i;\n        &#125;\n    &#125;\n    \n    &#x2F;&#x2F; 将节点 p 和节点 q 连通\n    public void union(int p, int q) &#123;\n        int rootP &#x3D; find(p);\n        int rootQ &#x3D; find(q);\n        \n        if (rootP &#x3D;&#x3D; rootQ)\n            return;\n        \n        parent[rootQ] &#x3D; rootP;\n        &#x2F;&#x2F; 两个连通分量合并成一个连通分量\n        count--;\n    &#125;\n\n    &#x2F;&#x2F; 判断节点 p 和节点 q 是否连通\n    public boolean connected(int p, int q) &#123;\n        int rootP &#x3D; find(p);\n        int rootQ &#x3D; find(q);\n        return rootP &#x3D;&#x3D; rootQ;\n    &#125;\n\n    public int find(int x) &#123;\n        if (parent[x] !&#x3D; x) &#123;\n            parent[x] &#x3D; find(parent[x]);\n        &#125;\n        return parent[x];\n    &#125;\n\n    &#x2F;&#x2F; 返回图中的连通分量个数\n    public int count() &#123;\n        return count;\n    &#125;\n&#125;\n快速幂：为了在O(logn)的时间内计算a^n的技巧\n\n理论依据：a^(b+c) = a^b * a^c，与二分查找思想结合可以得出a^(2b) =a^b * a^b =  (a^b) ^2\n\n代码实现\n\n递归\nlong binpow(long a,long b)&#123;\n  if(b &#x3D;&#x3D; 0)&#123;\n    return 1;\n  &#125;\n  long res &#x3D; binpow(a, b&#x2F;2);\n  if(b % 2 &#x3D;&#x3D; 1)&#123;\n    return res * res * a; &#x2F;&#x2F;奇数次幂\n  &#125;else&#123;\n    return res * res; &#x2F;&#x2F;偶数次幂\n  &#125;\n&#125;\n非递归\nlong binpow(long a, long b)&#123;\n  long res &#x3D; 1;\n  while(b &gt; 0)&#123;\n    if((b &amp; 1) &#x3D;&#x3D; 1)&#123; &#x2F;&#x2F;当前位为1，则需要乘二进制幂，否则跳过此次\n      res &#x3D; res * a;\n    &#125;\n    a &#x3D; a*a;\n    b &gt;&gt;&#x3D; 1;\n  &#125;\n  return res;\n&#125;\n\n\n应用\n\n计算 (x^n) mod m：取模运算不会干涉乘法，所以计算过程中直接取模就行\n\n另：根据费马小定理，如果m是一个质数，可以计算x^(n mod (m-1) )来加速算法过程\n\nlong binpow(long a, long b, long m)&#123;\n  a %&#x3D; m;\n  long res &#x3D; 1;\n  while(b &gt; 0)&#123;\n    if((b &amp; 1) &#x3D;&#x3D; 1)&#123;\n      res &#x3D; res * a % m;\n    &#125;\n    a &#x3D; a * a % m;\n    b &gt;&gt;&#x3D; 1;\n  &#125;\n  return res;\n&#125;\n\n\n\n\n线段树\n\n目的：用来维护区间信息的数据结构，可以在O(logN)的时间复杂度内实现单点修改、区间修改、区间查询（区间求和、求区间最大值、求区间最小值）等操作\n\n基本结构\n&#x2F;&#x2F; 对区间[s,t]递归建树\n&#x2F;&#x2F; int[] d &#x3D; new int[n*4];\nvoid build(int s, int t, int p)&#123;\n  if(s &#x3D;&#x3D; t)&#123;\n    d[p] &#x3D; a[s];\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  build(s,m,p*2);\n  build(m+1,t,p*2+1);\n  &#x2F;&#x2F;从下向上递归建树\n  d[p] &#x3D; d[p*2] + d[p*2+1];\n&#125;\n区间查询\nint getSum(int l, int r, int s, int t, int p)&#123;\n  &#x2F;&#x2F;[l,r]为查询区间，[s,t]为当前节点包含的区间，p为当前节点的编号\n  if(l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r)&#123;\n    return d[p]; &#x2F;&#x2F;当前区间为查询区间的子集时直接返回当前节点的和\n  &#125;\n  int m &#x3D; s + ((t-s) &gt;&gt; 1);\n  int sum &#x3D; 0;\n  &#x2F;&#x2F;左儿子与查询区间有交集，递归查询左儿子\n  if(l &lt;&#x3D; m)&#123;\n    sum +&#x3D; getSum(l, r, s, m, p*2);\n  &#125;\n  &#x2F;&#x2F;右儿子与查询区间有交集，递归查询右儿子\n  if(r &gt; m)&#123;\n    sum +&#x3D; getSum(l, r, m+1, t, p*2+1);\n  &#125;\n  return sum;\n&#125;\n区间修改（存在标记的情况）\nvoid update(int l, int r, int c, int s, int t, int p)&#123;\n  &#x2F;&#x2F; [l, r] 为修改区间, c 为被修改的元素的变化量, [s, t] 为当前节点包含的区间, p为当前节点的编号\n  if(l &lt;&#x3D; s &amp;&amp;  t &lt;&#x3D; r)&#123;\n    d[p] +&#x3D; (t - s + 1) * c;\n    b[p] +&#x3D; c;\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (b[p] &amp;&amp; s !&#x3D; t) &#123;\n    &#x2F;&#x2F; 如果当前节点的懒标记非空,则更新当前节点两个子节点的值和懒标记值\n    d[p * 2] +&#x3D; b[p] * (m - s + 1);\n    d[p * 2 + 1] +&#x3D; b[p] * (t - m);\n    &#x2F;&#x2F; 将标记下传给子节点\n    b[p * 2] +&#x3D; b[p];\n    b[p * 2 + 1] +&#x3D; b[p];  \n    &#x2F;&#x2F; 清空当前节点的标记\n    b[p] &#x3D; 0;                                \n  &#125;\n  if (l &lt;&#x3D; m) &#123;\n    update(l, r, c, s, m, p * 2);\n  &#125;\n  if (r &gt; m) &#123;\n    update(l, r, c, m + 1, t, p * 2 + 1);\n  &#125;\n  d[p] &#x3D; d[p * 2] + d[p * 2 + 1];\n&#125;\n区间求和（存在标记的情况）\nint getsum(int l, int r, int s, int t, int p) &#123;\n  &#x2F;&#x2F; [l, r] 为查询区间, [s, t] 为当前节点包含的区间, p 为当前节点的编号\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) return d[p];\n  &#x2F;&#x2F; 当前区间为询问区间的子集时直接返回当前区间的和\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (b[p]) &#123;\n    &#x2F;&#x2F; 如果当前节点的懒标记非空,则更新当前节点两个子节点的值和懒标记值\n    d[p * 2] +&#x3D; b[p] * (m - s + 1);\n    d[p * 2 + 1] +&#x3D; b[p] * (t - m);\n    &#x2F;&#x2F; 将标记下传给子节点\n    b[p * 2] +&#x3D; b[p];\n    b[p * 2 + 1] +&#x3D; b[p];  \n    &#x2F;&#x2F; 清空当前节点的标记\n    b[p] &#x3D; 0;                                \n  &#125;\n  int sum &#x3D; 0;\n  if (l &lt;&#x3D; m) sum &#x3D; getsum(l, r, s, m, p * 2);\n  if (r &gt; m) sum +&#x3D; getsum(l, r, m + 1, t, p * 2 + 1);\n  return sum;\n&#125;\n区间修改为某一个值而不是加上某一个值\nvoid update(int l, int r, int c, int s, int t, int p) &#123;\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) &#123;\n    d[p] &#x3D; (t - s + 1) * c, b[p] &#x3D; c;\n    return;\n  &#125;\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  &#x2F;&#x2F; 额外数组储存是否修改值\n  if (v[p]) &#123;\n    d[p * 2] &#x3D; b[p] * (m - s + 1), d[p * 2 + 1] &#x3D; b[p] * (t - m);\n    b[p * 2] &#x3D; b[p * 2 + 1] &#x3D; b[p];\n    v[p * 2] &#x3D; v[p * 2 + 1] &#x3D; 1;\n    v[p] &#x3D; 0;\n  &#125;\n  if (l &lt;&#x3D; m) update(l, r, c, s, m, p * 2);\n  if (r &gt; m) update(l, r, c, m + 1, t, p * 2 + 1);\n  d[p] &#x3D; d[p * 2] + d[p * 2 + 1];\n&#125;\n\nint getsum(int l, int r, int s, int t, int p) &#123;\n  if (l &lt;&#x3D; s &amp;&amp; t &lt;&#x3D; r) return d[p];\n  int m &#x3D; s + ((t - s) &gt;&gt; 1);\n  if (v[p]) &#123;\n    d[p * 2] &#x3D; b[p] * (m - s + 1), d[p * 2 + 1] &#x3D; b[p] * (t - m);\n    b[p * 2] &#x3D; b[p * 2 + 1] &#x3D; b[p];\n    v[p * 2] &#x3D; v[p * 2 + 1] &#x3D; 1;\n    v[p] &#x3D; 0;\n  &#125;\n  int sum &#x3D; 0;\n  if (l &lt;&#x3D; m) sum &#x3D; getsum(l, r, s, m, p * 2);\n  if (r &gt; m) sum +&#x3D; getsum(l, r, m + 1, t, p * 2 + 1);\n  return sum;\n&#125;\n\n\n\n2.3算法思想\n排序\n\n基础排序算法\n\nO（n^2）\n\n冒泡排序：一对对比较，一对对交换\n\n插入排序：分为已排和未排区间，取未排插入到已排。例：希尔排序\n\n选择排序：分为已排和未排区间，从未排选一个最小的插入到已排的\n\n希尔排序\n\n\n\nO（nlogn）\n\n归并排序：“分治思想”，分而治之，然后再合并\n\n快速排序：选一个pivot，大的放左，小的放右\n\n堆排序：先将数组原地建成一个堆，从下往上堆化，取堆顶元素，将下标n的元素放到堆顶，堆化\n\n二叉排序树排序\n\n\n\nO（n）\n\n计数排序：例：10G数据，100个桶\n\n基数排序：高考成绩排序，760个桶\n\n桶排序：10万个手机号码排序，从个位开始一位位进行桶或基数排序\n\n\n\n\n\n常见题型\n\n特殊排序：不是单纯的增减顺序，而是有一些特殊要求\nTop K：找到前K个大的，第K个大的……\n链表上的排序：数据结构由数组转换为链表，并进行排序\n排序预处理：排序只是问题的一部分预处理，可以运用库函数\n区间问题：（252题、56题） 先排序，再处理\n\n\n\n\n二分查找：大部分都是变形二分查找或二分答案，代码不长，但容易写对。难点在于：确定搜索区间，循环条件，区间更新，返回值\n\n查找区间永远是闭区间[low,high]\n\n循环条件永远是：low &lt;= high\n\n对于low == high的情况，必要的时候特殊处理，在while内部补充退出条件\n\n返回值永远是mid，而不是low，high\n\nlow、high的更新永远是low = mid + 1和high = mid - 1\n\n对于非确定性查找，使用前后探测法，来确定搜索区间（不用while，而只更新low或high）\n\n先处理命中情况，再处理在左右半部分查找的情况\n\n非确定查找：第一个、最后一个、第一个大于等于、最后一个小于等于、循环数组寻找最小值、寻找峰值\n\n\n\nbfs\n&#x2F;&#x2F; 计算从起点 start 到终点 target 的最近距离\nint BFS(Node start, Node target) &#123;\n    Queue&lt;Node&gt; q; &#x2F;&#x2F; 核心数据结构\n    Set&lt;Node&gt; visited; &#x2F;&#x2F; 避免走回头路\n    \n    q.offer(start); &#x2F;&#x2F; 将起点加入队列\n    visited.add(start);\n    int step &#x3D; 0; &#x2F;&#x2F; 记录扩散的步数\n\n    while (q not empty) &#123;\n        int sz &#x3D; q.size();\n        &#x2F;* 将当前队列中的所有节点向四周扩散 *&#x2F;\n        for (int i &#x3D; 0; i &lt; sz; i++) &#123;\n            Node cur &#x3D; q.poll();\n            &#x2F;* 划重点：这里判断是否到达终点 *&#x2F;\n            if (cur is target)\n                return step;\n            &#x2F;* 将 cur 的相邻节点加入队列 *&#x2F;\n            for (Node x : cur.adj()) &#123;\n                if (x not in visited) &#123;\n                    q.offer(x);\n                    visited.add(x);\n                &#125;\n            &#125;\n        &#125;\n        &#x2F;* 划重点：更新步数在这里 *&#x2F;\n        step++;\n    &#125;\n&#125;\ndfs\n\n递归\n\n代码技巧：千万不要试图想清楚整个递和归的执行过程，实际上是进入了一个思维误区\n\n怎么发现这个问题可以用递归来做：\n\n规模更小的问题，跟规模大点的问题，解决思路相同，但规模不同\n\n利用子问题的解可以组合得到原问题的解\n\n存在最小子问题，可以直接返回结果，即存在递归终止条件\n\n\n\n递归的正确编写姿势：\n\n我们可以假设子问题B,C已经解决，在此基础上思考如何解决原问题A，基于此，找递推公式+终止条件，然后翻译成代码\n\n\n\n\n时间复杂度和空间复杂度分析：\n\n时间复杂度：递推公式或者递归树\n空间复杂度：跟递归的函数调用栈最大深度成正比，即递归树的高度\n\n\n解题技巧：寻找重复结构，是否能将问题结构转化成结构相同，规模更小的子问题，然后写递推公式，包括递归终止条件，然后翻译成代码\n\n原问题解决思路和子问题解决思路是否一样\n\n子问题的解能否构造出原问题的解（递推公式）\n\n找到最小子问题（终止条件）\n\n\n\n\n\n回溯：回溯是递归的副产品，只要有递归就会有回溯，本质就是穷举+剪枝\nresult &#x3D; []\ndef backtrack(路径, 选择列表):\n    if 满足结束条件:\n        result.add(路径)\n        return\n    \n    for 选择 in 选择列表:\n        做选择\n        backtrack(路径, 选择列表)\n        撤销选择\ndfs\npublic List&lt;Integer&gt; dfs(int s,int t)&#123;\n        List&lt;Integer&gt; path &#x3D; new ArrayList&lt;&gt;();\n        path.add(s);\n        visited[s] &#x3D; true;\n        dfs_backtrack(s,t,path);\n        return resultPath;\n    &#125;\n\n    public void dfs_backtrack(int s,int t,List&lt;Integer&gt; path)&#123;\n        &#x2F;&#x2F;结束条件\n        if (s &#x3D;&#x3D; t)&#123;\n            resultPath &#x3D; new ArrayList&lt;&gt;(path);\n            return;\n        &#125;\n        for (int i &#x3D; 0; i &lt; adj[s].size(); i++) &#123;\n            int q &#x3D; adj[s].get(i);\n            if (!visited[q])&#123;\n                path.add(q);\n                visited[q] &#x3D; true;\n                dfs_backtrack(q,t,path);\n                path.remove(path.size()-1);\n            &#125;\n        &#125;\n    &#125;\n\n\ndp\n\n解题步骤\n\n可用回溯解决：使用穷举结果才能得到结果的问题（最值、可行、计数等）\n构建多阶段决策模型：看是否能将问题求解的过程分为多个阶段\n查看是否存在重复子问题：是否有多个路径到达同一状态\n定义状态：也就是如何记录每一阶段的不重复状态\n定义状态转移方程：也就是找到如何通过上一阶段的状态推导下一阶段的状态\n画状态转移表：辅助理解，验证正确性，确定状态转移的初始值\n\n\n代码结构\n# 自顶向下递归的动态规划\ndef dp(状态1, 状态2, ...):\n    for 选择 in 所有可能的选择:\n        # 此时的状态已经因为做了选择而改变\n        result &#x3D; 求最值(result, dp(状态1, 状态2, ...))\n    return result\n\n# 自底向上迭代的动态规划\n# 初始化 base case\ndp[0][0][...] &#x3D; base case\n# 进行状态转移\nfor 状态1 in 状态1的所有取值：\n    for 状态2 in 状态2的所有取值：\n        for ...\n            dp[状态1][状态2][...] &#x3D; 求最值(选择1，选择2...)\n0-1背包的最值、可行、计数\n\n最值1：有n个物品，选择其中一些物品装入背包，在不超过背包最大重量限制的前提下，背包中可装物品总重量的最大值是多少\n\n最值2：有n个物品，选择其中一些物品装入背包，正好装满背包所需物品最小个数（如果装不满，返回-1）\n\n可行：有n个物品，选择其中一些物品装入背包，能不能正好装满背包\n\n计数：有n个物品，选择其中一些物品装入背包，装满背包有多少种不同的装法\n\n\n\n完全背包（同一个物品可装n次）的最值、可行、计数\n\n背包可装物品总重量的最大值是多少\n是否能装满整个背包\n正好装满背包至少需要多少物品\n装满背包有多少种装法\n\n\n空间优化\n\n\n\n\n3.经典代码1.二叉树\n构建\n\n根据数组构建节点结构\npublic class Solution &#123;\n    static class TreeNode &#123;\n        int val;\n        TreeNode left;\n        TreeNode right;\n        public TreeNode(int x) &#123;\n            this.val &#x3D; x;\n            this.left &#x3D; null;\n            this.right &#x3D; null;\n        &#125;\n    &#125;\n    \n    &#x2F;**\n     * 根据数组构建二叉树\n     * @param arr 树的数组表示\n     * @return 构建成功后树的根节点\n     *&#x2F;\n    public TreeNode constructBinaryTree(final int[] arr) &#123;\n        &#x2F;&#x2F; 构建和原数组相同的树节点列表\n        List&lt;TreeNode&gt; treeNodeList &#x3D; arr.length &gt; 0 ? new ArrayList&lt;&gt;(arr.length) : null;\n        TreeNode root &#x3D; null;\n        &#x2F;&#x2F; 把输入数值数组，先转化为二叉树节点列表\n        for (int i &#x3D; 0; i &lt; arr.length; i++) &#123;\n            TreeNode node &#x3D; null;\n            if (arr[i] !&#x3D; -1) &#123; &#x2F;&#x2F; 用 -1 表示null\n                node &#x3D; new TreeNode(arr[i]);\n            &#125;\n            treeNodeList.add(node);\n            if (i &#x3D;&#x3D; 0) &#123;\n                root &#x3D; node;\n            &#125;\n        &#125;\n        &#x2F;&#x2F; 遍历一遍，根据规则左右孩子赋值就可以了\n        &#x2F;&#x2F; 注意这里 结束规则是 i * 2 + 1 &lt; arr.length，避免空指针\n        &#x2F;&#x2F; 为什么结束规则不能是i * 2 + 2 &lt; arr.length呢?\n        &#x2F;&#x2F; 如果i * 2 + 2 &lt; arr.length 是结束条件\n        &#x2F;&#x2F; 那么i * 2 + 1这个符合条件的节点就被忽略掉了\n        &#x2F;&#x2F; 例如[2,7,9,-1,1,9,6,-1,-1,10] 这样的一个二叉树,最后的10就会被忽略掉\n        for (int i &#x3D; 0; i * 2 + 1 &lt; arr.length; i++) &#123;\n            TreeNode node &#x3D; treeNodeList.get(i);\n            if (node !&#x3D; null) &#123;\n                &#x2F;&#x2F; 线性存储转连式存储关键逻辑\n                node.left &#x3D; treeNodeList.get(2 * i + 1);\n                &#x2F;&#x2F;  再次判断下 不忽略任何一个节点\n                if(i * 2 + 2 &lt; arr.length)\n                node.right &#x3D; treeNodeList.get(2 * i + 2);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n&#125;\n直接构建邻接表\n\nArrayList&lt;Integer&gt;[] adjs &#x3D; new ArrayList[n];\nfor(int i &#x3D; 0; adjs.size(); i++)&#123;\n  adjs[i] &#x3D; new ArrayList&lt;&gt;();\n&#125;\nfor(int i &#x3D; 2; i &lt;&#x3D; n; i++)&#123;\n  adjs[father].add(son);\n&#125;\n图的构建\n&#x2F;&#x2F; 邻接表\n&#x2F;&#x2F; graph[x] 存储 x 的所有邻居节点\nList&lt;Integer&gt;[] graph;\n\n&#x2F;&#x2F; 邻接矩阵\n&#x2F;&#x2F; matrix[x][y] 记录 x 是否有一条指向 y 的边\nboolean[][] matrix;\n\n\n递归遍历\n&#x2F;&#x2F; 前序遍历·递归·LC144_二叉树的前序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;Integer&gt;();\n        preorder(root, result);\n        return result;\n    &#125;\n\n    public void preorder(TreeNode root, List&lt;Integer&gt; result) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        result.add(root.val);\n        preorder(root.left, result);\n        preorder(root.right, result);\n    &#125;\n&#125;\n&#x2F;&#x2F; 中序遍历·递归·LC94_二叉树的中序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; res &#x3D; new ArrayList&lt;&gt;();\n        inorder(root, res);\n        return res;\n    &#125;\n\n    void inorder(TreeNode root, List&lt;Integer&gt; list) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        inorder(root.left, list);\n        list.add(root.val);             &#x2F;&#x2F; 注意这一句\n        inorder(root.right, list);\n    &#125;\n&#125;\n&#x2F;&#x2F; 后序遍历·递归·LC145_二叉树的后序遍历\nclass Solution &#123;\n    public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; res &#x3D; new ArrayList&lt;&gt;();\n        postorder(root, res);\n        return res;\n    &#125;\n\n    void postorder(TreeNode root, List&lt;Integer&gt; list) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return;\n        &#125;\n        postorder(root.left, list);\n        postorder(root.right, list);\n        list.add(root.val);             &#x2F;&#x2F; 注意这一句\n    &#125;\n&#125;\n非递归遍历\n&#x2F;&#x2F; 前序遍历顺序：中-左-右，入栈顺序：中-右-左\nclass Solution &#123;\n    public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        stack.push(root);\n        while (!stack.isEmpty())&#123;\n            TreeNode node &#x3D; stack.pop();\n            result.add(node.val);\n            if (node.right !&#x3D; null)&#123;\n                stack.push(node.right);\n            &#125;\n            if (node.left !&#x3D; null)&#123;\n                stack.push(node.left);\n            &#125;\n        &#125;\n        return result;\n    &#125;\n&#125;\n\n&#x2F;&#x2F; 中序遍历顺序: 左-中-右 入栈顺序： 左-右\nclass Solution &#123;\n    public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        TreeNode cur &#x3D; root;\n        while (cur !&#x3D; null || !stack.isEmpty())&#123;\n           if (cur !&#x3D; null)&#123;\n               stack.push(cur);\n               cur &#x3D; cur.left;\n           &#125;else&#123;\n               cur &#x3D; stack.pop();\n               result.add(cur.val);\n               cur &#x3D; cur.right;\n           &#125;\n        &#125;\n        return result;\n    &#125;\n&#125;\n\n&#x2F;&#x2F; 后序遍历顺序 左-右-中 入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果\nclass Solution &#123;\n    public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;\n        List&lt;Integer&gt; result &#x3D; new ArrayList&lt;&gt;();\n        if (root &#x3D;&#x3D; null)&#123;\n            return result;\n        &#125;\n        Stack&lt;TreeNode&gt; stack &#x3D; new Stack&lt;&gt;();\n        stack.push(root);\n        while (!stack.isEmpty())&#123;\n            TreeNode node &#x3D; stack.pop();\n            result.add(node.val);\n            if (node.left !&#x3D; null)&#123;\n                stack.push(node.left);\n            &#125;\n            if (node.right !&#x3D; null)&#123;\n                stack.push(node.right);\n            &#125;\n        &#125;\n        Collections.reverse(result);\n        return result;\n    &#125;\n&#125;\n层序遍历\n&#x2F;&#x2F; 102.二叉树的层序遍历\nclass Solution &#123;\n    public List&lt;List&lt;Integer&gt;&gt; resList &#x3D; new ArrayList&lt;List&lt;Integer&gt;&gt;();\n\n    public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;\n        &#x2F;&#x2F;checkFun01(root,0);\n        checkFun02(root);\n\n        return resList;\n    &#125;\n\n    &#x2F;&#x2F;DFS--递归方式\n    public void checkFun01(TreeNode node, Integer deep) &#123;\n        if (node &#x3D;&#x3D; null) return;\n        deep++;\n\n        if (resList.size() &lt; deep) &#123;\n            &#x2F;&#x2F;当层级增加时，list的Item也增加，利用list的索引值进行层级界定\n            List&lt;Integer&gt; item &#x3D; new ArrayList&lt;Integer&gt;();\n            resList.add(item);\n        &#125;\n        resList.get(deep - 1).add(node.val);\n\n        checkFun01(node.left, deep);\n        checkFun01(node.right, deep);\n    &#125;\n\n    &#x2F;&#x2F;BFS--迭代方式--借助队列\n    public void checkFun02(TreeNode node) &#123;\n        if (node &#x3D;&#x3D; null) return;\n        Queue&lt;TreeNode&gt; que &#x3D; new LinkedList&lt;TreeNode&gt;();\n        que.offer(node);\n\n        while (!que.isEmpty()) &#123;\n            List&lt;Integer&gt; itemList &#x3D; new ArrayList&lt;Integer&gt;();\n            int len &#x3D; que.size();\n\n            while (len &gt; 0) &#123;\n                TreeNode tmpNode &#x3D; que.poll();\n                itemList.add(tmpNode.val);\n\n                if (tmpNode.left !&#x3D; null) que.offer(tmpNode.left);\n                if (tmpNode.right !&#x3D; null) que.offer(tmpNode.right);\n                len--;\n            &#125;\n\n            resList.add(itemList);\n        &#125;\n\n    &#125;\n&#125;\n翻转二叉树\n&#x2F;&#x2F;DFS递归\nclass Solution &#123;\n   &#x2F;**\n     * 前后序遍历都可以\n     * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换）\n     *&#x2F;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return null;\n        &#125;\n        invertTree(root.left);\n        invertTree(root.right);\n        swapChildren(root);\n        return root;\n    &#125;\n\n    private void swapChildren(TreeNode root) &#123;\n        TreeNode tmp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; tmp;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;BFS\nclass Solution &#123;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;return null;&#125;\n        ArrayDeque&lt;TreeNode&gt; deque &#x3D; new ArrayDeque&lt;&gt;();\n        deque.offer(root);\n        while (!deque.isEmpty()) &#123;\n            int size &#x3D; deque.size();\n            while (size-- &gt; 0) &#123;\n                TreeNode node &#x3D; deque.poll();\n                swap(node);\n                if (node.left !&#x3D; null) deque.offer(node.left);\n                if (node.right !&#x3D; null) deque.offer(node.right);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n\n    public void swap(TreeNode root) &#123;\n        TreeNode temp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; temp;\n    &#125;\n&#125;\n二叉树的所有路径\n&#x2F;&#x2F;DFS递归\nclass Solution &#123;\n   &#x2F;**\n     * 前后序遍历都可以\n     * 中序不行，因为先左孩子交换孩子，再根交换孩子（做完后，右孩子已经变成了原来的左孩子），再右孩子交换孩子（此时其实是对原来的左孩子做交换）\n     *&#x2F;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;\n            return null;\n        &#125;\n        invertTree(root.left);\n        invertTree(root.right);\n        swapChildren(root);\n        return root;\n    &#125;\n\n    private void swapChildren(TreeNode root) &#123;\n        TreeNode tmp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; tmp;\n    &#125;\n&#125;\n\n&#x2F;&#x2F;BFS\nclass Solution &#123;\n    public TreeNode invertTree(TreeNode root) &#123;\n        if (root &#x3D;&#x3D; null) &#123;return null;&#125;\n        ArrayDeque&lt;TreeNode&gt; deque &#x3D; new ArrayDeque&lt;&gt;();\n        deque.offer(root);\n        while (!deque.isEmpty()) &#123;\n            int size &#x3D; deque.size();\n            while (size-- &gt; 0) &#123;\n                TreeNode node &#x3D; deque.poll();\n                swap(node);\n                if (node.left !&#x3D; null) deque.offer(node.left);\n                if (node.right !&#x3D; null) deque.offer(node.right);\n            &#125;\n        &#125;\n        return root;\n    &#125;\n\n    public void swap(TreeNode root) &#123;\n        TreeNode temp &#x3D; root.left;\n        root.left &#x3D; root.right;\n        root.right &#x3D; temp;\n    &#125;\n&#125;\n前序和后序构造二叉树\nclass Solution &#123;\n    Map&lt;Integer, Integer&gt; map;  &#x2F;&#x2F; 方便根据数值查找位置\n    public TreeNode buildTree(int[] inorder, int[] postorder) &#123;\n        map &#x3D; new HashMap&lt;&gt;();\n        for (int i &#x3D; 0; i &lt; inorder.length; i++) &#123; &#x2F;&#x2F; 用map保存中序序列的数值对应位置\n            map.put(inorder[i], i);\n        &#125;\n\n        return findNode(inorder,  0, inorder.length, postorder,0, postorder.length);  &#x2F;&#x2F; 前闭后开\n    &#125;\n\n    public TreeNode findNode(int[] inorder, int inBegin, int inEnd, int[] postorder, int postBegin, int postEnd) &#123;\n        &#x2F;&#x2F; 参数里的范围都是前闭后开\n        if (inBegin &gt;&#x3D; inEnd || postBegin &gt;&#x3D; postEnd) &#123;  &#x2F;&#x2F; 不满足左闭右开，说明没有元素，返回空树\n            return null;\n        &#125;\n        int rootIndex &#x3D; map.get(postorder[postEnd - 1]);  &#x2F;&#x2F; 找到后序遍历的最后一个元素在中序遍历中的位置\n        TreeNode root &#x3D; new TreeNode(inorder[rootIndex]);  &#x2F;&#x2F; 构造结点\n        int lenOfLeft &#x3D; rootIndex - inBegin;  &#x2F;&#x2F; 保存中序左子树个数，用来确定后序数列的个数\n        root.left &#x3D; findNode(inorder, inBegin, rootIndex,\n                            postorder, postBegin, postBegin + lenOfLeft);\n        root.right &#x3D; findNode(inorder, rootIndex + 1, inEnd,\n                            postorder, postBegin + lenOfLeft, postEnd - 1);\n\n        return root;\n    &#125;\n&#125;\n前序和中序构造二叉树\nclass Solution &#123;\n    Map&lt;Integer, Integer&gt; map;\n    public TreeNode buildTree(int[] preorder, int[] inorder) &#123;\n        map &#x3D; new HashMap&lt;&gt;();\n        for (int i &#x3D; 0; i &lt; inorder.length; i++) &#123; &#x2F;&#x2F; 用map保存中序序列的数值对应位置\n            map.put(inorder[i], i);\n        &#125;\n\n        return findNode(preorder, 0, preorder.length, inorder,  0, inorder.length);  &#x2F;&#x2F; 前闭后开\n    &#125;\n\n    public TreeNode findNode(int[] preorder, int preBegin, int preEnd, int[] inorder, int inBegin, int inEnd) &#123;\n        &#x2F;&#x2F; 参数里的范围都是前闭后开\n        if (preBegin &gt;&#x3D; preEnd || inBegin &gt;&#x3D; inEnd) &#123;  &#x2F;&#x2F; 不满足左闭右开，说明没有元素，返回空树\n            return null;\n        &#125;\n        int rootIndex &#x3D; map.get(preorder[preBegin]);  &#x2F;&#x2F; 找到前序遍历的第一个元素在中序遍历中的位置\n        TreeNode root &#x3D; new TreeNode(inorder[rootIndex]);  &#x2F;&#x2F; 构造结点\n        int lenOfLeft &#x3D; rootIndex - inBegin;  &#x2F;&#x2F; 保存中序左子树个数，用来确定前序数列的个数\n        root.left &#x3D; findNode(preorder, preBegin + 1, preBegin + lenOfLeft + 1,\n                            inorder, inBegin, rootIndex);\n        root.right &#x3D; findNode(preorder, preBegin + lenOfLeft + 1, preEnd,\n                            inorder, rootIndex + 1, inEnd);\n\n        return root;\n    &#125;\n&#125;\n\n2.动态规划\n背包问题：0-1、完全、多重、二维费用、分组、有依赖的\n路径问题\n打家劫舍和股票买卖\n一般动态规划问题，上一个阶段做了什么决策，不影响下一个阶段的决策。但是打家劫舍&amp;股票买卖这类问题，上一个阶段的决策会影响下一个阶段的决策，所以，每个阶段需要记录不同的决策对应的最值，而不是一个全局的最值\n\n\n爬楼梯\n匹配问题\n\n3.海量数据处理\n使用Hash取余进行分治\n给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL\n首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000 ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, …, a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, …, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, …, a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。接着遍历 ai( i∈[0,999] )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。\n\n\n有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)\n首先遍历大文件，对遍历到的每个词 x，执行 hash(x) % 5000 ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1) ；若存在，则执行 map.put(x, map.get(x)+1) ，将该词频数加 1。上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。\n\n\n\n\n位图\n在 2.5 亿个整数中找出不重复的整数\n用 2 个 bit 来表示各个数字的状态：00 表示这个数字没出现过；01 表示这个数字出现过一次（即为题目所找的不重复整数）；10 表示这个数字出现了多次。那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。\n\n\n给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中\n由于 unsigned int 数字的范围是 [0, 1 &lt;&lt; 32)，我们用 1&lt;&lt;32=4,294,967,296 个 bit 来表示每个数字。初始位均为 0，那么总共需要内存：4,294,967,296b≈512M。我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。\n\n\n\n\n前缀树：常被用来统计字符串的出现次数，另外一个大的用途是字符串查找，判断是否有重复的字符串\n堆\n有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？\n首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。（为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。）\n\n\n从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。\n数据量小：维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数（一个堆保存一半数）；保证这两个堆中的元素个数的差不超过 1。若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。\n数据量大：顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。\n\n\n\n\n\n3.其它\n@SuppressWarnings(&quot;unchecked&quot;)\n\nSometimes Java generics just doesn’t let you do what you want to, and you need to effectively tell the compiler that what you’re doing really will be legal at execution time.\n\n可选的值\n\n\n\nAll\nIt will suppress all warnings.\n解释\n\n\n\nCast\nSuppress the warning while casting from a generic type to a nonqualified type or the other way around.\n\n\n\nDeprecation\nIgnores when we’re using a deprecated(no longer important) method or type.\n使用了不赞成使用的类或方法时的警告\n\n\ndivzero\nSuppresses division by zero warning.\n\n\n\nempty\nIgnores warning of a statement with an empty body.\n\n\n\nunchecked\nIt doesn’t check if the data type is Object or primitive.\n例如使用集合时没有用泛型来指定集合保存的类型\n\n\nfallthrough\nIgnores fall-through on switch statements usually (if “break” is missing).\n当switch程序块直接通往下一种情况而没有break时的警告\n\n\nhiding\nIt suppresses warnings relative to locals that hide variable\n\n\n\nserial\nIt makes the compiler shut up about a missing serialVersionUID.\n在可序列化的类上缺少serialVersionUID定义时的警告\n\n\nfinally\nAvoids warnings relative to finally block that doesn’t return.\n任何 finally 子句不能正常完成时的警告\n\n\nunused\nTo suppress warnings relative to unused code.\n\n\n\n\n\n\nRuntime Error Hangup通常是因为程序在运行时被强制终止或意外终止导致的错误。这个错误通常出现在操作系统或程序遇到了无法处理的异常情况时。一些可能导致Runtime Error 0Hangup错误的原因包括：\n\n内存不足或堆栈溢出；\n访问无效的内存地址；\n文件操作失败或无效的文件指针；\n操作系统或其他软件的错误或冲突；\n程序代码错误或逻辑错误；\n程序被用户手动终止。\n\n\n笔试系统：输入输出学习链接（https://ac.nowcoder.com/acm/contest/5657#question）\n\n示例一：\n&#x2F;&#x2F; 有些输入可能是：\n&#x2F;&#x2F; 输入一个矩阵，每行以空格分隔。\n&#x2F;&#x2F; 3 2 3\n&#x2F;&#x2F; 1 6 5\n&#x2F;&#x2F; 7 8 9\nimport java.io.*;\nimport java.util.*;\n\nclass Solution &#123;\n  public void myFunc(ArrayList&lt;ArrayList&lt;Integer&gt;&gt; arr) &#123;\n    &#x2F;&#x2F; 使用自测数据按钮时调试用，正式提交时要删掉。\n    System.out.println(arr);\n  &#125;\n&#125;\npublic class Main\n&#123;\n  public static void main(String args[])\n  &#123;\n    Scanner cin &#x3D; new Scanner(System.in);\n    ArrayList&lt;ArrayList&lt;Integer&gt;&gt; arr &#x3D; new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;();\n    while(cin.hasNextLine())\n    &#123;\n      ArrayList&lt;Integer&gt; row &#x3D; new ArrayList&lt;Integer&gt;();\n      String line &#x3D; cin.nextLine();\n      if (line.length() &gt; 0) &#123;\n        String[] arrLine &#x3D; line.split(&quot; &quot;);\n        for (int i&#x3D;0; i&lt;arrLine.length; i++) &#123;\n          row.add(Integer.parseInt(arrLine[i]));\n        &#125;\n        arr.add(row);\n      &#125;\n    &#125;\n        \n    new Solution().myFunc(arr);\n  &#125;\n&#125;\n示例二：\n&#x2F;&#x2F;package main\n&#x2F;&#x2F;注意不要添加包名称，否则会报错。\n&#x2F;&#x2F; 不要自定义包名称，否则会报错，即不要添加package answer之类的语句；\n&#x2F;&#x2F; 您可以写很多个类，但是必须有一个类名为Main，并且为public属性，并且Main为唯一的public class；\n&#x2F;&#x2F; Main类的里面必须包含一个名字为&#39;main&#39;的静态方法（函数），这个方法是程序的入口。\n\nimport java.io.*;\nimport java.util.*;\nclass Solution &#123;\n  public int addab(int a, int b) &#123;\n    return a+b;\n  &#125;\n&#125;\npublic class Main\n&#123;\n  public static void main(String args[])\n  &#123;\n    Scanner cin &#x3D; new Scanner(System.in);\n    int a, b;\n    while(cin.hasNextInt())\n    &#123;\n      a &#x3D; cin.nextInt();\n      b &#x3D; cin.nextInt();\n      Solution s &#x3D; new Solution();\n      int c &#x3D; s.addab(a, b);\n      System.out.println(c);\n    &#125;\n  &#125;\n&#125;\n示例三：从在键盘上按Ctrl+Z。这样输入会读取到EOF，表示读取结束。\nwhile (sc.hasNextLine())&#123;\n\t\tScanner sc &#x3D; new Scanner(System.in);\n    String temp &#x3D; sc.nextLine();\n    String[] ss &#x3D; temp.trim().split(&quot; &quot;);\n    int num1 &#x3D; Integer.parseInt(ss[0]);\n    int num2 &#x3D; Integer.parseInt(ss[1]);\n    if (temp.isEmpty())&#123;\n        break;\n    &#125;\n    System.out.println(temp);\n&#125;\n\npublic class Main&#123;\n    public static void main(String[] args)&#123;\n        Scanner sc &#x3D; new Scanner(System.in);\n        int n &#x3D; sc.nextInt();\n        &#x2F;&#x2F;nextInt不会吸收掉换行符，后的nextLine会直接读取换行符，然后结束输入\n        sc.nextLine();\n        String temp &#x3D; sc.nextLine();\n        String[] data;\n        data &#x3D; temp.trim().split(&quot; &quot;);\n        Arrays.sort(data);\n        for(int i &#x3D; 0; i &lt; n; i++)&#123;\n            System.out.print(data[i]);\n            if(i !&#x3D; n-1)&#123;\n                System.out.print(&quot; &quot;);\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n\n","slug":"DataStructure","date":"2023-04-01T04:25:03.000Z","categories_index":"","tags_index":"algorithm","author_index":"Dajunnnnnn"}]